{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22a6bce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda_envs\\envs\\pytorch2.3.1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n",
      "20000 5000 25000\n",
      "vocab_size=  25954\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "raw_datasets = load_dataset(\"imdb\")     #二分类：0=neg， 1=pos\n",
    "\n",
    "print(raw_datasets)\n",
    "\n",
    "train_valid = raw_datasets[\"train\"]\n",
    "test_dataset = raw_datasets[\"test\"]\n",
    "\n",
    "#拆成20000train和5000valid\n",
    "train_valid = train_valid.train_test_split(test_size=5000, seed=42)\n",
    "train_dataset = train_valid[\"train\"]\n",
    "valid_dataset = train_valid[\"test\"]\n",
    "\n",
    "print(len(train_dataset), len(valid_dataset), len(test_dataset))\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    #r表示原始字符串raw string, 避免/这种转义带来的麻烦\n",
    "    #+表示前面的模式至少出现一次，可以连续多次\n",
    "    #可在一起的意思是，匹配text里连续的小写英文字母字符串，也就是按英文单词切分\n",
    "    tokens = re.findall(r\"[a-z]+\", text)\n",
    "    return tokens\n",
    "\n",
    "min_freq = 5 #词频小于5的当<unk>\n",
    "counter = Counter()\n",
    "\n",
    "for example in train_dataset:\n",
    "    tokens = simple_tokenizer(example[\"text\"])\n",
    "    counter.update(tokens)\n",
    "\n",
    "#special tokens\n",
    "PAD = \"<pad>\"\n",
    "UNK = \"<unk>\"\n",
    "\n",
    "#<pad>用于补齐长度，编号0；<unk>表示未知词，编号1\n",
    "vocab = {PAD: 0, UNK: 1}\n",
    "\n",
    "#counter.items()返回(单词，词频)\n",
    "for word, freq in counter.items():\n",
    "    if freq >= min_freq:\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"vocab_size= \", vocab_size)\n",
    "\n",
    "#encode: 把一条原始文本转换成固定长度的token id序列\n",
    "def encode(text, vocab, max_len=256):\n",
    "    tokens = simple_tokenizer(text)\n",
    "    #把token转换成对应的词表id，如果token不在词表中，则用<UNK>的id\n",
    "    ids = [vocab.get(tok, vocab[UNK]) for tok in tokens]\n",
    "\n",
    "    if len(ids) < max_len:\n",
    "        pad_len = max_len - len(ids)\n",
    "        ids += [vocab[PAD]] * pad_len\n",
    "    \n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf91b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    #cpu\n",
    "    torch.manual_seed(seed)\n",
    "    #gpu\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # 强制 PyTorch 使用“确定性”的 cuDNN 算法\n",
    "    # 作用：即使速度慢一些，也保证同样的输入 → 同样的输出\n",
    "    # 否则 cuDNN 可能为了加速选择非确定性实现\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # 关闭 cuDNN 的自动性能优化\n",
    "    # benchmark=True 会根据输入动态选择最快算法，但可能导致结果不稳定\n",
    "    # 这里设为 False，是为了结果可复现而不是追求速度\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, vocab, max_len=256):\n",
    "        self.dataset = hf_dataset\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx][\"text\"]\n",
    "        label = self.dataset[idx][\"label\"]\n",
    "\n",
    "        input_ids = encode(text, vocab=self.vocab, max_len=self.max_len)\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "        return input_ids, label\n",
    "    \n",
    "max_len = 256\n",
    "train_data = TextClassificationDataset(train_dataset, vocab, max_len=max_len)\n",
    "valid_data = TextClassificationDataset(valid_dataset, vocab, max_len=max_len)\n",
    "test_data = TextClassificationDataset(test_dataset, vocab, max_len=max_len)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMAttentionClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim,\n",
    "                 num_classes=2, num_layers=1, bidirectional=True,\n",
    "                 dropout=0.5, pad_idx=0, attn_dim=128):\n",
    "        super().__init__()\n",
    "        #输入：[B, T]\n",
    "        #输出：[B, T, embed_dim]\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "\n",
    "       #输出 H: [B, T, hidden_dim * 2]\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        lstm_out_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "\n",
    "        #attention\n",
    "        self.atten_w = nn.Linear(lstm_out_dim, attn_dim, bias=True)\n",
    "        self.atten_v = nn.Linear(attn_dim, 1, bias=False)\n",
    "\n",
    "        #分类器\n",
    "        self.fc = nn.Linear(lstm_out_dim, num_classes)\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def attention(self, H, mask=None):\n",
    "        \"\"\"\n",
    "        H: [B, T, H_dim]\n",
    "        mask: [B, T]，padding 位置为 0, 真实token为1\n",
    "        \"\"\"\n",
    "        u = torch.tanh(self.atten_w(H))     #[B, T, atten_dim]\n",
    "        #线性映射成标量打分\n",
    "        e = self.atten_v(u).squeeze(-1)     #[B, T]\n",
    "\n",
    "        #mask掉padding的位置\n",
    "        if mask is not None:\n",
    "            e = e.masked_fill(mask==0, float(\"-inf\"))\n",
    "\n",
    "        #softmax归一化得到attention的权重分布\n",
    "        alpha = F.softmax(e, dim=-1)        #[B, T]\n",
    "\n",
    "        alpha_expanded = alpha.unsqueeze(-1)\n",
    "\n",
    "        #加权隐藏状态\n",
    "        weighted_H = H * alpha_expanded     #[B, T, H_dim]\n",
    "\n",
    "        #沿时间求和得到上下文向量\n",
    "        context = weighted_H.sum(dim=1)     #[B, H_dim]\n",
    "\n",
    "        return context, alpha\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids: [B, T]\n",
    "        \"\"\"\n",
    "        emb = self.embedding(input_ids)     #[B, T, E]\n",
    "        H, _ = self.lstm(emb)               #[B, T, H_dim]\n",
    "\n",
    "        #标记真实token的位置\n",
    "        mask = (input_ids != self.pad_idx).long() #[B, T]\n",
    "\n",
    "        context, atten_weights = self.attention(H, mask)\n",
    "        out = self.dropout(context)\n",
    "        logits = self.fc(out)               #[B, num_classes]\n",
    "\n",
    "        return logits, atten_weights\n",
    "    \n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_examples = 0, 0, 0\n",
    "\n",
    "    for input_ids, labels in dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #区分两种模型，有无attention\n",
    "        outputs = model(input_ids)\n",
    "        if isinstance(outputs, tuple):\n",
    "            logits, _ = outputs\n",
    "        else:\n",
    "            logits = outputs\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_examples += input_ids.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_examples\n",
    "    avg_acc = total_correct / total_examples\n",
    "\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_examples = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for input_ids, labels in dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids)\n",
    "        if isinstance(outputs, tuple):\n",
    "            logits, _ = outputs\n",
    "        else:\n",
    "            logits = outputs\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_examples += input_ids.size(0)\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    avg_loss = total_loss / total_examples\n",
    "    avg_acc = total_correct / total_examples\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    return avg_loss, avg_acc, f1\n",
    "\n",
    "def run_experiment(model, train_loader, valid_loader, test_loader, lr=1e-3, epochs=5, model_name=\"model\"):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    best_state_dict = None\n",
    "    best_val_metrics = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        val_loss, val_acc, val_f1 = evaluate(model, valid_loader, criterion)\n",
    "        print(f\"[{model_name}] Epoch: {epoch+1}: \"\n",
    "              f\"trian loss: {train_loss:.4f}, train_acc: {train_acc:.4f},\"\n",
    "              f\"val loss: {val_loss:.4f}, val acc: {val_acc:.4f}, val f1: {val_f1:.4f}\")\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            best_val_metrics = (val_loss, val_acc, val_f1)\n",
    "\n",
    "    #用best checkpoint 在test上评估\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    model.to(device)\n",
    "    test_loss, test_acc, test_f1 = evaluate(model, test_loader, criterion)\n",
    "\n",
    "    print(f\"[{model_name}] BEST on val_f1: \"\n",
    "          f\"val loss: {best_val_metrics[0]:.4f}, val acc: {best_val_metrics[1]:.4f}, val f1: {best_val_metrics[2]:.4f}\")\n",
    "    print(f\"[{model_name}] Test: loss={test_loss:.4f}, acc={test_acc:.4f}, f1={test_f1:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"val_loss\": best_val_metrics[0],\n",
    "        \"val_acc\": best_val_metrics[1],\n",
    "        \"val_f1\": best_val_metrics[2],\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_f1\": test_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cafba56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTMAttentionClassifier] Epoch: 1: trian loss: 0.4905, train_acc: 0.7489,val loss: 0.3863, val acc: 0.8298, val f1: 0.8283\n",
      "[LSTMAttentionClassifier] Epoch: 2: trian loss: 0.2961, train_acc: 0.8756,val loss: 0.3085, val acc: 0.8746, val f1: 0.8745\n",
      "[LSTMAttentionClassifier] Epoch: 3: trian loss: 0.2002, train_acc: 0.9218,val loss: 0.2969, val acc: 0.8758, val f1: 0.8756\n",
      "[LSTMAttentionClassifier] Epoch: 4: trian loss: 0.1321, train_acc: 0.9516,val loss: 0.3197, val acc: 0.8732, val f1: 0.8731\n",
      "[LSTMAttentionClassifier] Epoch: 5: trian loss: 0.0745, train_acc: 0.9746,val loss: 0.4092, val acc: 0.8692, val f1: 0.8690\n",
      "[LSTMAttentionClassifier] BEST on val_f1: val loss: 0.2969, val acc: 0.8758, val f1: 0.8756\n",
      "[LSTMAttentionClassifier] Test: loss=0.3244, acc=0.8594, f1=0.8591\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "hidden_dim = 128\n",
    "num_classes = 2\n",
    "pad_idx = vocab[PAD]\n",
    "epochs = 5\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "attn_model = LSTMAttentionClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_classes=num_classes,\n",
    "    num_layers=1,\n",
    "    bidirectional=True,\n",
    "    dropout=0.5,\n",
    "    pad_idx=pad_idx,\n",
    "    attn_dim=128\n",
    ")\n",
    "metrics_attn = run_experiment(\n",
    "    attn_model, train_loader, valid_loader, test_loader, lr=1e-3, epochs=epochs, model_name=\"LSTMAttentionClassifier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39013ff",
   "metadata": {},
   "source": [
    "### 构建结果表\n",
    "test_dataset：HF 的原始数据集（里面有 \"text\" / \"label\"）\n",
    "\n",
    "attn_model：训练完并且 run_experiment 里已经 load 了 best checkpoint 的模型\n",
    "\n",
    "encode(text, vocab, max_len)：把原始文本转成 id 序列\n",
    "\n",
    "循环遍历 test_dataset（按 batch），对每一条跑 attn_model 得到 logits。\n",
    "\n",
    "用 softmax 得到各类概率、argmax 得到预测标签。\n",
    "\n",
    "把这些东西塞进 pandas.DataFrame：\n",
    "\n",
    "text：原始文本\n",
    "\n",
    "y_true：真实标签（0/1）\n",
    "\n",
    "y_pred：预测标签（0/1）\n",
    "\n",
    "prob_pos：预测为正类的概率（或者 0 类的也行，看你习惯）\n",
    "\n",
    "is_error：是否预测错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "481ff2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_results_table(model, hf_dataset, vocab, max_len=256, batch_size=64, device=device):\n",
    "    \"\"\"\n",
    "    针对给定的 HuggingFace 数据集（如 test_dataset），\n",
    "    用当前模型跑一遍，返回一个 DataFrame 结果表：\n",
    "    - text: 原始文本\n",
    "    - y_true: 真实标签\n",
    "    - y_pred: 预测标签\n",
    "    - prob_pos: 预测为正类(1)的概率\n",
    "    - is_error: 是否预测错误\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_texts = []\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_prob_pos = []\n",
    "    all_pred_conf = []\n",
    "\n",
    "    num_samples = len(hf_dataset)\n",
    "\n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        #end为当前的batch下标\n",
    "        end = min(start + batch_size, num_samples)\n",
    "\n",
    "        batch = hf_dataset[start:end]\n",
    "        batch_texts = batch[\"text\"]\n",
    "        batch_labels = batch[\"label\"]\n",
    "\n",
    "        #编码成ids，对当前batch内每条文本做分词 / 映射到token id，并统一max_len长度\n",
    "        batch_ids = [encode(t, vocab=vocab, max_len=max_len) for t in batch_texts]\n",
    "        batch_ids = torch.tensor(batch_ids, dtype=torch.long, device=device)\n",
    "\n",
    "        outputs = model(batch_ids)\n",
    "\n",
    "        if isinstance(outputs, tuple):\n",
    "          logits, attn_weights = outputs\n",
    "        else:\n",
    "          logits = outputs\n",
    "        \n",
    "        #通过softmax将logits打分转换成概率分布\n",
    "        probs = F.softmax(logits, dim=-1) #[batch_size, num_classes]\n",
    "\n",
    "        #取标签1（正类）的概率\n",
    "        prob_pos = probs[:, 1].detach().cpu().tolist()          #lis[int]\n",
    "        preds = probs.argmax(dim=-1).detach().cpu().tolist()    #list[float]\n",
    "        #预测置信度\n",
    "        pred_conf = probs.max(dim=-1).values.cpu().tolist()     #list[float]\n",
    "\n",
    "        #收集当前batch的结果到总列表中\n",
    "        all_texts.extend(batch_texts)\n",
    "        all_labels.extend(batch_labels)\n",
    "        all_preds.extend(preds)\n",
    "        all_prob_pos.extend(prob_pos)\n",
    "        all_pred_conf.extend(pred_conf)\n",
    "\n",
    "    #长度对齐 sanity check\n",
    "    assert len(all_texts) == len(all_labels) == len(all_preds) == len(all_prob_pos) == len(all_pred_conf)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "       \"text\": all_texts,\n",
    "       \"y_true\": all_labels,\n",
    "       \"y_pred\": all_preds,\n",
    "       \"prob_pos\": all_prob_pos,\n",
    "       \"pred_conf\": all_pred_conf\n",
    "    })\n",
    "    #根据真实标签与预测标签是否相等来标记每条样本是否预测错误\n",
    "    df[\"is_error\"] = df[\"y_true\"] != df[\"y_pred\"]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69223219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  y_true  y_pred  \\\n",
      "0  I love sci-fi and am willing to put up with a ...       0       0   \n",
      "1  Worth the entertainment value of a rental, esp...       0       0   \n",
      "2  its a totally average film with a few semi-alr...       0       0   \n",
      "3  STAR RATING: ***** Saturday Night **** Friday ...       0       0   \n",
      "4  First off let me say, If you haven't enjoyed a...       0       1   \n",
      "\n",
      "   prob_pos  pred_conf  is_error  \n",
      "0  0.002610   0.997390     False  \n",
      "1  0.116442   0.883558     False  \n",
      "2  0.016801   0.983199     False  \n",
      "3  0.016499   0.983501     False  \n",
      "4  0.935788   0.935788      True  \n",
      "LSTM + Attention 测试集结果已保存 -> imdb_lstm_attention_test_results.csv\n"
     ]
    }
   ],
   "source": [
    "results_attn_test = build_results_table(\n",
    "    model=attn_model,\n",
    "    hf_dataset=test_dataset,\n",
    "    vocab=vocab,\n",
    "    max_len=256,\n",
    "    batch_size=64,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(results_attn_test.head())\n",
    "\n",
    "results_attn_test.to_csv(\"imdb_lstm_attention_test_results.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"LSTM + Attention 测试集结果已保存 -> imdb_lstm_attention_test_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6481694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总样本数：25000\n",
      "\n",
      "错误样本数：3515\n",
      "\n",
      "错误率：0.1406\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"imdb_lstm_attention_test_results.csv\")\n",
    "errors = df[df[\"is_error\"]].copy()\n",
    "print(f\"总样本数：{len(df)}\\n\")\n",
    "print(f\"错误样本数：{len(errors)}\\n\")\n",
    "print(f\"错误率：{len(errors) / len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54f8d72",
   "metadata": {},
   "source": [
    "## step 1\n",
    "1.整体 accuracy / F1\n",
    "\n",
    "2.按类别的 precision / recall / F1\n",
    "\n",
    "3.混淆矩阵（confusion matrix）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d8ba88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== overall metrics ===\n",
      "Accuracy       : 0.8594\n",
      "F1 macro       : 0.8591\n",
      "F1 micro       : 0.8594\n",
      "F1 weighted    : 0.8591\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>neg(0)</th>\n",
       "      <td>0.830355</td>\n",
       "      <td>0.90336</td>\n",
       "      <td>0.865321</td>\n",
       "      <td>12500.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos(1)</th>\n",
       "      <td>0.894044</td>\n",
       "      <td>0.81544</td>\n",
       "      <td>0.852935</td>\n",
       "      <td>12500.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.859400</td>\n",
       "      <td>0.85940</td>\n",
       "      <td>0.859400</td>\n",
       "      <td>0.8594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.862200</td>\n",
       "      <td>0.85940</td>\n",
       "      <td>0.859128</td>\n",
       "      <td>25000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.862200</td>\n",
       "      <td>0.85940</td>\n",
       "      <td>0.859128</td>\n",
       "      <td>25000.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision   recall  f1-score     support\n",
       "neg(0)         0.830355  0.90336  0.865321  12500.0000\n",
       "pos(1)         0.894044  0.81544  0.852935  12500.0000\n",
       "accuracy       0.859400  0.85940  0.859400      0.8594\n",
       "macro avg      0.862200  0.85940  0.859128  25000.0000\n",
       "weighted avg   0.862200  0.85940  0.859128  25000.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "             pre_neg(0)  pre_pos(01)\n",
      "true_neg(0)       11292         1208\n",
      "true_pos(1)        2307        10193\n",
      "\n",
      "Normalized Confusion Matrix:\n",
      "             pred_neg(0)  pred_pos(1)\n",
      "true_neg(0)       0.9034       0.0966\n",
      "true_pos(1)       0.1846       0.8154\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "y_true = df[\"y_true\"].to_numpy()\n",
    "y_pred = df[\"y_pred\"].to_numpy()\n",
    "\n",
    "#1.整体指标\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "f1_micro = f1_score(y_true, y_pred, average=\"micro\")\n",
    "f1_weighted = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "print(\"=== overall metrics ===\")\n",
    "print(f\"Accuracy       : {acc:.4f}\\n\"\n",
    "        f\"F1 macro       : {f1_macro:.4f}\\n\"\n",
    "        f\"F1 micro       : {f1_micro:.4f}\\n\"\n",
    "        f\"F1 weighted    : {f1_weighted:.4f}\\n\")\n",
    "\n",
    "#2.按类别指标\n",
    "rep = classification_report(y_true, y_pred, target_names=[\"neg(0)\", \"pos(1)\"], digits=4, output_dict=True)\n",
    "#T是转置，让报告里的每个指标成为行\n",
    "rep_df = pd.DataFrame(rep).T\n",
    "#在jupyter notebook中显示dataframe\n",
    "display(rep_df)\n",
    "\n",
    "#3.混淆矩阵（含归一化）\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "cm_df = pd.DataFrame(cm, index=[\"true_neg(0)\", \"true_pos(1)\"], columns=[\"pre_neg(0)\", \"pre_pos(01)\"])\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_df)\n",
    "\n",
    "#按行归一化, 保留原数组维度\n",
    "cm_norm = cm / cm.sum(axis=1, keepdims=True)\n",
    "cm_norm_df = pd.DataFrame(cm_norm, index=[\"true_neg(0)\", \"true_pos(1)\"], columns=[\"pred_neg(0)\", \"pred_pos(1)\"])\n",
    "print(\"\\nNormalized Confusion Matrix:\")\n",
    "print(cm_norm_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601851d1",
   "metadata": {},
   "source": [
    "## step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "982d4501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP (neg->pos) = 1208\n",
      "FN (pos->neg) = 2307\n"
     ]
    }
   ],
   "source": [
    "#计算 FN/FP 数量\n",
    "TN, FP, FN, TP = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
    "\n",
    "print(f\"FP (neg->pos) = {FP}\")\n",
    "print(f\"FN (pos->neg) = {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba202930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP: 1208 FN: 2307\n"
     ]
    }
   ],
   "source": [
    "fp = errors[(errors[\"y_true\"] == 0) & (errors[\"y_pred\"] == 1)].copy()  # neg->pos\n",
    "fn = errors[(errors[\"y_true\"] == 1) & (errors[\"y_pred\"] == 0)].copy()  # pos->neg\n",
    "print(\"FP:\", len(fp), \"FN:\", len(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1fa913a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- High-conf FN (pos->neg) ----\n",
      "============================================================\n",
      "true=1 pred=0 pred_conf=0.9999 prob_pos=0.0001\n",
      "David Morse and Andre Braugher are very talented actors, which is why I'm trying so hard to support this program. Unfortunately, an irrational plot, and very poor writing is making it difficult for me. I'm hoping that the show gets a serious overhaul, or that the actors find new projects that are wo...\n",
      "============================================================\n",
      "true=1 pred=0 pred_conf=0.9992 prob_pos=0.0008\n",
      "It's not Citizen Kane, but it does deliver. Cleavage, and lots of it.<br /><br />Badly acted and directed, poorly scripted. Who cares? I didn't watch it for the dialog.\n",
      "============================================================\n",
      "true=1 pred=0 pred_conf=0.9986 prob_pos=0.0014\n",
      "Sam Firstenberg's \"Ninja 3:The Domination\" mixes martial arts with \"The Exorcist\" like horror.The horror elements thrown on screen are simply laughable,but the film works as a mindless action/martial arts flick.The fight scenes are well-choreographed and exciting,and the film is never boring.So forg...\n",
      "============================================================\n",
      "true=1 pred=0 pred_conf=0.9983 prob_pos=0.0017\n",
      "Good grief sethrp-1, you COMPLETELY missed the point. The girl was only seen briefly specifically BECAUSE she was the one who was going to kill herself...everyone else was so wrapped up in their own stories they didn't notice her, nor did we. As one of the other students says at the end - we're all ...\n",
      "============================================================\n",
      "true=1 pred=0 pred_conf=0.9979 prob_pos=0.0021\n",
      "This was Laurel and Hardy's last silent film for Roach Studios. However, since the public had a real thirst for \"talkies\", this same short was re-made by the team just a few years later with only a few small plot changes. LAUGHING GRAVY was essentially the same plot except that Stan and Ollie were t...\n",
      "============================================================\n",
      "true=1 pred=0 pred_conf=0.9975 prob_pos=0.0025\n",
      "I just saw this for the first time in 10 or 15 years...maybe close to 20. In some ways, it was better than I remembered...in other, it was MUCH worse.<br /><br />First of all, there's the music. It's just plain awful. There are only 5 songs in the movie, most of them used more than once. The opening...\n",
      "============================================================\n",
      "true=1 pred=0 pred_conf=0.9957 prob_pos=0.0043\n",
      "A very strange, disturbing but intriguing film. I don't think I ever needed to see what a human being can do with his butt, and I doubt if I'll ever want to see it again. That said, there is much to be amused by, like Divine's take on Jayne Mansfield's classic walk in \"The Girl Can't Help It\" and pu...\n",
      "============================================================\n",
      "true=1 pred=0 pred_conf=0.9956 prob_pos=0.0044\n",
      "Yeah, I guess this movie is kinda dull compared to some of Pam Grier's other films. The plot is overly familiar, the dialog stilted, and some of the acting isn't too good. But it's worth seeing for the lengthy stretch near the end of the film, where we see Ms Grier in a sexy blue wetsuit, with the z...\n",
      "============================================================\n",
      "true=1 pred=0 pred_conf=0.9947 prob_pos=0.0053\n",
      "Haggard: The Movie is the real life story of Ryan Dunn, and his girlfriend who cheated on him, also with the help from his two friend, 1. A skate boarder who lives for nothing, and, 2. A trying-to-be funny scientist (which doesn't really work) played by Bam Margera and Brandom Dicamillo.<br /><br />...\n",
      "============================================================\n",
      "true=1 pred=0 pred_conf=0.9945 prob_pos=0.0055\n",
      "COMING on the heels of that 1970's \"Blackploitation\" Era, CONRACK (20th Century-Fox, 1974) offered audiences a low-key, sincere and everyday people sort of a drama. Offering a far different fair to its audience (which was far more general than those \"Gansta\" flicks); being a down to earth dose of re...\n",
      "---- High-conf FP (neg->pos) ----\n",
      "============================================================\n",
      "true=0 pred=1 pred_conf=0.9995 prob_pos=0.9995\n",
      "This movie was pure genius. John Waters is brilliant. It is hilarious and I am not sick of it even after seeing it about 20 times since I bought it a few months ago. The acting is great, although Ricki Lake could have been better. And Johnny Depp is magnificent. He is such a beautiful man and a very...\n",
      "============================================================\n",
      "true=0 pred=1 pred_conf=0.9987 prob_pos=0.9987\n",
      "With the plethora of repetitive and derivative sitcoms jamming fall, summer, winter and spring line-ups, it's nice to see a show that sets itself from the lot in more than one area. <br /><br />'Earl' takes an unusual approach. It's not about the \"daily musings of an eccentric family\" (zzzz..) nor a...\n",
      "============================================================\n",
      "true=0 pred=1 pred_conf=0.9968 prob_pos=0.9968\n",
      "After seeing Forever Hollywood, it would be natural to want to see a John Waters film. At least, one get to say that they have joined the legions of cinema cognoscenti who have experienced the unique cinematic stylings of perhaps the best known non-mainstream director. It's worth the effort, and PF ...\n",
      "============================================================\n",
      "true=0 pred=1 pred_conf=0.9959 prob_pos=0.9959\n",
      "'Helen of Troy' follows the story of Helen and the outbreak of the Trojan War. This is more of a love story between Helen and Paris, who is shipwrecked and falls in love with Helen without knowing she is queen. The film portrays the couple as lovesick and wanting nothing more than to be together. (O...\n",
      "============================================================\n",
      "true=0 pred=1 pred_conf=0.9958 prob_pos=0.9958\n",
      "Based on the comments made so far, everyone seems to either hate this movie or love it. I think it would be fair to point out that although this is not a GREAT movie, it has its interesting moments. For one thing, it was filmed on location in Colorado (was it Breckinridge or Telluride? I can't remem...\n",
      "============================================================\n",
      "true=0 pred=1 pred_conf=0.9958 prob_pos=0.9958\n",
      "This is definitely one of the best Kung fu movies in the history of Cinema. The screenplay is really well done (which is not often the case for this type of movies) and you can see that Chuck (in one of his first role)is a great actor. The final fight with the sherif deputy in the bullring is a mast...\n",
      "============================================================\n",
      "true=0 pred=1 pred_conf=0.9958 prob_pos=0.9958\n",
      "Woody Allen (who I have to confess at the outset I have never been a big fan of) directed this quasi-documentary about the life of Emmett Ray (Sean Penn), a 1930's jazz guitarist whose star apparently shone for a while, then quickly faded. Penn does a credible job in the role, portraying a complicat...\n",
      "============================================================\n",
      "true=0 pred=1 pred_conf=0.9955 prob_pos=0.9955\n",
      "Masterpiece. Carrot Top blows the screen away. Never has one movie captured the essence of the human spirit quite like \"Chairman of the Board.\" 10/10... don't miss this instant classic.\n",
      "============================================================\n",
      "true=0 pred=1 pred_conf=0.9954 prob_pos=0.9954\n",
      "my friend and i rented this one a few nights ago. and, i must say, this is the single best movie i have ever seen. i mean, woah! \"dude, we better get some brew before this joint closes\" and \"dude, linda's not wearin' a bra again.\" what poetry! woah! and it's such a wonderfuly original movie, too. i ...\n",
      "============================================================\n",
      "true=0 pred=1 pred_conf=0.9943 prob_pos=0.9943\n",
      "Why does the poster & artwork say \"Clubbed is one of the best UK indie films I have seen in a very long time. SCREEN INTERNATIONAL\" when it was a quote of the French distributor REPORTED by Screen International (an influential film trade publication). See www.screendaily.com/ScreenDailyArticle.aspx?...\n"
     ]
    }
   ],
   "source": [
    "high_conf_fp = fp.sort_values(\"pred_conf\", ascending=False).head(50)\n",
    "high_conf_fn = fn.sort_values(\"pred_conf\", ascending=False).head(50)\n",
    "def preview(df_subset, k=5, max_chars=300):\n",
    "    for _, r in df_subset.head(k).iterrows():\n",
    "        print(\"=\"*60)\n",
    "        print(f\"true={r['y_true']} pred={r['y_pred']} pred_conf={r['pred_conf']:.4f} prob_pos={r['prob_pos']:.4f}\")\n",
    "        print(r[\"text\"][:max_chars].replace(\"\\n\",\" \") + (\"...\" if len(r[\"text\"])>max_chars else \"\"))\n",
    "\n",
    "print(\"---- High-conf FN (pos->neg) ----\")\n",
    "preview(high_conf_fn, k=10)\n",
    "\n",
    "print(\"---- High-conf FP (neg->pos) ----\")\n",
    "preview(high_conf_fp, k=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e9ac9f",
   "metadata": {},
   "source": [
    "高置信错例偏“模型盲点”，边界错例偏“决策阈值附近/特征不足/可通过数据增强修复”。\n",
    "\n",
    "对于二分类，用 prob_pos 做桶更直观：\n",
    "\n",
    "FN（pos→neg）：模型预测 neg，通常 prob_pos 很低；边界 FN 往往在 prob_pos≈0.4~0.5\n",
    "\n",
    "FP（neg→pos）：边界 FP 往往在 prob_pos≈0.5~0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6aad3fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "border_fn: 50 border_fp: 50\n"
     ]
    }
   ],
   "source": [
    "# 边界FN：本来是pos，但prob_pos靠近0.5（模型其实有点犹豫）\n",
    "border_fn = fn[(fn[\"prob_pos\"] > 0.35) & (fn[\"prob_pos\"] < 0.5)].sample(n=min(50, len(fn)), random_state=42)\n",
    "\n",
    "# 边界FP：本来是neg，但prob_pos靠近0.5（模型也犹豫）\n",
    "border_fp = fp[(fp[\"prob_pos\"] > 0.5) & (fp[\"prob_pos\"] < 0.65)].sample(n=min(50, len(fp)), random_state=42)\n",
    "\n",
    "print(\"border_fn:\", len(border_fn), \"border_fp:\", len(border_fp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569cf57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # 导入正则表达式库，用于做简单的英文分词（按字母序列切词）\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    text = text.lower()  # 统一转小写，避免 \"Good\" 和 \"good\" 被当成两个词\n",
    "    return re.findall(r\"[a-z]+\", text)  # 用正则找出所有连续的英文小写字母串，作为 token 列表\n",
    "\n",
    "\n",
    "def add_diagnostics(df_in, vocab, max_len=256, unk_id=1):\n",
    "    \"\"\"\n",
    "    给输入的 DataFrame 增加一些“诊断列”，用于错误分析的分桶抽样：\n",
    "    - tok_len: 原文本分词后的 token 数（未截断前）\n",
    "    - is_truncated: token 数是否超过 max_len（是否会被截断）\n",
    "    - unk_ratio: 编码到词表后，<unk> 占比（词表覆盖不足的信号）\n",
    "    \"\"\"\n",
    "    df2 = df_in.copy()  # 复制一份，避免直接修改原 DataFrame（防止副作用）\n",
    "\n",
    "    lens = []       # 存每条样本的 token 总长度（未截断）\n",
    "    truncated = []  # 存每条样本是否会被截断（True/False）\n",
    "    unk_ratio = []  # 存每条样本的 <unk> 比例（0~1）\n",
    "\n",
    "    for t in df2[\"text\"]:  # 遍历 DataFrame 中每条文本（逐样本处理）\n",
    "        toks = simple_tokenizer(t)          # 对当前文本做分词，得到 token 列表\n",
    "        lens.append(len(toks))              # 记录 token 数（文本原始长度信号）\n",
    "        truncated.append(len(toks) > max_len)  # 判断是否超过 max_len（超过意味着 encode 会截断）\n",
    "\n",
    "        # 将 token 映射到词表 id；只取前 max_len 个 token（与 encode 的截断逻辑一致）\n",
    "        ids = [vocab.get(tok, unk_id) for tok in toks[:max_len]]\n",
    "\n",
    "        # 如果当前文本分词后为空（极少情况，比如全是符号），UNK 比例设为 1.0\n",
    "        if len(ids) == 0:\n",
    "            unk_ratio.append(1.0)\n",
    "        else:\n",
    "            # 统计 ids 中等于 unk_id 的个数，再除以长度，得到 <unk> 占比\n",
    "            unk_ratio.append(sum(i == unk_id for i in ids) / len(ids))\n",
    "\n",
    "    df2[\"tok_len\"] = lens            # 新增列：token 总长度（未截断前）\n",
    "    df2[\"is_truncated\"] = truncated  # 新增列：是否会被截断\n",
    "    df2[\"unk_ratio\"] = unk_ratio     # 新增列：<unk> 比例\n",
    "\n",
    "    return df2  # 返回带诊断信息的新 DataFrame\n",
    "\n",
    "\n",
    "# 在错误样本 errors 上增加诊断列：\n",
    "# unk_id 用 vocab[\"<unk>\"]，确保与训练/编码时的 UNK id 一致\n",
    "errors_diag = add_diagnostics(\n",
    "    errors,\n",
    "    vocab=vocab,\n",
    "    max_len=256,\n",
    "    unk_id=vocab[\"<unk>\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81d01d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trunc_samples: 50 high_unk_samples: 22\n"
     ]
    }
   ],
   "source": [
    "# 截断桶：长文本被截断导致关键信息丢失\n",
    "trunc_samples = errors_diag[errors_diag[\"is_truncated\"]].sample(\n",
    "    n=min(50, (errors_diag[\"is_truncated\"]).sum()), random_state=42\n",
    ")\n",
    "\n",
    "# 高UNK桶：词表覆盖不足/表达很口语/拼写变体多\n",
    "high_unk_samples = errors_diag[errors_diag[\"unk_ratio\"] > 0.1].sample(\n",
    "    n=min(50, (errors_diag[\"unk_ratio\"] > 0.1).sum()), random_state=42\n",
    ")\n",
    "\n",
    "print(\"trunc_samples:\", len(trunc_samples), \"high_unk_samples:\", len(high_unk_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cadafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contrast_samples: 50 negation_samples: 50\n"
     ]
    }
   ],
   "source": [
    "# 一组“转折/让步”关键词：常见结构是“前面夸，but 后面骂”，很容易导致情感翻转\n",
    "contrast = [\"but\", \"however\", \"though\", \"yet\", \"although\", \"nevertheless\"]\n",
    "\n",
    "# 一组“否定”关键词：否定会改变情感极性（not good / not bad），模型常在这类句子上犯错\n",
    "negation = [\"not\", \"no\", \"never\", \"nothing\", \"none\", \"hardly\", \"barely\"]\n",
    "\n",
    "def contains_any(text, keywords):\n",
    "    t = text.lower()                     # 统一小写，避免大小写影响匹配（\"But\" vs \"but\"）\n",
    "    return any(k in t for k in keywords) # 只要 keywords 里任意一个子串出现在文本里，就返回 True\n",
    "\n",
    "# 给 errors_diag 增加一列：该文本是否包含“转折/让步”关键词（True/False）\n",
    "errors_diag[\"has_contrast\"] = errors_diag[\"text\"].apply(\n",
    "    lambda x: contains_any(x, contrast)  # 对每条文本 x 调用 contains_any，判断是否包含 contrast 关键词\n",
    ")\n",
    "\n",
    "# 给 errors_diag 增加一列：该文本是否包含“否定”关键词（True/False）\n",
    "errors_diag[\"has_negation\"] = errors_diag[\"text\"].apply(\n",
    "    lambda x: contains_any(x, negation)  # 对每条文本 x 调用 contains_any，判断是否包含 negation 关键词\n",
    ")\n",
    "\n",
    "# 从“包含转折词”的错误样本里抽样（最多抽 50 条；如果不足 50 条就全抽）\n",
    "contrast_samples = errors_diag[errors_diag[\"has_contrast\"]].sample(\n",
    "    n=min(50, (errors_diag[\"has_contrast\"]).sum()),  # sum() 会把 True 当 1，统计总共有多少条 True\n",
    "    random_state=42                                  # 固定随机种子，保证每次抽样结果可复现\n",
    ")\n",
    "\n",
    "# 从“包含否定词”的错误样本里抽样（最多抽 50 条；如果不足 50 条就全抽）\n",
    "negation_samples = errors_diag[errors_diag[\"has_negation\"]].sample(\n",
    "    n=min(50, (errors_diag[\"has_negation\"]).sum()),  # 同理：最多 50，少于 50 就按实际数量抽\n",
    "    random_state=42                                  # 固定随机种子，保证可复现\n",
    ")\n",
    "\n",
    "# 打印两个桶分别抽到了多少条（用于 sanity check：是不是桶里压根没数据）\n",
    "print(\"contrast_samples:\", len(contrast_samples), \"negation_samples:\", len(negation_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e357074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>prob_pos</th>\n",
       "      <th>pred_conf</th>\n",
       "      <th>is_error</th>\n",
       "      <th>tok_len</th>\n",
       "      <th>is_truncated</th>\n",
       "      <th>unk_ratio</th>\n",
       "      <th>has_contrast</th>\n",
       "      <th>has_negation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>The very first talking picture has returned fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.394698</td>\n",
       "      <td>0.605301</td>\n",
       "      <td>True</td>\n",
       "      <td>523.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>If you want to have a great time then this is ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014021</td>\n",
       "      <td>0.985979</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>Basically an endearingly chintzy and moronic $...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.048224</td>\n",
       "      <td>0.951776</td>\n",
       "      <td>True</td>\n",
       "      <td>259.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>Extraordinary Rendition is a frightening pract...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.467025</td>\n",
       "      <td>0.532975</td>\n",
       "      <td>True</td>\n",
       "      <td>649.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Woody Allen (who I have to confess at the outs...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995810</td>\n",
       "      <td>0.995810</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Walter Matthau and Jack Lemmon, both of whom a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.407434</td>\n",
       "      <td>0.592566</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>I agree that Mary Woronov (Murdoch's secretary...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.505140</td>\n",
       "      <td>0.505140</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>It was the first action movie made in banned i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.629593</td>\n",
       "      <td>0.629593</td>\n",
       "      <td>True</td>\n",
       "      <td>86.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>I'm sick and tired of people complaining that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.090048</td>\n",
       "      <td>0.909952</td>\n",
       "      <td>True</td>\n",
       "      <td>888.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>I had never heard about this film prior to com...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469841</td>\n",
       "      <td>0.530159</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  y_true  y_pred  \\\n",
       "224  The very first talking picture has returned fr...       1       0   \n",
       "42   If you want to have a great time then this is ...       1       0   \n",
       "266  Basically an endearingly chintzy and moronic $...       1       0   \n",
       "312  Extraordinary Rendition is a frightening pract...       1       0   \n",
       "56   Woody Allen (who I have to confess at the outs...       0       1   \n",
       "..                                                 ...     ...     ...   \n",
       "115  Walter Matthau and Jack Lemmon, both of whom a...       1       0   \n",
       "199  I agree that Mary Woronov (Murdoch's secretary...       0       1   \n",
       "305  It was the first action movie made in banned i...       0       1   \n",
       "247  I'm sick and tired of people complaining that ...       1       0   \n",
       "120  I had never heard about this film prior to com...       1       0   \n",
       "\n",
       "     prob_pos  pred_conf  is_error  tok_len is_truncated  unk_ratio  \\\n",
       "224  0.394698   0.605301      True    523.0         True   0.074219   \n",
       "42   0.014021   0.985979      True      NaN          NaN        NaN   \n",
       "266  0.048224   0.951776      True    259.0         True   0.101562   \n",
       "312  0.467025   0.532975      True    649.0         True   0.023438   \n",
       "56   0.995810   0.995810      True      NaN          NaN        NaN   \n",
       "..        ...        ...       ...      ...          ...        ...   \n",
       "115  0.407434   0.592566      True      NaN          NaN        NaN   \n",
       "199  0.505140   0.505140      True      NaN          NaN        NaN   \n",
       "305  0.629593   0.629593      True     86.0        False   0.000000   \n",
       "247  0.090048   0.909952      True    888.0         True   0.015625   \n",
       "120  0.469841   0.530159      True      NaN          NaN        NaN   \n",
       "\n",
       "    has_contrast has_negation  \n",
       "224          NaN          NaN  \n",
       "42           NaN          NaN  \n",
       "266          NaN          NaN  \n",
       "312         True         True  \n",
       "56           NaN          NaN  \n",
       "..           ...          ...  \n",
       "115          NaN          NaN  \n",
       "199          NaN          NaN  \n",
       "305         True         True  \n",
       "247          NaN          NaN  \n",
       "120          NaN          NaN  \n",
       "\n",
       "[200 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> imdb_error_samples_to_label.csv, size= 200\n"
     ]
    }
   ],
   "source": [
    "to_label = pd.concat([\n",
    "    high_conf_fn,\n",
    "    high_conf_fp,\n",
    "    border_fn,\n",
    "    border_fp,\n",
    "    trunc_samples,\n",
    "    high_unk_samples,\n",
    "    contrast_samples,\n",
    "    negation_samples\n",
    "], ignore_index=True).drop_duplicates(subset=[\"text\"])\n",
    "\n",
    "# 只取前200条（你也可以随机抽200）\n",
    "to_label = to_label.sample(n=min(200, len(to_label)), random_state=42)\n",
    "\n",
    "display(to_label)\n",
    "\n",
    "to_label[\"error_type\"] = \"\"   # 手动标：label_issue / semantic_hard / data_coverage / preprocess_length ...\n",
    "to_label[\"comment\"] = \"\"\n",
    "to_label.to_csv(\"imdb_error_samples_to_label.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved -> imdb_error_samples_to_label.csv, size=\", len(to_label))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
