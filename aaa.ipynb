{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4f1a1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
      "Extracting...\n",
      "Dataset ready: ./data_sms\\SMSSpamCollection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda_envs\\envs\\pytorch2.3.1\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss=0.3653 | acc=0.9489 | P=0.9259 R=0.6711 F1=0.7782 | AUC=0.9583\n",
      "Confusion Matrix:\n",
      " [[958   8]\n",
      " [ 49 100]]\n",
      "Epoch 02 | loss=0.1395 | acc=0.9668 | P=0.9242 R=0.8188 F1=0.8683 | AUC=0.9794\n",
      "Confusion Matrix:\n",
      " [[956  10]\n",
      " [ 27 122]]\n",
      "Epoch 03 | loss=0.0958 | acc=0.9731 | P=0.8889 R=0.9128 F1=0.9007 | AUC=0.9853\n",
      "Confusion Matrix:\n",
      " [[949  17]\n",
      " [ 13 136]]\n",
      "Epoch 04 | loss=0.0653 | acc=0.9776 | P=0.9559 R=0.8725 F1=0.9123 | AUC=0.9864\n",
      "Confusion Matrix:\n",
      " [[960   6]\n",
      " [ 19 130]]\n",
      "Epoch 05 | loss=0.0560 | acc=0.9785 | P=0.9032 R=0.9396 F1=0.9211 | AUC=0.9894\n",
      "Confusion Matrix:\n",
      " [[951  15]\n",
      " [  9 140]]\n",
      "Epoch 06 | loss=0.0429 | acc=0.9650 | P=0.8198 R=0.9463 F1=0.8785 | AUC=0.9902\n",
      "Confusion Matrix:\n",
      " [[935  31]\n",
      " [  8 141]]\n",
      "Epoch 07 | loss=0.0342 | acc=0.9830 | P=0.9392 R=0.9329 F1=0.9360 | AUC=0.9896\n",
      "Confusion Matrix:\n",
      " [[957   9]\n",
      " [ 10 139]]\n",
      "Epoch 08 | loss=0.0270 | acc=0.9821 | P=0.9329 R=0.9329 F1=0.9329 | AUC=0.9882\n",
      "Confusion Matrix:\n",
      " [[956  10]\n",
      " [ 10 139]]\n",
      "Epoch 09 | loss=0.0248 | acc=0.9821 | P=0.9448 R=0.9195 F1=0.9320 | AUC=0.9898\n",
      "Confusion Matrix:\n",
      " [[958   8]\n",
      " [ 12 137]]\n",
      "Epoch 10 | loss=0.0181 | acc=0.9839 | P=0.9580 R=0.9195 F1=0.9384 | AUC=0.9876\n",
      "Confusion Matrix:\n",
      " [[960   6]\n",
      " [ 12 137]]\n",
      "Best F1: 0.9383561643835616\n",
      "Saved to best_sms_transformer.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, roc_auc_score\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Reproducibility\n",
    "# -----------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Download + Load Dataset\n",
    "# -----------------------------\n",
    "UCI_ZIP_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "DATA_DIR = \"./data_sms\"\n",
    "ZIP_PATH = os.path.join(DATA_DIR, \"smsspamcollection.zip\")\n",
    "RAW_PATH = os.path.join(DATA_DIR, \"SMSSpamCollection\")\n",
    "\n",
    "def download_and_extract():\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(RAW_PATH):\n",
    "        if not os.path.exists(ZIP_PATH):\n",
    "            print(f\"Downloading: {UCI_ZIP_URL}\")\n",
    "            r = requests.get(UCI_ZIP_URL, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            with open(ZIP_PATH, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "\n",
    "        print(\"Extracting...\")\n",
    "        with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
    "            z.extractall(DATA_DIR)\n",
    "\n",
    "    assert os.path.exists(RAW_PATH), \"Dataset file not found after extraction.\"\n",
    "    print(\"Dataset ready:\", RAW_PATH)\n",
    "\n",
    "def load_sms():\n",
    "    texts, labels = [], []\n",
    "    with open(RAW_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            # format: label \\t text\n",
    "            parts = line.rstrip(\"\\n\").split(\"\\t\", maxsplit=1)\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            lab, txt = parts\n",
    "            labels.append(1 if lab == \"spam\" else 0)\n",
    "            texts.append(txt)\n",
    "    return texts, labels\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Tokenize + Vocab\n",
    "# -----------------------------\n",
    "TOKEN_RE = re.compile(r\"[A-Za-z0-9']+\")\n",
    "\n",
    "def simple_tokenize(text: str):\n",
    "    # lowercase + keep simple tokens\n",
    "    return TOKEN_RE.findall(text.lower())\n",
    "\n",
    "def build_vocab(texts, min_freq=2, max_size=20000, special_tokens=(\"<pad>\", \"<unk>\")):\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        counter.update(simple_tokenize(t))\n",
    "\n",
    "    vocab = {tok: i for i, tok in enumerate(special_tokens)}\n",
    "    for tok, freq in counter.most_common():\n",
    "        if freq < min_freq:\n",
    "            break\n",
    "        if tok in vocab:\n",
    "            continue\n",
    "        vocab[tok] = len(vocab)\n",
    "        if len(vocab) >= max_size:\n",
    "            break\n",
    "    return vocab\n",
    "\n",
    "def numericalize(text, vocab):\n",
    "    unk_id = vocab[\"<unk>\"]\n",
    "    return [vocab.get(tok, unk_id) for tok in simple_tokenize(text)]\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Dataset / Collate\n",
    "# -----------------------------\n",
    "class SmsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=128):\n",
    "        self.labels = labels\n",
    "        self.seqs = []\n",
    "        for t in texts:\n",
    "            ids = numericalize(t, vocab)[:max_len]\n",
    "            self.seqs.append(ids)\n",
    "        self.max_len = max_len\n",
    "        self.pad_id = vocab[\"<pad>\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seqs[idx], self.labels[idx]\n",
    "\n",
    "def collate_batch(batch, pad_id):\n",
    "    seqs, labels = zip(*batch)\n",
    "    max_len = max(len(s) for s in seqs)\n",
    "    x = torch.full((len(seqs), max_len), pad_id, dtype=torch.long)\n",
    "    attn_mask = torch.zeros((len(seqs), max_len), dtype=torch.bool)  # True for padding\n",
    "    for i, s in enumerate(seqs):\n",
    "        x[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
    "        attn_mask[i, len(s):] = True\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "    return x, attn_mask, y\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Transformer Model\n",
    "# -----------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model % 2 == 1:\n",
    "            # if odd dim, last cos part truncated\n",
    "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerTextClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size: int, d_model=128, nhead=4, num_layers=2,\n",
    "        dim_feedforward=256, dropout=0.1, num_classes=2, pad_id=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.pos_enc = PositionalEncoding(d_model, dropout=dropout, max_len=512)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, key_padding_mask):\n",
    "        # input_ids: (B, T)\n",
    "        x = self.embedding(input_ids)             # (B, T, D)\n",
    "        x = self.pos_enc(x)                       # (B, T, D)\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding_mask)  # (B, T, D)\n",
    "\n",
    "        # mean pooling over non-pad tokens\n",
    "        non_pad = (~key_padding_mask).float()     # (B, T)\n",
    "        lengths = non_pad.sum(dim=1).clamp(min=1) # (B,)\n",
    "        pooled = (x * non_pad.unsqueeze(-1)).sum(dim=1) / lengths.unsqueeze(-1)  # (B, D)\n",
    "\n",
    "        logits = self.classifier(pooled)          # (B, C)\n",
    "        return logits\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Train / Eval\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_y, all_pred, all_prob = [], [], []\n",
    "    for x, pad_mask, y in loader:\n",
    "        x, pad_mask, y = x.to(DEVICE), pad_mask.to(DEVICE), y.to(DEVICE)\n",
    "        logits = model(x, pad_mask)\n",
    "        prob = torch.softmax(logits, dim=-1)[:, 1]  # P(spam)\n",
    "        pred = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        all_y.extend(y.cpu().tolist())\n",
    "        all_pred.extend(pred.cpu().tolist())\n",
    "        all_prob.extend(prob.cpu().tolist())\n",
    "\n",
    "    acc = accuracy_score(all_y, all_pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_y, all_pred, average=\"binary\", zero_division=0)\n",
    "    cm = confusion_matrix(all_y, all_pred)\n",
    "    auc = roc_auc_score(all_y, all_prob)\n",
    "    return acc, p, r, f1, auc, cm\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x, pad_mask, y in loader:\n",
    "        x, pad_mask, y = x.to(DEVICE), pad_mask.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x, pad_mask)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def main():\n",
    "    download_and_extract()\n",
    "    texts, labels = load_sms()\n",
    "\n",
    "    # split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    vocab = build_vocab(X_train, min_freq=2, max_size=20000)\n",
    "    pad_id = vocab[\"<pad>\"]\n",
    "\n",
    "    train_ds = SmsDataset(X_train, y_train, vocab, max_len=128)\n",
    "    test_ds  = SmsDataset(X_test,  y_test,  vocab, max_len=128)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=64, shuffle=True,\n",
    "        collate_fn=lambda b: collate_batch(b, pad_id)\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=128, shuffle=False,\n",
    "        collate_fn=lambda b: collate_batch(b, pad_id)\n",
    "    )\n",
    "\n",
    "    model = TransformerTextClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        d_model=128, nhead=4, num_layers=2,\n",
    "        dim_feedforward=256, dropout=0.1,\n",
    "        num_classes=2, pad_id=pad_id\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    for epoch in range(1, 11):\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        acc, p, r, f1, auc, cm = evaluate(model, test_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | loss={loss:.4f} | acc={acc:.4f} | P={p:.4f} R={r:.4f} F1={f1:.4f} | AUC={auc:.4f}\")\n",
    "        print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save({\"model\": model.state_dict(), \"vocab\": vocab}, \"best_sms_transformer.pt\")\n",
    "\n",
    "    print(\"Best F1:\", best_f1)\n",
    "    print(\"Saved to best_sms_transformer.pt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
