一、npl和cv的本质差异

cv输入天然是数值 图片 -> 像素矩阵 -> 张量 -> 模型
cv的难点是“结构建模”，不是如何表示
“结构建模”（structure modeling）：指模型不只利用特征值本身，而是显式或隐式地建模数据内部元素之间的组织方式、依赖关系和排列规则。
为什么cv的难点是结构建模：cv的输入具有显式的二维空间结构，因此底层结构建模相对容易；但从像素级结构中抽象出物体、关系等高层语义结构依然非常困难，所以结构建模依然是cv的核心难点。

npl的输入是符号系统（symbol）
例如：“猫”和“狗”谁大、谁小，差值是多少，没有数学意义。
npl输入本身就是符号，结构是隐含的，所以npl的第一难点是表示，然后才是结构建模

cv结构建模：起点容易，终点难 npl的结构建模：起点即很难

cv：数值 -> 结构    npl：符号 -> 数值 -> 结构


二、为什么在npl里不能直接把此当数字？

词是离散符号，id只是索引，没有语义连续性。
如果直接使用，会引入伪距离和伪大小关系，模型学到的是编号规则而不是语言规律。


三、文本转换成数值的三条主路线

1.统计表示（count-base）：把文本当成“离散事件集合/序列”，用计数或技术加权来表示文本
建词表（Vocabulary）：收集训练语料里出现过的词/子词/字符片段
映射成向量：每个维度对应词表里的一个 token，用计数填进去

（1）BoW（big of word）是最基础的词频向量
BoW假设一段文本只由其中出现的词和次数来决定，忽略语序和语法结构。
例子：V = [I, love, NLP, and, machine, learning]
     句子：I love NLP and I love machine learning
     向量x=[2, 2, 1, 1, 1, 1]
BoW的三个缺点：无语义、无顺序、维度高且稀疏（词表可能很大，但每句话只包含其中的几个词，导致向量99.9%都是0。

（2）TF/IDF/TF-IDF
a）TF（Term Frequency）直观就是“词频”，但常见会做归一化，避免长文本天然更大。
   TF表示此在当前文档的重要程度，出现越多越重要。
b）IDF（Inverse Document Frequency）表达的是越常见的词，越不“信息量大”。
   如果一个词在几乎所有文档都出现，那它对区分文档没用 → 权重应该低。
   如果一个词只在少数文档出现，它更“稀有” → 更能区分 → 权重高。
c）TF-IDF = “在本篇出现得多” × “在全局出现得少”
   结果就是：“and/the/is” → tf 高，但 idf 极低 → 权重整体低
            “transformer/semiconductor/eczema” → idf 高 → 更突出

（3）N-gram的思路是不只统计词，还统计了“连续n个token序列”。以此来解决BoW的最大问题--没有顺序
例子：I love NLP
    1-gram：I, love, NLP
    2-gram：I love, love NLP
    3-gram：I love NLP
这样就能补捉局部顺序信息，比如not good 和 good 的区别。
但是会出现维度爆炸：如果词表大小是 |V|，1-gram 维度 ~ |V|，2-gram 理论上 ~ |V|²，3-gram 理论上 ~ |V|³。

2.分布式表示（embeding）
BoW/TF-IDF的三大硬伤：维度爆炸，极度稀疏，无语义。
Embedding的目标：用低维、稠密、可比较的向量表示词，使语义关系能通过几何关系表示出来。也就是把一个词从一个维度变成一个点（向量）。

You shall konw a word by the company it keeps.一个词的意义由它的上下文决定
这句话表示了分布式语义的思想：如果两个词经常出现在相似的上下文中，他们的语义就相似。

embeding不是直接学习语义，而是通过一个代理任务学出来的：给定上下文，预测中心词（CBOW）；给定中心词，预测上下文（Skip-gram）。因为要想预测得准，模型必须把“语义相近的词”学成相近的表示。
此变成了向量，就可以用向量距离/角度衡量相似度，这就是一个“质变点”：NLP 第一次能用统一的数学方式计算“猫和狗更像，猫和椅子不像”。

（1）Word2Vec 是最经典的 embedding 学习框架
其核心是两个学习框架：
a）CBOW（Continuous Bag of Words）：输入：上下文词；输出：中心词
b）Skip-gram：输入：中心词；输出：上下文词
CBOW 快，Skip-gram 对低频词好。

aaa）Word2Vec 用负采样（每次只挑少量负例，变成二分类任务）等技巧避免全词表 softmax 的计算爆炸。
正例是与当前上下文相关的词，负例则是不相关。正例负责拉近相关词的距离，负例负责拉开不相关词的距离。负采样（negative simpling）是指不用把“所有其他词”都当负例，只随机抽一小部分代表“非上下文词”。这样就可以把很多个类分类问题（比如有100000个词，那么就要把这100000个词分类），变成多个二分类问题。

（2）GloVe
Word2Vec 更偏“局部上下文预测”；GloVe 更偏“全局共现统计”。
Word2Vec：预测驱动（predictive）
GloVe：计数驱动（count-based）但最终仍学 embedding

（3）FastText：它补 Word2Vec 的“致命短板”
Word2Vec隐含假设是词是原子单位（一个词一个向量），这会导致低频词很难学好、OOV（没见过的词）直接没向量、形态变化（played/playing）关系学不到。
FastText 的核心：把词拆成 字符 n-gram 子词，词向量由子词向量组合而成。
例子：playing：pla, lay, ayi, yin, ing …
FastText 用子词建模缓解低频和 OOV

3.预训练表示（Contextual）
预训练解决Word2Vec的问题：一个词只能有一个含义。
它的特点是：同一个词，在不同的上下文有不同向量；预训练+下游任务微调


四、NLP 任务分类（决定模型结构）
1.分类
例子：情感分析、垃圾邮件识别
形式：
    输入：一句话
    输出：一个标签

2.序列标注（Sequence Labeling）
例子：NER（命名实体识别）、分词、POS tagging
形式：
    输入：一句话
    输出：每个 token 一个标签

3.生成（Generation）
例子：机器翻译、对话、文本摘要
形式：
    输入：序列
    输出：新序列


五、NPL的技术演进路线图
规则系统
   ↓
统计方法（BoW / TF-IDF）
   ↓
神经网络 + Embedding（Word2Vec）
   ↓
RNN / LSTM / GRU
   ↓
Attention
   ↓
Transformer
   ↓
预训练语言模型（BERT / GPT）


 NLP 的核心不是模型，而是“如何把符号映射为可学习的数值表示