{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f309a2bf",
   "metadata": {},
   "source": [
    "## Ablation消融实验\n",
    "\n",
    "**Attention vs 均匀池化**\n",
    "\n",
    "baseline模型：BiLSTM + 掩码mean pooling （所有token权重一样=1 / 长度）\n",
    "\n",
    "attention模型：BiLSTM + attention （权重由网络学习得到）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "615c32a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda_envs\\envs\\pytorch2.3.1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n",
      "20000 5000 25000\n",
      "vocab_size=  25954\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "raw_datasets = load_dataset(\"imdb\")     #二分类：0=neg， 1=pos\n",
    "\n",
    "print(raw_datasets)\n",
    "\n",
    "train_valid = raw_datasets[\"train\"]\n",
    "test_dataset = raw_datasets[\"test\"]\n",
    "\n",
    "#拆成20000train和5000valid\n",
    "train_valid = train_valid.train_test_split(test_size=5000, seed=42)\n",
    "train_dataset = train_valid[\"train\"]\n",
    "valid_dataset = train_valid[\"test\"]\n",
    "\n",
    "print(len(train_dataset), len(valid_dataset), len(test_dataset))\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    #r表示原始字符串raw string, 避免/这种转义带来的麻烦\n",
    "    #+表示前面的模式至少出现一次，可以连续多次\n",
    "    #可在一起的意思是，匹配text里连续的小写英文字母字符串，也就是按英文单词切分\n",
    "    tokens = re.findall(r\"[a-z]+\", text)\n",
    "    return tokens\n",
    "\n",
    "min_freq = 5 #词频小于5的当<unk>\n",
    "counter = Counter()\n",
    "\n",
    "for example in train_dataset:\n",
    "    tokens = simple_tokenizer(example[\"text\"])\n",
    "    counter.update(tokens)\n",
    "\n",
    "#special tokens\n",
    "PAD = \"<pad>\"\n",
    "UNK = \"<unk>\"\n",
    "\n",
    "#<pad>用于补齐长度，编号0；<unk>表示未知词，编号1\n",
    "vocab = {PAD: 0, UNK: 1}\n",
    "\n",
    "#counter.items()返回(单词，词频)\n",
    "for word, freq in counter.items():\n",
    "    if freq >= min_freq:\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"vocab_size= \", vocab_size)\n",
    "\n",
    "#encode: 把一条原始文本转换成固定长度的token id序列\n",
    "def encode(text, vocab, max_len=256):\n",
    "    tokens = simple_tokenizer(text)\n",
    "    #把token转换成对应的词表id，如果token不在词表中，则用<UNK>的id\n",
    "    ids = [vocab.get(tok, vocab[UNK]) for tok in tokens]\n",
    "\n",
    "    if len(ids) < max_len:\n",
    "        pad_len = max_len - len(ids)\n",
    "        ids += [vocab[PAD]] * pad_len\n",
    "    \n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21af62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    #cpu\n",
    "    torch.manual_seed(seed)\n",
    "    #gpu\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # 强制 PyTorch 使用“确定性”的 cuDNN 算法\n",
    "    # 作用：即使速度慢一些，也保证同样的输入 → 同样的输出\n",
    "    # 否则 cuDNN 可能为了加速选择非确定性实现\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # 关闭 cuDNN 的自动性能优化\n",
    "    # benchmark=True 会根据输入动态选择最快算法，但可能导致结果不稳定\n",
    "    # 这里设为 False，是为了结果可复现而不是追求速度\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, vocab, max_len=256):\n",
    "        self.dataset = hf_dataset\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx][\"text\"]\n",
    "        label = self.dataset[idx][\"label\"]\n",
    "\n",
    "        input_ids = encode(text, vocab=self.vocab, max_len=self.max_len)\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "        return input_ids, label\n",
    "    \n",
    "max_len = 256\n",
    "train_data = TextClassificationDataset(train_dataset, vocab, max_len=max_len)\n",
    "valid_data = TextClassificationDataset(valid_dataset, vocab, max_len=max_len)\n",
    "test_data = TextClassificationDataset(test_dataset, vocab, max_len=max_len)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be5a55",
   "metadata": {},
   "source": [
    "### BiLSTMPoolingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a8dd71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiLSTMPoolingClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, \n",
    "                 num_classes=2, num_layers=1, \n",
    "                 bidirectional=True, dropout=0.5, pad_idx=0, pooling=\"mean\"):\n",
    "        super().__init__()\n",
    "\n",
    "        #输入：[B, T]\n",
    "        #输出：[B, T, embed_dim]\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "\n",
    "        #输出 H: [B, T, hidden_dim * 2]\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pad_idx = pad_idx\n",
    "        self.pooling = pooling\n",
    "        lstm_output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Linear(lstm_output_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids: [B, T]\n",
    "        \"\"\"\n",
    "        emb = self.embedding(input_ids)     #[B, T, E]\n",
    "        H, _ = self.lstm(emb)      #H: [B, T, 2H]\n",
    "\n",
    "        #构造mask，标记哪些位置是真实token， 哪些位置是padding\n",
    "        mask = (input_ids != self.pad_idx).unsqueeze(-1)    #mask:[B, T, 1], detype=bool\n",
    "\n",
    "        #对非pad位置做加权平均\n",
    "        if self.pooling == \"mean\":\n",
    "            #把pad位置的hidden置为0， 真实token保留原值\n",
    "            H_masked = H * mask                     #[B, T, 2H]\n",
    "            #延时间维度求和\n",
    "            sum_hidden = H_masked.sum(dim=1)        #[B, 2H]\n",
    "            #计算每个样本的真实长度\n",
    "            lengths = mask.sum(dim=1).clamp(min=1)  #[B, 1]\n",
    "            pooled = sum_hidden / lengths\n",
    "        \n",
    "        #对时间维度做最大池化\n",
    "        elif self.pooling == \"max\":\n",
    "            #对于padding位置，把hidden的值设为1e-9\n",
    "            H_masked = H.maked_fill(mask == 0, 1e-9) #[B, T, 2H]\n",
    "            #延时间维度T取每个维度上的最大值\n",
    "            #_是max的索引，不需要\n",
    "            pooled, _ = H_masked.max(dim=1)\n",
    "\n",
    "        #取最后一个非pad_token的hidden\n",
    "        elif self.pooling == \"last_nonpad\":\n",
    "            #对mask在时间维度上求和，计算每一个样本的真是长度\n",
    "            lengths = mask.squeeze(-1).max(dim=1)   #[B]\n",
    "            #取最后一个pad的索引\n",
    "            idx = (lengths - 1).clamp(min=1)        #[B]\n",
    "            #构造一个batch索引，用来和时间索引组合取值\n",
    "            batch_idx = torch.arange(H.size(0), device=H.device)    #[B]\n",
    "            #取每个样本最后一个非pad位置的hidden\n",
    "            pooled = H[batch_idx, idx, :]           #[B, 2H]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unkown pooling type: {self.pooling}\")\n",
    "\n",
    "        #缓解过拟合\n",
    "        out = self.dropout(pooled)                  #[B, 2H]\n",
    "        logits = self.fc(out)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811485ae",
   "metadata": {},
   "source": [
    "### LSTMAttentionClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b604d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMAttentionClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim,\n",
    "                 num_classes=2, num_layers=1, bidirectional=True,\n",
    "                 dropout=0.5, pad_idx=0, attn_dim=128):\n",
    "        super().__init__()\n",
    "        #输入：[B, T]\n",
    "        #输出：[B, T, embed_dim]\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "\n",
    "       #输出 H: [B, T, hidden_dim * 2]\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        lstm_out_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "\n",
    "        #attention\n",
    "        self.atten_w = nn.Linear(lstm_out_dim, attn_dim, bias=True)\n",
    "        self.atten_v = nn.Linear(attn_dim, 1, bias=False)\n",
    "\n",
    "        #分类器\n",
    "        self.fc = nn.Linear(lstm_out_dim, num_classes)\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def attention(self, H, mask=None):\n",
    "        \"\"\"\n",
    "        H: [B, T, H_dim]\n",
    "        mask: [B, T]，padding 位置为 0, 真实token为1\n",
    "        \"\"\"\n",
    "        u = torch.tanh(self.atten_w(H))     #[B, T, atten_dim]\n",
    "        #线性映射成标量打分\n",
    "        e = self.atten_v(u).squeeze(-1)     #[B, T]\n",
    "\n",
    "        #mask掉padding的位置\n",
    "        if mask is not None:\n",
    "            e = e.masked_fill(mask==0, float(\"-inf\"))\n",
    "\n",
    "        #softmax归一化得到attention的权重分布\n",
    "        alpha = F.softmax(e, dim=-1)        #[B, T]\n",
    "\n",
    "        alpha_expanded = alpha.unsqueeze(-1)\n",
    "\n",
    "        #加权隐藏状态\n",
    "        weighted_H = H * alpha_expanded     #[B, T, H_dim]\n",
    "\n",
    "        #沿时间求和得到上下文向量\n",
    "        context = weighted_H.sum(dim=1)     #[B, H_dim]\n",
    "\n",
    "        return context, alpha\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids: [B, T]\n",
    "        \"\"\"\n",
    "        emb = self.embedding(input_ids)     #[B, T, E]\n",
    "        H, _ = self.lstm(emb)               #[B, T, H_dim]\n",
    "\n",
    "        #标记真实token的位置\n",
    "        mask = (input_ids != self.pad_idx).long() #[B, T]\n",
    "\n",
    "        context, atten_weights = self.attention(H, mask)\n",
    "        out = self.dropout(context)\n",
    "        logits = self.fc(out)               #[B, num_classes]\n",
    "\n",
    "        return logits, atten_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec8b05",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "838726c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_examples = 0, 0, 0\n",
    "\n",
    "    for input_ids, labels in dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #区分两种模型，有无attention\n",
    "        outputs = model(input_ids)\n",
    "        if isinstance(outputs, tuple):\n",
    "            logits, _ = outputs\n",
    "        else:\n",
    "            logits = outputs\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_examples += input_ids.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_examples\n",
    "    avg_acc = total_correct / total_examples\n",
    "\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_examples = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for input_ids, labels in dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids)\n",
    "        if isinstance(outputs, tuple):\n",
    "            logits, _ = outputs\n",
    "        else:\n",
    "            logits = outputs\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_examples += input_ids.size(0)\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    avg_loss = total_loss / total_examples\n",
    "    avg_acc = total_correct / total_examples\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    return avg_loss, avg_acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9fb4f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, train_loader, valid_loader, test_loader, lr=1e-3, epochs=5, model_name=\"model\"):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    best_state_dict = None\n",
    "    best_val_metrics = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        val_loss, val_acc, val_f1 = evaluate(model, valid_loader, criterion)\n",
    "        print(f\"[{model_name}] Epoch: {epoch+1}: \"\n",
    "              f\"trian loss: {train_loss:.4f}, train_acc: {train_acc:.4f},\"\n",
    "              f\"val loss: {val_loss:.4f}, val acc: {val_acc:.4f}, val f1: {val_f1:.4f}\")\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            best_val_metrics = (val_loss, val_acc, val_f1)\n",
    "\n",
    "    #用best checkpoint 在test上评估\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    model.to(device)\n",
    "    test_loss, test_acc, test_f1 = evaluate(model, test_loader, criterion)\n",
    "\n",
    "    print(f\"[{model_name}] BEST on val_f1: \"\n",
    "          f\"val loss: {best_val_metrics[0]:.4f}, val acc: {best_val_metrics[1]:.4f}, val f1: {best_val_metrics[2]:.4f}\")\n",
    "    print(f\"[{model_name}] Test: loss={test_loss:.4f}, acc={test_acc:.4f}, f1={test_f1:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"val_loss\": best_val_metrics[0],\n",
    "        \"val_acc\": best_val_metrics[1],\n",
    "        \"val_f1\": best_val_metrics[2],\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_f1\": test_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49101cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BiLSTMPoolingClassifier] Epoch: 1: trian loss: 0.4995, train_acc: 0.7538,val loss: 0.3651, val acc: 0.8330, val f1: 0.8328\n",
      "[BiLSTMPoolingClassifier] Epoch: 2: trian loss: 0.3096, train_acc: 0.8711,val loss: 0.3280, val acc: 0.8626, val f1: 0.8621\n",
      "[BiLSTMPoolingClassifier] Epoch: 3: trian loss: 0.2160, train_acc: 0.9147,val loss: 0.2997, val acc: 0.8770, val f1: 0.8770\n",
      "[BiLSTMPoolingClassifier] Epoch: 4: trian loss: 0.1477, train_acc: 0.9459,val loss: 0.3279, val acc: 0.8794, val f1: 0.8793\n",
      "[BiLSTMPoolingClassifier] Epoch: 5: trian loss: 0.0911, train_acc: 0.9685,val loss: 0.3878, val acc: 0.8818, val f1: 0.8817\n",
      "[BiLSTMPoolingClassifier] BEST on val_f1: val loss: 0.3878, val acc: 0.8818, val f1: 0.8817\n",
      "[BiLSTMPoolingClassifier] Test: loss=0.4684, acc=0.8516, f1=0.8515\n",
      "[LSTMAttentionClassifier] Epoch: 1: trian loss: 0.5097, train_acc: 0.7352,val loss: 0.3882, val acc: 0.8284, val f1: 0.8273\n",
      "[LSTMAttentionClassifier] Epoch: 2: trian loss: 0.3010, train_acc: 0.8743,val loss: 0.2960, val acc: 0.8742, val f1: 0.8742\n",
      "[LSTMAttentionClassifier] Epoch: 3: trian loss: 0.2068, train_acc: 0.9197,val loss: 0.2971, val acc: 0.8800, val f1: 0.8800\n",
      "[LSTMAttentionClassifier] Epoch: 4: trian loss: 0.1346, train_acc: 0.9509,val loss: 0.3610, val acc: 0.8766, val f1: 0.8765\n",
      "[LSTMAttentionClassifier] Epoch: 5: trian loss: 0.0773, train_acc: 0.9741,val loss: 0.3963, val acc: 0.8732, val f1: 0.8731\n",
      "[LSTMAttentionClassifier] BEST on val_f1: val loss: 0.2971, val acc: 0.8800, val f1: 0.8800\n",
      "[LSTMAttentionClassifier] Test: loss=0.3277, acc=0.8658, f1=0.8658\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "hidden_dim = 128\n",
    "num_classes = 2\n",
    "pad_idx = vocab[PAD]\n",
    "epochs = 5\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "baseline_model = BiLSTMPoolingClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_classes=num_classes,\n",
    "    num_layers=1,\n",
    "    bidirectional=True,\n",
    "    dropout=0.5,\n",
    "    pad_idx=pad_idx,\n",
    "    pooling=\"mean\"\n",
    ")\n",
    "metrics_base = run_experiment(\n",
    "    baseline_model, train_loader, valid_loader, test_loader, lr=1e-3, epochs=epochs, model_name=\"BiLSTMPoolingClassifier\"\n",
    ")\n",
    "\n",
    "attn_model = LSTMAttentionClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_classes=num_classes,\n",
    "    num_layers=1,\n",
    "    bidirectional=True,\n",
    "    dropout=0.5,\n",
    "    pad_idx=pad_idx,\n",
    "    attn_dim=128\n",
    ")\n",
    "metrics_attn = run_experiment(\n",
    "    attn_model, train_loader, valid_loader, test_loader, lr=1e-3, epochs=epochs, model_name=\"LSTMAttentionClassifier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de7e905b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 模型                 | Attention | Val Acc | Val F1 | Test Acc | Test F1 | Test ΔF1 |\n",
      "|----------------------|-----------|---------|--------|----------|---------|----------|\n",
      "| BiLSTM + MeanPooling | ✗         | 0.8818 | 0.8817 | 0.8516 | 0.8515 | 0.0000   |\n",
      "| BiLSTM + Attention   | ✓         | 0.8800 | 0.8800 | 0.8658 | 0.8658 | +0.0143 |\n"
     ]
    }
   ],
   "source": [
    "def print_ablation_table(metrics_base, metrics_attn):\n",
    "    print(\"| 模型                 | Attention | Val Acc | Val F1 | Test Acc | Test F1 | Test ΔF1 |\")\n",
    "    print(\"|----------------------|-----------|---------|--------|----------|---------|----------|\")\n",
    "\n",
    "    base_va, base_vf = metrics_base[\"val_acc\"], metrics_base[\"val_f1\"]\n",
    "    base_ta, base_tf = metrics_base[\"test_acc\"], metrics_base[\"test_f1\"]\n",
    "\n",
    "    attn_va, attn_vf = metrics_attn[\"val_acc\"], metrics_attn[\"val_f1\"]\n",
    "    attn_ta, attn_tf = metrics_attn[\"test_acc\"], metrics_attn[\"test_f1\"]\n",
    "\n",
    "    delta_f1 = attn_tf - base_tf\n",
    "\n",
    "    print(f\"| BiLSTM + MeanPooling | ✗         | {base_va:.4f} | {base_vf:.4f} | {base_ta:.4f} | {base_tf:.4f} | 0.0000   |\")\n",
    "    print(f\"| BiLSTM + Attention   | ✓         | {attn_va:.4f} | {attn_vf:.4f} | {attn_ta:.4f} | {attn_tf:.4f} | {delta_f1:+.4f} |\")\n",
    "\n",
    "print_ablation_table(metrics_base, metrics_attn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
