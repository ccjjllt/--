{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa7c706d",
   "metadata": {},
   "source": [
    "## 1) è®­ç»ƒå¾ªç¯åˆ°åº•åœ¨åšä»€ä¹ˆ\n",
    "\n",
    "ä¸€ä¸ªæ ‡å‡†çš„æ·±åº¦å­¦ä¹ è®­ç»ƒï¼Œæœ¬è´¨æ˜¯åœ¨åå¤æ‰§è¡Œï¼š\n",
    "\n",
    ">ç”¨å½“å‰å‚æ•° Î¸ è®¡ç®—æŸå¤± L(Î¸)ï¼Œæ±‚æ¢¯åº¦ g=âˆ‚L/âˆ‚Î¸ï¼Œç„¶åç”¨ä¼˜åŒ–å™¨æŠŠ Î¸ å¾€è®© L å˜å°çš„æ–¹å‘æ›´æ–°ã€‚\n",
    "\n",
    "è®­ç»ƒå¾ªç¯å°±æ˜¯æŠŠè¿™ä»¶äº‹å·¥ç¨‹åŒ–ã€å¯æ§åŒ–ï¼šå¯æ‰©å±•ï¼ˆå¤§æ•°æ®/å¤šå¡ï¼‰ã€å¯ç¨³å®šï¼ˆä¸ç‚¸æ¢¯åº¦/ä¸ NaNï¼‰ã€å¯å¤ç°ï¼ˆèƒ½é‡è·‘ä¸€è‡´ï¼‰ã€å¯è§‚æµ‹ï¼ˆæ—¥å¿—/æŒ‡æ ‡ï¼‰ã€å¯æ¢å¤ï¼ˆæ–­ç‚¹ç»­è®­ï¼‰ã€‚\n",
    "\n",
    "### 1.1 æœ€å°è®­ç»ƒå¾ªç¯\n",
    "\n",
    "ä»¥ PyTorch æ€ç»´å†™ï¼ˆæ¡†æ¶æ— å…³ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c644e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(E):\n",
    "    model.train()\n",
    "    for x, y in dataloader:\n",
    "        y_hat = model(x)\n",
    "        loss = loss_fn(y_hat, y)    #forward\n",
    "\n",
    "        optimizer.zero_grad()   #æ¸…ç©ºä¸Šä¸€è½®ç´¯è®¡çš„æ¢¯åº¦\n",
    "        loss.backward()         #backwardï¼šæŠŠæ¢¯åº¦å†™åˆ°æ¯ä¸ªå‚æ•°çš„.gradä¸Š\n",
    "        optimizer.step()        #upgradï¼šç”¨.gradæ›´æ–°å‚æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f4d514",
   "metadata": {},
   "source": [
    "## 2) è®­ç»ƒå¾ªç¯çš„å…³é”®å·¥ç¨‹ç»†èŠ‚\n",
    "\n",
    "### 2.1 æ¢¯åº¦ç´¯ç§¯ï¼ˆgradient accumulationï¼‰\n",
    "\n",
    "ç›®çš„ï¼šæ˜¾å­˜ä¸å¤Ÿï¼Œä½†ä½ æƒ³è¦æ›´å¤§çš„ç­‰æ•ˆ batch sizeã€‚\n",
    "\n",
    "åšæ³•ï¼šæŠŠå¤šä¸ª micro-batch çš„æ¢¯åº¦ç´¯åŠ èµ·æ¥ï¼Œæœ€åå† step() ä¸€æ¬¡ã€‚\n",
    "\n",
    "ç­‰æ•ˆ batchï¼šB_effective = B_micro Ã— accum_steps Ã— num_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286dcc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "for step, (x, y) in enumerate(dataloader):\n",
    "    loss = loss_fn(model(x), y) / accum_steps\n",
    "    loss.backward()\n",
    "\n",
    "    if (step + 1) % accum_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed2871",
   "metadata": {},
   "source": [
    "### 2.2 æ¢¯åº¦è£å‰ªï¼ˆgradient clippingï¼‰\n",
    "Transformer è®­ç»ƒå¾ˆå¸¸è§ï¼šæ¢¯åº¦å¶å‘çˆ†ç‚¸ â†’ loss NaNã€‚\n",
    "\n",
    "å…¸å‹åšæ³•ï¼šæŒ‰èŒƒæ•°è£å‰ªï¼ˆglobal normï¼‰ï¼š\n",
    "\n",
    "è‹¥ â€–gâ€–â‚‚ > cï¼Œåˆ™ä»¤ g â† g * (c / â€–gâ€–â‚‚)\n",
    "\n",
    "è¿™ä¼šä¿ç•™æ–¹å‘ã€é™åˆ¶æ­¥é•¿ä¸Šé™ã€‚å¸¸ç”¨ c=1.0ã€‚\n",
    "\n",
    "### 2.3 å­¦ä¹ ç‡è°ƒåº¦ï¼ˆschedulerï¼‰ä¸ warmup\n",
    "å¯¹ Transformer æ¥è¯´ï¼Œwarmup åŸºæœ¬æ˜¯æ ‡é…ï¼šå‰æœŸç”¨å° lrï¼Œè®© Adam çš„äºŒé˜¶çŸ©ä¼°è®¡ç¨³å®šï¼ŒåŒæ—¶é¿å…éšæœºåˆå§‹åŒ–é˜¶æ®µçš„å¤§æ­¥æ›´æ–°æŠŠè®­ç»ƒç‚¸æ‰ã€‚\n",
    "\n",
    "å¸¸è§ç»„åˆï¼š\n",
    "\n",
    "Warmup + Linear Decayï¼ˆæœ€ç»å…¸ï¼ŒBERT/è®¸å¤š finetuneï¼‰\n",
    "\n",
    "Warmup + Cosine Decay\n",
    "\n",
    "é¢„è®­ç»ƒå¤§æ¨¡å‹ï¼šè¿˜ä¼šæœ‰æ›´å¤æ‚çš„åˆ†æ®µ/å¾ªç¯ç­–ç•¥ï¼Œä½†æ ¸å¿ƒä»æ˜¯ warmup + ä¸‹é™ã€‚\n",
    "\n",
    "### 2.4 Mixed Precisionï¼ˆAMPï¼‰ä¸ Loss Scaling\n",
    "FP16/BF16 æé€Ÿçœæ˜¾å­˜ï¼Œä½† FP16 æ˜“ä¸‹æº¢ï¼šæ¢¯åº¦å˜ 0ã€‚\n",
    "\n",
    "è§£å†³ï¼šloss scalingï¼šæŠŠ loss ä¹˜ä¸€ä¸ª scaleï¼Œåä¼ åå†æŠŠæ¢¯åº¦é™¤å›å»ã€‚\n",
    "\n",
    "å·¥ç¨‹ä¸Šç”¨ AMP è‡ªåŠ¨åšï¼ˆå«åŠ¨æ€ scaleï¼‰ï¼Œä½ éœ€è¦èƒ½è®²æ¸…æ¥šâ€œä¸ºä»€ä¹ˆ scale èƒ½é¿å…ä¸‹æº¢â€ã€‚\n",
    "\n",
    "### 2.5 Dropout/è¯„ä¼°æ¨¡å¼åˆ‡æ¢ï¼ˆtrain/evalï¼‰\n",
    "model.train()ï¼šdropout ç”Ÿæ•ˆï¼ˆéšæœºå±è”½ï¼‰ï¼Œè¾“å‡ºå…·æœ‰éšæœºæ€§\n",
    "\n",
    "model.eval()ï¼šdropout å…³é—­ï¼Œè¾“å‡ºç¡®å®šï¼Œç”¨äºéªŒè¯/æµ‹è¯•\n",
    "\n",
    "éªŒè¯è¦é… no_grad()ï¼šä¸å»ºå›¾ï¼Œçœæ˜¾å­˜çœç®—åŠ›\n",
    "\n",
    "### 2.6 æ–­ç‚¹ç»­è®­ï¼ˆcheckpointï¼‰å¿…é¡»ä¿å­˜ä»€ä¹ˆ\n",
    "åªå­˜æ¨¡å‹å‚æ•°ä¸å¤Ÿï¼Œæƒ³â€œæ— ç¼æ¢å¤â€ï¼Œä½ è‡³å°‘è¦å­˜ï¼š\n",
    "\n",
    "model.state_dict()\n",
    "\n",
    "optimizer.state_dict()ï¼ˆAdam é‡Œæœ‰ä¸€å †åŠ¨é‡/äºŒé˜¶çŸ©çŠ¶æ€ï¼‰\n",
    "\n",
    "scheduler.state_dict()ï¼ˆä¸ç„¶ lr æ–­æ¡£ï¼‰\n",
    "\n",
    "å½“å‰ step/epochã€éšæœºç§å­çŠ¶æ€ï¼ˆä¸¥æ ¼å¤ç°æ—¶ï¼‰\n",
    "\n",
    "**ä¸ºä»€ä¹ˆ optimizer state å¾ˆé‡è¦ï¼Ÿ**\n",
    "Adam è¿™ç±»è‡ªé€‚åº”æ–¹æ³•æ¯ä¸ªå‚æ•°éƒ½æœ‰ m, vï¼Œä¸æ¢å¤å°±ç­‰äºæ¢äº†ä¼˜åŒ–å™¨å†å²ï¼Œä¼šå‡ºç°è®­ç»ƒæŠ–åŠ¨ç”šè‡³æ€§èƒ½å›é€€ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f9e0a",
   "metadata": {},
   "source": [
    "## 3) ä¼˜åŒ–å™¨ï¼šä» SGD åˆ° AdamWï¼ŒæŠŠâ€œåŸç† + ç›´è§‰ + é€‚ç”¨åœºæ™¯â€è®²é€\n",
    "### 3.1 SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰\n",
    "æ›´æ–°ï¼š\n",
    "\n",
    ">Î¸ â† Î¸ âˆ’ Î· g\n",
    "\n",
    "ç›´è§‰ï¼šæ²¿ç€å½“å‰ batch çš„æŸå¤±ä¸‹é™æ–¹å‘èµ°ä¸€æ­¥ã€‚\n",
    "\n",
    "ç¼ºç‚¹ï¼šå™ªå£°å¤§ã€æ”¶æ•›æ…¢ã€å¯¹æ¡ä»¶æ•°å·®çš„é—®é¢˜ï¼ˆä¸åŒæ–¹å‘æ›²ç‡å·®å¼‚å¤§ï¼‰å¾ˆæ•æ„Ÿã€‚\n",
    "\n",
    "### 3.2 Momentumï¼ˆåŠ¨é‡ï¼‰\n",
    "æ ¸å¿ƒï¼šå¯¹æ¢¯åº¦åšæŒ‡æ•°æ»‘åŠ¨å¹³å‡ï¼Œè®©æ›´æ–°æ›´â€œæœ‰æƒ¯æ€§â€ã€‚\n",
    "\n",
    "ä¸€ç§å¸¸è§å†™æ³•ï¼š\n",
    "\n",
    ">v â† Î¼ v + g\n",
    "\n",
    ">Î¸ â† Î¸ âˆ’ Î· v\n",
    "\n",
    "ç›´è§‰ï¼šåœ¨ä¸€è‡´æ–¹å‘ä¸ŠåŠ é€Ÿï¼ŒæŠ‘åˆ¶æ¥å›æŠ–åŠ¨ï¼ˆå°¤å…¶åœ¨ç‹­é•¿è°·åº•ï¼‰ã€‚\n",
    "\n",
    "Nesterov Momentumï¼šå…ˆâ€œçœ‹ä¸€çœ¼æœªæ¥ä½ç½®â€å†ç®—æ¢¯åº¦ï¼ˆç»†èŠ‚ç•¥ï¼‰ï¼Œé€šå¸¸ç•¥æ›´ç¨³ã€‚\n",
    "\n",
    "### 3.3 AdaGrad / RMSPropï¼ˆè‡ªé€‚åº”å­¦ä¹ ç‡çš„èµ·ç‚¹ï¼‰\n",
    "**AdaGradï¼šç´¯ç§¯å¹³æ–¹æ¢¯åº¦**\n",
    "\n",
    ">r â† r + gÂ²\n",
    "\n",
    ">Î¸ â† Î¸ âˆ’ Î· * g / (sqrt(r)+Îµ)\n",
    "\n",
    "é—®é¢˜ï¼šr å•è°ƒå¢ï¼Œlr è¶Šæ¥è¶Šå°ï¼ŒåæœŸå¯èƒ½èµ°ä¸åŠ¨ã€‚\n",
    "\n",
    "**RMSPropï¼šç”¨æ»‘åŠ¨å¹³å‡æ›¿ä»£ç´¯ç§¯**\n",
    "\n",
    ">r â† Ï r + (1âˆ’Ï) gÂ²\n",
    "\n",
    ">Î¸ â† Î¸ âˆ’ Î· * g / (sqrt(r)+Îµ)\n",
    "\n",
    "ç›´è§‰ï¼šæŸä¸ªç»´åº¦å¦‚æœæ¢¯åº¦é•¿æœŸå¤§ï¼Œå°±è‡ªåŠ¨ç»™å®ƒæ›´å°çš„æ­¥é•¿ã€‚\n",
    "\n",
    "## 3.4 Adam\n",
    "Adam = Momentumï¼ˆmï¼Œä¸€é˜¶çŸ©ï¼‰ + RMSPropï¼ˆvï¼ŒäºŒé˜¶çŸ©ï¼‰\n",
    "\n",
    ">m â† Î²1 m + (1âˆ’Î²1) g\n",
    "\n",
    ">v â† Î²2 v + (1âˆ’Î²2) gÂ²\n",
    "\n",
    "å› ä¸ºåˆå§‹åŒ– m,v ä» 0 å¼€å§‹ï¼Œå‰æœŸæœ‰åå·®ï¼Œæ‰€ä»¥è¦ bias correctionï¼š\n",
    "\n",
    ">mÌ‚ = m / (1âˆ’Î²1^t)\n",
    "\n",
    ">vÌ‚ = v / (1âˆ’Î²2^t)\n",
    "\n",
    "æœ€ç»ˆæ›´æ–°ï¼š\n",
    "\n",
    ">Î¸ â† Î¸ âˆ’ Î· * mÌ‚ / (sqrt(vÌ‚)+Îµ)\n",
    "\n",
    "**ç›´è§‰ï¼š**\n",
    "\n",
    "mÌ‚ ç»™ä½ â€œç¨³å®šæ–¹å‘â€ï¼ˆé™å™ªï¼‰\n",
    "\n",
    "vÌ‚ ç»™ä½ â€œæŒ‰ç»´åº¦ç¼©æ”¾æ­¥é•¿â€ï¼ˆå¤§æ¢¯åº¦ç»´åº¦å°‘èµ°ç‚¹ï¼‰\n",
    "\n",
    "bias correction è®©å‰å‡ æ­¥ä¸è‡³äºè¢«â€œåˆå§‹åŒ–ä¸º 0â€ä¸¥é‡ä½ä¼°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84abdc6",
   "metadata": {},
   "source": [
    "## 4) AdamWï¼šä¸ºä»€ä¹ˆ Transformer é‡Œå‡ ä¹éƒ½ç”¨å®ƒ\n",
    "### 4.1 L2 æ­£åˆ™ vs Weight Decay çš„å·®åˆ«\n",
    "L2 æ­£åˆ™ï¼šæŠŠ Î»/2 * ||Î¸||Â² åŠ åˆ° loss é‡Œ\n",
    "\n",
    "æ¢¯åº¦ä¼šå˜æˆï¼šg_total = g + Î»Î¸\n",
    "\n",
    "ç„¶åäº¤ç»™ä¼˜åŒ–å™¨ï¼ˆAdamï¼‰å¤„ç†ã€‚\n",
    "\n",
    "Weight Decayï¼ˆæƒé‡è¡°å‡ï¼‰ï¼šæ˜¯â€œæ›´æ–°è§„åˆ™é‡Œç›´æ¥è¡°å‡å‚æ•°â€\n",
    "\n",
    ">Î¸ â† (1 âˆ’ Î·Î») Î¸ âˆ’ Î· * update_from_grad\n",
    "\n",
    "åœ¨ SGD ä¸‹è¿™ä¸¤è€…ç­‰ä»·ï¼ˆå› ä¸ºæ›´æ–°æ˜¯çº¿æ€§çš„ï¼‰ã€‚\n",
    "\n",
    "ä½†åœ¨ Adam ä¸‹ä¸ç­‰ä»·ï¼šå› ä¸º Adam ä¼šå¯¹æ¢¯åº¦åšè‡ªé€‚åº”ç¼©æ”¾ï¼ŒL2 æ­£åˆ™é¡¹ Î»Î¸ ä¹Ÿä¼šè¢«ç¼©æ”¾ï¼Œå¯¼è‡´â€œä¸åŒå‚æ•°è¢«ä¸ä¸€è‡´åœ°æ­£åˆ™åŒ–â€ï¼Œè¿™ä¼šå½±å“æ³›åŒ–ã€‚\n",
    "\n",
    "åœ¨ SGD ä¸‹è¿™ä¸¤è€…ç­‰ä»·ï¼ˆå› ä¸ºæ›´æ–°æ˜¯çº¿æ€§çš„ï¼‰ã€‚\n",
    "\n",
    "ä½†åœ¨ Adam ä¸‹ä¸ç­‰ä»·ï¼šå› ä¸º Adam ä¼šå¯¹æ¢¯åº¦åšè‡ªé€‚åº”ç¼©æ”¾ï¼ŒL2 æ­£åˆ™é¡¹ Î»Î¸ ä¹Ÿä¼šè¢«ç¼©æ”¾ï¼Œå¯¼è‡´â€œä¸åŒå‚æ•°è¢«ä¸ä¸€è‡´åœ°æ­£åˆ™åŒ–â€ï¼Œè¿™ä¼šå½±å“æ³›åŒ–ã€‚\n",
    "\n",
    "**AdamW åšçš„äº‹ï¼šæŠŠ weight decay ä»æ¢¯åº¦é‡Œâ€œè§£è€¦â€å‡ºæ¥ï¼Œç¡®ä¿è¡°å‡è¡Œä¸ºä¸è¢«è‡ªé€‚åº”ç¼©æ”¾æ‰­æ›²ã€‚**\n",
    "\n",
    ">é¢è¯•ä¸€å¥è¯æ€»ç»“ï¼šAdamW = Adam + decoupled weight decayï¼Œè§£å†³ Adam ä¸­ L2 æ­£åˆ™è¢«è‡ªé€‚åº”ç¼©æ”¾å¯¼è‡´çš„éé¢„æœŸæ­£åˆ™é—®é¢˜ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91794043",
   "metadata": {},
   "source": [
    "## 5) Transformer/NLP è®­ç»ƒçš„é»˜è®¤â€œé è°±é…ç½®â€\n",
    "### 5.1 å¾®è°ƒï¼ˆfinetuneï¼‰BERT/LLM encoder/decoder\n",
    "optimizerï¼šAdamW\n",
    "\n",
    "lrï¼šencoder ç±»å¸¸è§ 1e-5 ~ 5e-5ï¼›æ›´å¤§æ¨¡å‹/æ›´æ•æ„Ÿä»»åŠ¡å¾€å°\n",
    "\n",
    "betasï¼š(0.9, 0.999)\n",
    "\n",
    "epsï¼š1e-8\n",
    "\n",
    "weight_decayï¼š0.01ï¼ˆå¸¸è§ï¼‰\n",
    "\n",
    "schedulerï¼šwarmup(æ¯”ä¾‹ 0.06~0.1) + linear decay\n",
    "\n",
    "grad_clipï¼š1.0\n",
    "\n",
    "ä¸€èˆ¬ä¼šå¯¹ bias å’Œ LayerNorm æƒé‡ ä¸åš weight decayï¼ˆå·¥ç¨‹æƒ¯ä¾‹ï¼‰\n",
    "\n",
    "### ä»å¤´é¢„è®­ç»ƒï¼ˆpretrainï¼‰\n",
    "ä»å¸¸è§ AdamW/Adafactor/LAMB ç­‰ï¼ˆçœ‹è§„æ¨¡ä¸æ˜¾å­˜ï¼‰\n",
    "\n",
    "æ›´å¤§ batch + æ›´é•¿ warmup\n",
    "\n",
    "æ›´å¼ºè°ƒååï¼ˆAMPã€æ¢¯åº¦ç´¯ç§¯ã€åˆ†å¸ƒå¼ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a400ef3",
   "metadata": {},
   "source": [
    "## æ‰‹å†™ Adam / AdamW æ›´æ–°å…¬å¼\n",
    "è®¾ç¬¬ ğ‘¡ æ¬¡æ›´æ–°æ—¶ï¼Œå‚æ•°ä¸º ğœƒ_ğ‘¡ï¼Œå½“å‰ mini-batch çš„æ¢¯åº¦ä¸º\n",
    "$$g_{t}=\\nabla_{\\theta} L\\left(\\theta_{t}\\right)$$\n",
    "\n",
    "è¶…å‚ï¼šå­¦ä¹ ç‡$\\alpha, \\beta_{1}, \\beta_{2} \\in(0,1), \\epsilon>0 .$\n",
    "\n",
    "**Adam**\n",
    "ä¸€é˜¶çŸ©ï¼ˆåŠ¨é‡ï¼‰\n",
    "$$m_{t}=\\beta_{1} m_{t-1}+\\left(1-\\beta_{1}\\right) g_{t}$$\n",
    "\n",
    "äºŒé˜¶çŸ©ï¼ˆå¹³æ–¹æ¢¯åº¦çš„æ»‘åŠ¨å¹³å‡ï¼‰\n",
    "$$v_{t}=\\beta_{2} v_{t-1}+\\left(1-\\beta_{2}\\right) g_{t}^{2}$$\n",
    "\n",
    "è¿™é‡Œ$g_{t}^{2}$æ˜¯æŒ‰å…ƒç´ å¹³æ–¹\n",
    "\n",
    "Bias correctionï¼ˆå…³é”®ï¼‰\n",
    "$$\\hat{m}_{t}=\\frac{m_{t}}{1-\\beta_{1}^{t}}, \\quad \\hat{v}_{t}=\\frac{v_{t}}{1-\\beta_{2}^{t}}$$\n",
    "\n",
    "å‚æ•°æ›´æ–°\n",
    "$$\\theta_{t+1}=\\theta_{t}-\\alpha \\frac{\\hat{m}_{t}}{\\sqrt{\\hat{v}_{t}}+\\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3850856",
   "metadata": {},
   "source": [
    "AdamWï¼ˆDecoupled Weight Decay å•ç‹¬å†™ä¸€é¡¹ï¼‰\n",
    "\n",
    "è®¾ weight decay ç³»æ•° Î»ã€‚\n",
    "\n",
    "AdamW çš„æ ¸å¿ƒæ˜¯ï¼šæŠŠæƒé‡è¡°å‡ä»æ¢¯åº¦é‡Œæ‹¿å‡ºæ¥ï¼Œä¸å‚ä¸ Adam çš„è‡ªé€‚åº”ç¼©æ”¾ã€‚\n",
    "\n",
    "å› æ­¤æ›´æ–°å†™æˆâ€œä¸¤éƒ¨åˆ†ç›¸åŠ â€æœ€æ¸…æ¥šï¼š\n",
    "\n",
    "**(A) å…ˆåš decoupled weight decayï¼ˆè¡°å‡é¡¹ï¼‰**\n",
    "$$\\theta_{t}^{\\prime}=\\theta_{t}-\\alpha \\lambda \\theta_{t}$$\n",
    "\n",
    "**(B) å†åš Adam çš„è‡ªé€‚åº”æ¢¯åº¦æ›´æ–°ï¼ˆç”¨åŒæ ·çš„ ğ‘š,ğ‘£ä¸ bias correctionï¼‰**\n",
    "$$\\begin{array}{c}m_{t}=\\beta_{1} m_{t-1}+\\left(1-\\beta_{1}\\right) g_{t}, \\quad v_{t}=\\beta_{2} v_{t-1}+\\left(1-\\beta_{2}\\right) g_{t}^{2} \\\\\\hat{m}_{t}=\\frac{m_{t}}{1-\\beta_{1}^{t}}, \\quad \\hat{v}_{t}=\\frac{v_{t}}{1-\\beta_{2}^{t}} \\\\\\theta_{t+1}=\\theta_{t}^{\\prime}-\\alpha \\frac{\\hat{m}_{t}}{\\sqrt{\\hat{v}_{t}}+\\epsilon}\\end{array}$$\n",
    "\n",
    "æ³¨æ„ï¼šè¿™é‡Œçš„ ğœ†ğœƒ_ğ‘¡æ˜¯â€œç›´æ¥åŠ åˆ°æ›´æ–°é‡Œâ€ï¼Œä¸æ˜¯åŠ åˆ°æ¢¯åº¦é‡Œè®© Adam ç¼©æ”¾ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70886735",
   "metadata": {},
   "source": [
    "å†™ä¸€ä¸ªä½ è‡ªå·±çš„è®­ç»ƒå¾ªç¯ä»£ç ï¼Œå¿…é¡»åŒ…å«ï¼š \n",
    "\n",
    "grad accumulation \n",
    "\n",
    "AMPï¼ˆå« scaler/unscaleï¼‰ \n",
    "\n",
    "grad clipping \n",
    "\n",
    "scheduler \n",
    "\n",
    "eval åˆ‡æ¢ä¸ no_grad \n",
    "\n",
    "checkpoint ä¿å­˜ optimizer/scheduler çŠ¶æ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f094d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def save_checkpoint(ckpt_path, model, optimizer, scheduler, scaler, epoch, global_step, best_metric):\n",
    "    state = {\n",
    "        \"model\": model.state_dict(),    #ä¿å­˜æ¨¡å‹å‚æ•°ï¼ˆæƒé‡ï¼‰\n",
    "        \"optimizer\": optimizer.state_dict(),    #ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆAdamwçš„m/vç­‰ï¼‰\n",
    "        \"scheduler\": scheduler.state_dict() if scheduler else None, #ä¿å­˜å­¦ä¹ ç‡è°ƒåº¦çŠ¶æ€ï¼ˆé¿å…lræ–­æ¡£ï¼‰\n",
    "        \"scaler\": scaler.state_dict() if scaler else None,  #ä¿å­˜AMPçš„scalerçŠ¶æ€ï¼ˆåŠ¨æ€lossing scalerï¼‰\n",
    "        \"epoch\": epoch,\n",
    "        \"global_step\": global_step, #ä¿å­˜å…¨å±€stepï¼ˆè®°å½•è®­ç»ƒè¿­ä»£ï¼‰\n",
    "        \"best_metric\": best_metric, #ä¿å­˜æœ€ä¼˜æŒ‡æ ‡ï¼ˆç”¨äºearly step / best modelï¼‰\n",
    "        \"rng_state_cpu\": torch.get_rng_state(), #ä¿å­˜cpuçš„éšæœºæ•°çŠ¶æ€\n",
    "        \"rng_state_cuda\": torch.cuda.get_rng_state_all() if torch.cuda.is_available else None, #ä¿å­˜cudaçš„éšæœºæ•°çŠ¶æ€\n",
    "    }\n",
    "    os.makedirs(os.path.dirname(ckpt_path), exist_ok=True)  #ç¡®ä¿checkpointè·¯å¾„å­˜åœ¨\n",
    "    torch.save(state, ckpt_path)    #å°†stateåºåˆ—åŒ–åˆ°ç£ç›˜\n",
    "\n",
    "def load_checkpoint(ckpt_path, model, optimizer, scheduler, scaler, map_location=\"cpu\"):    \n",
    "    state = torch.load(ckpt_path, map_location=map_location)\n",
    "    model.load_state_dict(state[\"model\"])   #æ¢å¤æ¨¡å‹æƒé‡\n",
    "    optimizer.load_state_dict(state[\"optimizer\"])   #æ¢å¤ä¼˜åŒ–å™¨å†…éƒ¨çŠ¶æ€ï¼ˆm/vï¼Œstepç­‰ï¼‰\n",
    "    if scheduler and state.get(\"scheduler\") is not None:\n",
    "        scheduler.load_state_dict(state[\"scheduler\"])   #æ¢å¤schedulerçŠ¶æ€ï¼ˆå½“å‰lrï¼Œlast_epochç­‰ï¼‰\n",
    "    if scaler and state.get(\"scaler\") is not None:  #å¦‚æœå¯ç”¨AMPï¼Œä¸”ckptä¸­ä¿å­˜äº†scaler\n",
    "        scaler.load_state_dict(state[\"scaler\"]) #æ¢å¤scalerçŠ¶æ€ï¼ˆåŠ¨æ€scaleçš„å€¼ï¼‰\n",
    "    if state.get(\"rng_state_cpu\") is not None:\n",
    "        torch.set_rng_state(state[\"rng_state_cpu\"]) #æ¢å¤rng cpuçŠ¶æ€ï¼ˆå¤ç°å®éªŒç”¨ï¼‰\n",
    "    if torch.cuda.is_available() and state.get(\"rng_state_cuda\") is not None:\n",
    "        torch.cuda.set_rng_state_all(state[\"rng_state_cuda\"])   #æ¢å¤æ‰€æœ‰GPUçš„rngçŠ¶æ€\n",
    "    start_epoch = state.get(\"epoch\", 0) #ä»ckptè¯»å–åˆå§‹çš„epochï¼Œé»˜è®¤æ˜¯0\n",
    "    global_step = state.get(\"global_step\", 0)   #ä»ckptè¯»å–å…¨å±€çš„stepï¼Œé»˜è®¤æ˜¯0\n",
    "    best_metric = state.get(\"best_metric\", None)    #ä»ckptè¯»å–æœ€ä¼˜æŒ‡æ ‡ï¼Œå¯ä¸ºç©º\n",
    "    return start_epoch, global_step, best_metric\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "    for batch in val_loader:\n",
    "        batch = move_batch_to_device(batch, device) #å°†batchç§»åŠ¨åˆ°GPU/CPU\n",
    "        loss = compute_loss(model, batch)   #è®¡ç®—éªŒè¯loss\n",
    "        bs = get_batch_size(batch)  #è·å–batch size\n",
    "        total_loss += loss.item() * bs  #æŒ‰æ ·æœ¬åŠ æƒç´¯åŠ ï¼ˆé¿å…æœ€åä¸€æ‰¹å¤§å°ä¸åŒå¯¼è‡´åå·®ï¼‰\n",
    "        total_count += bs   #ç´¯åŠ æ ·æœ¬æ•°\n",
    "    avg_loss = total_loss / max(total_count, 1)\n",
    "    metric = -avg_loss  # ç¤ºä¾‹ï¼šæŠŠæŒ‡æ ‡è®¾ä¸º -lossï¼ˆä½ å¯ä»¥æ¢æˆ F1/EM/Accï¼‰\n",
    "    model.train()\n",
    "    return {\"val_loss\": avg_loss, \"metric\": metric}\n",
    "\n",
    "def trian(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf87eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'Moudle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 47\u001b[0m\n\u001b[0;32m     42\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(ckpt, path)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#å·¥å…·ï¼šåŠ è½½ checkpointï¼ˆæ–­ç‚¹ç»­è®­â€œæ— ç¼â€æ¢å¤ï¼‰\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_checkpoint\u001b[39m(\n\u001b[0;32m     46\u001b[0m         path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m---> 47\u001b[0m         model: \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMoudle\u001b[49m,\n\u001b[0;32m     48\u001b[0m         optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m     49\u001b[0m         scheduler: Optional[torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39m_LRscheduler],\n\u001b[0;32m     50\u001b[0m         scaler: Optional[torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mGradScaler],\n\u001b[0;32m     51\u001b[0m         map_location: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     52\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m     53\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(path, map_location\u001b[38;5;241m=\u001b[39mmap_location)  \u001b[38;5;66;03m#è¯»å–checkpoint\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;66;03m#æ¢å¤æ¨¡å‹å‚æ•°\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'Moudle'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#é…ç½®ï¼šæŠŠè®­ç»ƒå¾ªç¯é‡Œéœ€è¦çš„è¶…å‚ä¸è·¯å¾„é›†ä¸­ç®¡ç†\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    device: str = \"cuda\" if torch.cuda.is_available else \"cpu\"  #è®­ç»ƒè®¾å¤‡\n",
    "    epochs: int = 3 #è®­ç»ƒä¼¦æ•°\n",
    "    grad_accum_steps: int = 4   #æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆmicro-batch ç´¯åˆ°è¿™ä¸€æ­¥æ‰ stepï¼‰\n",
    "    max_grad_norm: float = 1.0  #å…¨å±€èŒƒæ•°è£å‰ªé˜ˆå€¼ï¼ˆTransformer å¸¸ç”¨ 1.0ï¼‰\n",
    "    log_every: int = 50 #æ‰“å°/è®°å½•é¢‘ç‡ï¼ˆæŒ‰ micro-stepï¼‰\n",
    "    eval_every_updates: int = 200   #æ¯å¤šå°‘ä¸ªâ€œoptimizeræ›´æ–°æ­¥â€åšä¸€æ¬¡éªŒè¯\n",
    "    ckpt_dir: str = \"./crpt\"    #checkpointç›®å½•\n",
    "    ckpt_name: str = \"latest.pt\"    #checkpointæ–‡ä»¶å\n",
    "    use_amp: bool = True    #æ˜¯å¦å¼€å¯AMPï¼ˆfp16/bf16ï¼‰\n",
    "    amp_dtype: str = \"fp16\" #\"fp16\" or \"bf16\"ï¼ˆbf16 é€šå¸¸ä¸éœ€è¦ scalerï¼‰\n",
    "\n",
    "#å·¥å…·ï¼šä¿å­˜ checkpointï¼ˆå¿…é¡»ä¿å­˜ optimizer/scheduler çŠ¶æ€ï¼‰\n",
    "def save_checkpoint(\n",
    "        path: str,\n",
    "        model: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler],\n",
    "        scaler: Optional[torch.cuda.amp.GradScaler],\n",
    "        extra: Dict[str, Any],\n",
    ") -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)   #ç¡®ä¿ç›®å½•å­˜åœ¨\n",
    "    ckpt = {\n",
    "        \"model\": model.state_dict(),    #ä¿å­˜æ¨¡å‹å‚æ•°\n",
    "        \"optimizer\": optimizer.state_dict(),    #ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆAdamW çš„ m/v ç­‰éƒ½åœ¨è¿™é‡Œï¼‰\n",
    "        \"scheduler\": scheduler.state_dict() if scheduler is not None else None, #ä¿å­˜å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€\n",
    "        \"scaler\": scaler.state_dict() if scaler is not None else None,  #ä¿å­˜ AMP scaler çŠ¶æ€ï¼ˆfp16 åŠ¨æ€ç¼©æ”¾éœ€è¦ï¼‰\n",
    "        \"extra\": extra, #ä¿å­˜ä½ è‡ªå®šä¹‰çš„è®­ç»ƒè¿›åº¦/æŒ‡æ ‡\n",
    "        \"rng_cpu\": torch.get_rng_state(),   #ä¿å­˜ CPU éšæœºæ•°çŠ¶æ€ï¼ˆå¯å¤ç°ï¼‰\n",
    "        \"rng_cuda\": torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,  #ä¿å­˜cudaéšæœºæ•°çŠ¶æ€\n",
    "    }\n",
    "    torch.save(ckpt, path)\n",
    "\n",
    "#å·¥å…·ï¼šåŠ è½½ checkpointï¼ˆæ–­ç‚¹ç»­è®­â€œæ— ç¼â€æ¢å¤ï¼‰\n",
    "def load_checkpoint(\n",
    "        path: str,\n",
    "        model: nn.Moudle,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: Optional[torch.optim.lr_scheduler._LRscheduler],\n",
    "        scaler: Optional[torch.cuda.amp.GradScaler],\n",
    "        map_location: str = \"cpu\",\n",
    ") -> Dict[str, Any]:\n",
    "    ckpt = torch.load(path, map_location=map_location)  #è¯»å–checkpoint\n",
    "    model.load_state_dict(ckpt[\"model\"]) #æ¢å¤æ¨¡å‹å‚æ•°\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer\"])    #æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆéå¸¸å…³é”®ï¼‰\n",
    "    if scheduler and ckpt.get(\"scheduler\") is None:\n",
    "        scheduler.load_state_dict(ckpt[\"scheduler\"])    #æ¢å¤ schedulerï¼ˆé¿å… lr æ–­æ¡£ï¼‰\n",
    "    if scaler and ckpt.get(\"scaler\") is None:\n",
    "        scaler.load_state_dict(ckpt[\"scaler\"])  #æ¢å¤ scalerï¼ˆé¿å…åŠ¨æ€ç¼©æ”¾è·³å˜ï¼‰\n",
    "    \n",
    "    # æ¢å¤ RNGï¼ˆéœ€è¦ä¸¥æ ¼å¤ç°æ—¶å¾ˆé‡è¦ï¼›ä¸è¦æ±‚ä¹Ÿå¯ä»¥ä¸åšï¼‰\n",
    "    if ckpt.get(\"rng_cpu\") is not None:\n",
    "        torch.set_rng_state(ckpt[\"rng_cpu\"])\n",
    "    if ckpt.get(\"rng_cuda\") is not None:\n",
    "        torch.cuda.set_rng_state_all(ckpt[\"rng_cuda\"])\n",
    "\n",
    "    return ckpt.get(\"extra\", {})    #è¿”å›ä½ ä¿å­˜çš„â€œè®­ç»ƒè¿›åº¦â€ç­‰ä¿¡æ¯\n",
    "\n",
    "#å·¥å…·ï¼šéªŒè¯ï¼ˆå¿…é¡» eval() + no_gradï¼‰\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, val_loader: DataLoader, device: str) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0    #å¯é€‰ï¼šæŒ‰ token å½’ä¸€åŒ–ï¼ˆNLP å¸¸ç”¨ï¼‰\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=-100)    #ç¤ºä¾‹ï¼šLM/seq2seq å¸¸ç”¨ ignore_index\n",
    "\n",
    "    for batch in val_loader:\n",
    "        #å‡è®¾ batch é‡Œæœ‰ input_ids, labels\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        #logits: [B, T, V]ï¼Œlabels: [B, T] -> reshape è®¡ç®— token-level CE\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        #ç»Ÿè®¡ï¼šè¿™é‡Œç®€å•æŒ‰ batch ç´¯åŠ ï¼›ä½ ä¹Ÿå¯ä»¥æŒ‰ tokens è®¡\n",
    "        total_loss += loss\n",
    "        total_tokens += labels.numel()  #tokenæ•°ï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "    model.train()                                                                 # è¯„ä¼°ç»“æŸåˆ‡å› trainï¼ˆé˜²æ­¢åç»­è®­ç»ƒå¿˜äº†ï¼‰\n",
    "    return {\n",
    "        \"val_loss\": total_loss / max(1, len(val_loader)),                         # å¹³å‡éªŒè¯ loss\n",
    "    }\n",
    "\n",
    "#è®­ç»ƒå¾ªç¯ï¼šåŒ…å« grad accumulation + AMP(scaler/unscale) + grad clipping + scheduler + eval/no_grad + checkpoint\n",
    "def train_loop(\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: Optional[DataLoader],\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler],\n",
    "        cfg: TrainConfig,\n",
    "        resume: bool = True\n",
    ") -> None:\n",
    "    device = cfg.device\n",
    "    model.to(device)\n",
    "\n",
    "    #AMP autocast çš„ dtypeï¼šfp16 æˆ– bf16ï¼ˆbf16 ä¸€èˆ¬æ— éœ€ GradScalerï¼Œä½†ä¸ºäº†ç»Ÿä¸€æ¥å£ä¹Ÿå¯ä»¥ä¸ç”¨ scalerï¼‰\n",
    "    use_fp16 = cfg.use_amp and cfg.amp_dtype.lower() == \"fp16\" and device.startswith(\"cuda\")\n",
    "    use_bf16 = cfg.use_amp and cfg.amp_dtype.lower() == \"bf16\" and device.startswith(\"cuda\")\n",
    "\n",
    "    #GradScalerï¼šä»… fp16 åœºæ™¯å¿…éœ€ï¼ˆbf16 é€šå¸¸ä¸éœ€è¦ loss scalingï¼‰\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=\"fp16\")  #fp16å¼€å¯åŠ¨æ€ç¼©æ”¾ï¼Œå¦åˆ™ç¦ç”¨\n",
    "\n",
    "    #æ–­ç‚¹ç»­è®­ï¼šåŠ è½½æ¨¡å‹/ä¼˜åŒ–å™¨/scheduler/scaler ä»¥åŠ global_step ç­‰è®­ç»ƒè¿›åº¦\n",
    "    ckpt_path = os.path.join(cfg.ckpt_dir, cfg.ckpt_name)   #checkpointè·¯å¾„\n",
    "    extra = {\"epoch\": 0, \"update_step\": 0, \"micro_step\": 0, \"best_metric\": 1e-9}    #åˆå§‹åŒ–è®­ç»ƒè¿›åº¦\n",
    "    if resume and os.path.exists(ckpt_path):    #å¦‚æœéœ€è¦resumeä¸”å­˜åœ¨ckpt\n",
    "        extra = load_checkpoint(                #æ¢å¤æ‰€æœ‰çŠ¶æ€\n",
    "            ckpt_path, model, optimizer, scheduler, scaler, map_location=\"cpu\"\n",
    "        )\n",
    "\n",
    "    #æŸå¤±å‡½æ•°ï¼šç¤ºä¾‹ä¸º token-level CEï¼›åˆ†ç±»ä»»åŠ¡æ›¿æ¢å³å¯\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=-100)    #å¿½ç•¥paddingçš„token\n",
    "\n",
    "    update_step =  int(extra.get(\"update_step\", 0))    #optimizer æ›´æ–°æ­¥ï¼ˆæ¯ accum å®Œæ‰ +1ï¼‰\n",
    "    micro_step = int(extra.get(\"micro_step\", 0))    #micro-batchæ­¥ï¼ˆæ¯ä¸ªbatch+1ï¼‰\n",
    "    best_metric = int(extra.get(\"best_metric\", 1e-9))   #è®°å½•æœ€å¥½æŒ‡æ ‡ï¼ˆå¦‚æœ€å° val_lossï¼‰\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)   #æ¸…ç©ºæ¢¯åº¦ï¼ˆset_to_none æ›´çœå†…å­˜ï¼‰\n",
    "\n",
    "    for epoch in range(int(extra.get(\"epoch\", 0)), cfg.epochs): #ä»æ¢å¤çš„epochå¼€å§‹ç»§ç»­\n",
    "        for batch in train_loader:\n",
    "            micro_step += 1 #micro-step +1ï¼ˆæ¯ä¸ª batchï¼‰\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # 2) AMP autocastï¼šåœ¨ fp16/bf16 ä¸‹ç”¨ä½ç²¾åº¦è®¡ç®— forwardï¼Œæé€Ÿçœæ˜¾å­˜\n",
    "            #    æ³¨æ„ï¼šautocast åªå½±å“ forward/éƒ¨åˆ†ç®—å­ï¼›backward ä»ç”± scaler ç®¡\n",
    "            autocast_enabled = (use_fp16 or use_bf16)   #æ˜¯å¦ä½¿ç”¨autocast\n",
    "            autocast_dtype = torch.float16 if use_fp16 else (torch.bfloat16 if use_bf16 else None) \n",
    "\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=autocast_dtype, enabled=autocast_enabled):\n",
    "                logits = model(input_ids)   # forwardï¼šå¾—åˆ° logits\n",
    "                loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))   #è®¡ç®— lossï¼ˆtoken-levelï¼‰\n",
    "                loss = loss / cfg.grad_accum_steps  #å…³é”®ï¼šæ¢¯åº¦ç´¯ç§¯æ—¶è¦é™¤ä»¥ accum_steps\n",
    "\n",
    "            #3) backwardï¼šfp16 ç”¨ scaler.scale(loss).backward() é¿å…æ¢¯åº¦ä¸‹æº¢ï¼›é fp16 ç›´æ¥ backward\n",
    "            if use_fp16:\n",
    "                scaler.sacle(loss).backward()   #åä¼ ï¼ˆloss è¢«æ”¾å¤§ï¼Œæ¢¯åº¦éšä¹‹æ”¾å¤§\n",
    "            else:\n",
    "                loss.backward() #åä¼ ï¼ˆbf16/fp32 ç›´æ¥åšï¼‰\n",
    "            \n",
    "            #4) æ¢¯åº¦ç´¯ç§¯è¾¹ç•Œï¼šå‡‘å¤Ÿ grad_accum_steps æ‰åšä¸€æ¬¡å‚æ•°æ›´æ–°\n",
    "            if micro_step % cfg.grad_accum_steps == 0:\n",
    "                #4.1) unscaleï¼šæŠŠæ¢¯åº¦ä»â€œæ”¾å¤§åçš„å°ºåº¦â€è¿˜åŸå›çœŸå®å°ºåº¦ï¼Œæ‰èƒ½æ­£ç¡®è£å‰ª/æ£€æµ‹ NaN/Inf\n",
    "                if use_fp16:\n",
    "                    scaler.unscale_(optimizer)  #è®©optimizeré‡Œçš„å‚æ•°gradä¾¿ä¼šçœŸå®å€¼\n",
    "                \n",
    "                #4.2) grad clippingï¼šå…¨å±€èŒƒæ•°è£å‰ªï¼Œé˜²æ­¢å¶å‘æ¢¯åº¦çˆ†ç‚¸ï¼ˆTransformer å¸¸ç”¨ï¼‰\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), cfg.max_grad_norm   \n",
    "                )   #è¿”å›è£å‰ªå‰ï¼ˆæˆ–åï¼‰çš„èŒƒæ•°ï¼Œä¾¿äºæ—¥å¿—\n",
    "\n",
    "                #4.3) optimizer stepï¼šfp16 ç”¨ scaler.stepï¼›å¦åˆ™æ™®é€š step\n",
    "                if use_fp16:\n",
    "                    scaler.step(optimizer)  #è‹¥æ£€æµ‹åˆ° inf/nanï¼Œå†…éƒ¨ä¼šè·³è¿‡ step\n",
    "                    scaler.update()     #æ›´æ–°åŠ¨æ€scaleï¼ˆæ›´ç¨³ï¼‰\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "\n",
    "                #4.4) scheduler stepï¼šé€šå¸¸ Transformer ä»¥â€œæ¯ä¸ª update_stepâ€æ›´æ–° lr\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()    #lr è°ƒåº¦ï¼ˆwarmup/decay åœ¨è¿™é‡Œå‘ç”Ÿï¼‰\n",
    "\n",
    "                #5) æ—¥å¿—ï¼šå»ºè®®è®°å½• update_step å¯¹é½çš„æŒ‡æ ‡ï¼ˆlossã€lrã€grad_normï¼‰\n",
    "                if update_step % max(1, cfg.eval_every_updates // 10) == 0:\n",
    "                    lr = optimizer.param_groups[0][lr]  #è·å–å½“å‰ lrï¼ˆç¤ºä¾‹å–ç¬¬ä¸€ä¸ª param groupï¼‰\n",
    "                    print(f\"[epoch={epoch} upd={update_step}] lr={lr:.3e} \"\n",
    "                          f\"loss={(loss.item()*cfg.grad_accum_steps):.4f} grad_norm={float(grad_norm):.3f}\")\n",
    "                    \n",
    "                #6)evalï¼šå¿…é¡» model.eval() + no_gradï¼ˆevaluate é‡Œå·²ç»åšäº†ï¼‰\n",
    "                if val_loader is not None and (update_step % cfg.eval_every_updates == 0):\n",
    "                    metrics = evaluate(model, val_loader, device)   #è¯„ä¼°ï¼ˆå†…éƒ¨ no_grad + evalï¼‰\n",
    "                    val_loss = metrics[\"val_loss\"]\n",
    "\n",
    "                    #7) checkpointï¼šä¿å­˜ latestï¼ˆå¹¶å¯æŒ‰ best ä¿å­˜ä¸€ä»½ï¼‰\n",
    "                    extra = {                                                     # æŠŠè®­ç»ƒè¿›åº¦å†™è¿› extra\n",
    "                        \"epoch\": epoch,\n",
    "                        \"update_step\": update_step,\n",
    "                        \"micro_step\": micro_step,\n",
    "                        \"best_metric\": best_metric,\n",
    "                        \"last_val_loss\": val_loss,\n",
    "                    }\n",
    "                    save_checkpoint(ckpt_path, model, optimizer, scheduler, scaler, extra)  #ä¿å­˜åŒ…å« optimizer/scheduler/scaler çš„ ckpt\n",
    "\n",
    "                    #8) best modelï¼ˆç¤ºä¾‹ï¼šval_loss è¶Šå°è¶Šå¥½ï¼‰\n",
    "                    if val_loss < best_metric or best_metric < 0:\n",
    "                        best_metric = val_loss\n",
    "                        best_path = os.path.join(cfg.ckpt_dir, \"best.pt\")   #best extra è·¯å¾„\n",
    "                        extra[\"best_metric\"] = best_metric                        # å†™å› extra\n",
    "                        save_checkpoint(best_path, model, optimizer, scheduler, scaler, extra)  # ä¿å­˜ best\n",
    "\n",
    "        # æ¯ä¸ª epoch ç»“æŸä¹Ÿå»ºè®®å­˜ä¸€æ¬¡ï¼ˆé˜²æ­¢ val_loader ä¸ºç©ºæ—¶å®Œå…¨ä¸å­˜ï¼‰\n",
    "        extra = {\n",
    "            \"epoch\": epoch + 1,                                                   # ä¸‹æ¬¡ä»ä¸‹ä¸€è½®å¼€å§‹\n",
    "            \"update_step\": update_step,\n",
    "            \"micro_step\": micro_step,\n",
    "            \"best_metric\": best_metric,\n",
    "        }\n",
    "        save_checkpoint(ckpt_path, model, optimizer, scheduler, scaler, extra)    # epoch end checkpoint\n",
    "\n",
    "# -----------------------------\n",
    "# ç”¨æ³•æç¤ºï¼ˆä½ åœ¨è‡ªå·±å·¥ç¨‹é‡Œæ›¿æ¢ model/loader/optimizer/scheduler å³å¯ï¼‰\n",
    "# -----------------------------\n",
    "\"\"\"\n",
    "ä½ éœ€è¦è‡ªå·±æä¾›ï¼š\n",
    "- modelï¼šforward(input_ids) -> logits\n",
    "- train_loader / val_loaderï¼šbatch è‡³å°‘åŒ…å« \"input_ids\" å’Œ \"labels\"\n",
    "- optimizerï¼štorch.optim.AdamW\n",
    "- schedulerï¼šä¾‹å¦‚ transformers é‡Œçš„ get_linear_schedule_with_warmup æˆ–è‡ªå®šä¹‰ lr_scheduler\n",
    "\n",
    "å…³é”®ç‚¹å·²å…¨éƒ¨è¦†ç›–ï¼š\n",
    "âœ… grad accumulation: loss/accum_steps + micro_step%accum==0 æ‰ step\n",
    "âœ… AMP: autocast + GradScaler(scale/backward, unscale_, step, update)\n",
    "âœ… grad clipping: clip_grad_norm_\n",
    "âœ… scheduler: æ¯æ¬¡ update_step å scheduler.step()\n",
    "âœ… eval åˆ‡æ¢ä¸ no_grad: evaluate() å†… model.eval() + @torch.no_grad()\n",
    "âœ… checkpoint: ä¿å­˜/åŠ è½½ model+optimizer+scheduler(+scaler) + step/epoch\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
