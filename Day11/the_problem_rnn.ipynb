{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86427675",
   "metadata": {},
   "source": [
    "# RNN 的致命问题\n",
    "\n",
    "## (1)vanilla RNN公式\n",
    "$$\\begin{array}{c}\n",
    "h_{t}=\\tanh \\left(W_{h} h_{t-1}+W_{x} x_{t}+b\\right) \\\\\n",
    "y_{t}=W_{y} h_{t}\n",
    "\\end{array}$$\n",
    "\n",
    "随着输入序列的增大，h_t中包含较前的信息就越来越少。信息会随着时间步的增多而逐渐丢失，无法补捉长距离依赖，而有的语句恰恰是距离很远的地方起到了关键作用。\n",
    "\n",
    "RNN 的长依赖失败不是因为输入梯度有问题，而是因为隐藏状态在时间维度上的梯度连乘导致早期状态对损失几乎没有贡献，所以模型学不会‘早期信息很重要’。所以这里我们重点讨论h_t\n",
    "\n",
    "## (2)反向传播部分\n",
    "我们关心的是：\n",
    ">损失L_t对早期隐状态h1的梯度：\n",
    "$$\\frac{\\partial L_{T}}{\\partial h_{1}}$$\n",
    "\n",
    "链式法则展开：\n",
    "$$\\frac{\\partial L_{T}}{\\partial h_{1}}=\\frac{\\partial L_{T}}{\\partial h_{T}} \\cdot \\frac{\\partial h_{T}}{\\partial h_{T-1}} \\cdot \\frac{\\partial h_{T-1}}{\\partial h_{T-2}} \\cdots \\frac{\\partial h_{2}}{\\partial h_{1}}$$\n",
    "$$\\frac{\\partial L_{T}}{\\partial h_{1}}=\\frac{\\partial L_{T}}{\\partial h_{T}} \\prod_{t=2}^{T} \\frac{\\partial h_{t}}{\\partial h_{t-1}}$$\n",
    "\n",
    "回到RNN的定义：\n",
    "$$h_{t}=\\tanh \\left(W_{h} h_{t-1}+W_{x} x_{t}+b\\right)$$\n",
    "\n",
    "对h_{t-1}求导：\n",
    "$$\\frac{\\partial h_{t}}{\\partial h_{t-1}}=\\frac{\\partial \\tanh \\left(a_{t}\\right)}{\\partial a_{t}} \\cdot \\frac{\\partial a_{t}}{\\partial h_{t-1}}$$\n",
    "\n",
    "其中：\n",
    "$$a_{t}=\\left(W_{h} h_{t-1}+W_{x} x_{t}+b\\right)$$\n",
    "\n",
    "- 激活函数的导数：\n",
    "$$\\tanh ^{\\prime}(x)=1-\\tanh ^{2}(x)$$\n",
    "\n",
    "- 线性部分的导数：\n",
    "$$\\frac{\\partial a_{t}}{\\partial h_{t-1}}=W_{h}$$\n",
    "\n",
    "合在一起（Jacobian）：\n",
    "$$\\frac{\\partial h_{t}}{\\partial h_{t-1}}=W_{h}^{\\top} \\cdot \\operatorname{diag}\\left(\\tanh ^{\\prime}\\left(a_{t}\\right)\\right)$$\n",
    "\n",
    "再把结果带回去：\n",
    "$$\\prod_{t=2}^{T} \\frac{\\partial h_{t}}{\\partial h_{t-1}}=\\prod_{t=2}^{T}\\left(W_{h}^{\\top} \\cdot \\operatorname{diag}\\left(\\tanh ^{\\prime}\\left(a_{t}\\right)\\right)\\right)$$\n",
    "\n",
    ">设:\n",
    "$$\\begin{array}{l}\n",
    "\\left\\|W_{h}\\right\\| \\approx \\lambda \\\\\n",
    "\\tanh ^{\\prime}\\left(a_{t}\\right) \\approx \\gamma, \\text { 且 } \\gamma<1\n",
    "\\end{array}$$\n",
    "\n",
    "那么：\n",
    "$$\\left\\|\\prod_{t=2}^{T} \\frac{\\partial h_{t}}{\\partial h_{t-1}}\\right\\| \\approx(\\lambda \\cdot \\gamma)^{T}$$\n",
    "\n",
    "这样就会出现下面的结局：\n",
    "| 情况                   | 数学结果   | 现象       |\n",
    "| -------------------- | ------ | -------- |\n",
    "| ($\\lambda \\gamma$ < 1) | 指数 → 0 | **梯度消失** |\n",
    "| ($\\lambda \\gamma$ > 1) | 指数 → ∞ | **梯度爆炸** |\n",
    "\n",
    "\n",
    "## (3)上面的推导结果直接导致RNN模型无法解决长距离依赖问题\n",
    "\n",
    "梯度消失 = 无法学习远处信息\n",
    "\n",
    "如果：\n",
    "$$\\frac{\\partial L_{T}}{\\partial h_{1}} \\approx 0$$\n",
    "\n",
    "那么：\n",
    "- h₁ 对损失几乎没影响\n",
    "\n",
    "- 网络 学不到“早期信息是有用的”\n",
    "\n",
    "- 参数更新只看最近几个 token\n",
    "\n",
    "这就是RNN“记不住”的本质\n",
    "\n",
    "## (4)RNN的另一个问题：模型不能进行并行计算，RNN必须按顺序处理，每个时间步必须依赖上一个时间步的隐藏状态的计算结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff18641",
   "metadata": {},
   "source": [
    "## (5)RNN原理图\n",
    "### 1.时间展开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac37093",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "x1     x2     x3     ...     xT\n",
    " |      |      |              |\n",
    " v      v      v              v\n",
    "[h1] -> [h2] -> [h3] -> ... -> [hT]\n",
    "                                   |\n",
    "                                   v\n",
    "                                  Loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d048f87d",
   "metadata": {},
   "source": [
    "### 2.反向传播(梯度回传路径)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e187cc2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Loss\n",
    " |\n",
    " v\n",
    "∂L/∂hT\n",
    " |\n",
    " v\n",
    "∂L/∂hT-1 = ∂L/∂hT · ∂hT/∂hT-1\n",
    " |\n",
    " v\n",
    "∂L/∂hT-2 = ∂L/∂hT-1 · ∂hT-1/∂hT-2\n",
    " |\n",
    " v\n",
    "...\n",
    " |\n",
    " v\n",
    "∂L/∂h1 = ∂L/∂hT · Π Jacobian\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
