{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2166fed1",
   "metadata": {},
   "source": [
    "## 1. PyTorch 数据管道全景：Dataset → Sampler → DataLoader → Batch\n",
    "\n",
    "DataLoader 不是“读取数据”这么简单，它其实是一个“批处理调度器”。流程是：\n",
    "\n",
    "1. Dataset：定义“第 i 个样本是什么”。\n",
    "\n",
    "2. Sampler / BatchSampler：定义“按什么顺序取 i”（以及怎么把 i 组成 batch）。\n",
    "\n",
    "3. DataLoader：并行取样本 + collate_fn 组 batch +（可选）pin memory +（你在训练 loop 里）搬到 GPU。\n",
    "\n",
    "**Dataset 负责“样本内容”，Sampler 负责“索引策略”，collate_fn 负责“怎么拼 batch”.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da099214",
   "metadata": {},
   "source": [
    "## 2. Dataset 两大类：Map-style vs Iterable-style\n",
    "\n",
    "### 2.1 Map-style Dataset（最常见\n",
    "\n",
    "满足：\n",
    "\n",
    "__len__()：数据集大小\n",
    "\n",
    "__getitem__(idx)：返回单个样本\n",
    "\n",
    "典型：图片/文本分类、NER 样本、pair 数据等。\n",
    "\n",
    "**关键理解：DataLoader 会不断产生 idx（来自 sampler），然后调用 dataset[idx] 拿样本**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0989058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __Len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        return {\"x\": x, \"y\": y}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149d26fc",
   "metadata": {},
   "source": [
    "### 2.2 IterableDataset（流式/无限/无法随机访问）\n",
    "\n",
    "当你：\n",
    "\n",
    "数据太大（不能 len / 不能随机访问）\n",
    "\n",
    "数据来自流（Kafka、socket、巨型日志）\n",
    "\n",
    "或者要做在线生成（无限数据）\n",
    "\n",
    "用 IterableDataset，核心是实现 __iter__()：\n",
    "\n",
    "**IterableDataset 通常 不能 shuffle（除非你自己做 buffer shuffle）。**\n",
    "\n",
    "**多 worker 时，必须“切分数据流”，否则每个 worker 会读到同样的数据（重复样本）。后面会讲怎么避免。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa1785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class StreamDataset(IterableDataset):\n",
    "    def __iter__(self):\n",
    "        for i in range(10*12):\n",
    "            yield torch.tensor([i]), torch.tensor(i % 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a60058",
   "metadata": {},
   "source": [
    "## DataLoader 的核心参数：不只是 batch_size + shuffle\n",
    "\n",
    "### 3.1 基本参数\n",
    "\n",
    "batch_size：每个 batch 样本数\n",
    "\n",
    "shuffle=True：本质是用 RandomSampler\n",
    "\n",
    "drop_last=True：丢掉最后不足 batch 的部分（分布式训练常用，保证步数一致）\n",
    "\n",
    "collate_fn：把 list[样本] → batch（非常重要，NLP 变长序列必用）\n",
    "\n",
    "### 3.2 采样相关（Sampler / BatchSampler）\n",
    "\n",
    "sampler=：自定义索引顺序（如类别均衡、长度分桶）\n",
    "\n",
    "batch_sampler=：直接产生“batch 的索引列表”，此时不能再传 batch_size/shuffle/sampler\n",
    "\n",
    ">面试题：shuffle=True 和 sampler=... 能同时用吗？\n",
    ">不能（逻辑冲突）。你要讲清：shuffle 其实就是一种 sampler\n",
    "\n",
    "### 3.3 并行与性能\n",
    "\n",
    "num_workers：并行加载数据的进程数（不是线程，默认多进程）\n",
    "\n",
    "pin_memory=True：把 batch 放到“页锁定内存”，CPU→GPU 拷贝更快\n",
    "\n",
    "persistent_workers=True：epoch 之间不销毁 worker，减少反复 fork 开销（训练大模型常用）\n",
    "\n",
    "prefetch_factor：每个 worker 预取 batch 数（默认 2）\n",
    "\n",
    "timeout：防止 worker 卡死（调试用）\n",
    "\n",
    "**经验但可讲成原理：**\n",
    "\n",
    "数据预处理重（tokenize、解压、增强）→ num_workers 开大；\n",
    "\n",
    "数据很轻（纯 tensor index）→ num_workers=0 反而更稳；\n",
    "\n",
    "pin_memory 对 GPU 训练通常有正收益，但注意内存占用上升。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f65b186",
   "metadata": {},
   "source": [
    "## 4. collate_fn：NLP 的“命门”\n",
    "\n",
    "默认 collate_fn 会尝试把同类型的样本 stack 成 tensor。\n",
    "\n",
    "但 NLP 常见：每个样本长度不一样 → 你必须 padding + mask。\n",
    "\n",
    "### 4.1 一个标准 NLP batch：input_ids + attention_mask + labels\n",
    "\n",
    "假设 Dataset 返回："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12989929",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"input_ids\": [101, 2003, ...], \"label\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe0992",
   "metadata": {},
   "source": [
    "我们写 collate_fn：padding 到 batch 内最大长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ebd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "PAD_ID = 0\n",
    "\n",
    "def collate_fn(batch):\n",
    "    #batch: List[dict]\n",
    "    input_ids = [torch.tensor(x[\"input_ids\"], dtype=torch.long) for x in batch]\n",
    "    labels = torch.tensor([x[\"label\"] for x in batch], dtype=torch.long)\n",
    "\n",
    "    lengths = torch.tensor([len(x) for x in input_ids], dtype=torch.long)\n",
    "    max_len = int(lengths.max())\n",
    "\n",
    "    padded = torch.full((len(input_ids), max_len), PAD_ID, dtype=torch.long)\n",
    "    attention_mask = torch.zeros((len(input_ids), max_len), dtype=torch.long)\n",
    "\n",
    "    for i, ids in enumerate(input_ids):\n",
    "        #当前样本的真实长度（token数）\n",
    "        L = ids.numel()\n",
    "\n",
    "        #将真实token拷贝到padded tensor的第i行；只填充前L个位置，其余保持PAD\n",
    "        padded[i, :L] = ids\n",
    "\n",
    "        #真实token为1，padding位置为0\n",
    "        attention_mask[i, :L] = 1\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": padded,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"lengths\": lengths,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd52d84",
   "metadata": {},
   "source": [
    "collate_fn 的输入是 list[样本]（每个样本是 __getitem__ 的返回）。\n",
    "\n",
    "padding 发生在 CPU；之后训练 loop 再 .to(device, non_blocking=True)。\n",
    "\n",
    "attention_mask/lengths 是后续模型/损失计算的关键。\n",
    "\n",
    "### 4.2 动态 padding vs 固定 padding\n",
    "\n",
    "动态 padding：每个 batch pad 到 batch max_len（更省算力，BERT/LLM 常用）\n",
    "\n",
    "固定 padding：pad 到全局 max_len（实现简单，但浪费）\n",
    "\n",
    "> 动态 padding 为什么会让 GPU 利用率不稳定？\n",
    "\n",
    ">你要答：不同 batch 的 max_len 波动导致计算量波动；解决：按长度分桶 bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a6a8e6",
   "metadata": {},
   "source": [
    "## 5. 长度分桶（Bucketing）：让 batch 更整齐、更快\n",
    "\n",
    "思路：把相近长度的样本放在同一 batch，减少 padding 浪费。\n",
    "\n",
    "实现方式有两类：\n",
    "\n",
    "1. 先排序/分桶，再 batch（batch_sampler）\n",
    "\n",
    "2. Sampler 产生 idx 时就按桶策略来\n",
    "\n",
    "**目标：** 降低 padding，提高吞吐\n",
    "\n",
    "**代价：** 随机性降低（通常桶内 shuffle / 桶间 shuffle 折中）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa84c8",
   "metadata": {},
   "source": [
    "## 6. 随机性与可复现：多 worker 的 seed 怎么搞\n",
    "\n",
    "### 6.1 为什么 num_workers>0 会让随机增强不稳定？\n",
    "\n",
    "因为每个 worker 是独立进程，有自己 RNG 状态。\n",
    "如果不设置，会出现：\n",
    "\n",
    "每次跑随机增强不一样（正常）\n",
    "\n",
    "但更糟：不同 worker 可能拿到相同 seed → 增强重复（看实现）\n",
    "\n",
    "### 6.2 标准做法：worker_init_fn + generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08795e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acaad78",
   "metadata": {},
   "source": [
    "7. 分布式训练（DDP）下 DataLoader 的关键：DistributedSampler\n",
    "\n",
    "在多卡 DDP 下，每个进程（rank）必须拿到不同切片的数据，否则会重复训练同一数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0371c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "sampler = DistributedSampler(dataset, shuffle=True)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, sampler=sampler, num_workers=4)\n",
    "\n",
    "for epoch in range(E):\n",
    "    sampler.set_epoch(epoch)    #关键：每一个epoch重新洗牌一致\n",
    "    for batch in loader:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114020b",
   "metadata": {},
   "source": [
    "> 为什么必须 set_epoch\n",
    "\n",
    "> 否则每个 epoch shuffle 的顺序不变（或者各 rank 不一致），影响训练效果/可复现。\n",
    "\n",
    ">drop_last 为什么常配合 DDP？\n",
    "\n",
    ">保证每个 rank step 数一致，否则最后一个 batch 大小不一可能导致同步/shape 问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59aaf15",
   "metadata": {},
   "source": [
    "## 8. IterableDataset + 多 worker：避免重复读\n",
    "\n",
    "如果是 IterableDataset，多 worker 时，每个 worker 都会跑一遍 __iter__，导致重复数据。\n",
    "\n",
    "常见做法：用 get_worker_info() 切分流："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b7471a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2933166974.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    if info is None:\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import IterableDataset, get_worker_info\n",
    "\n",
    "class ShardedStream(IterableDataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __iter__(self):\n",
    "        info = get_worker_info()\n",
    "        if info is None:\n",
    "            #单进程\n",
    "            for x in self.data:\n",
    "                yield x\n",
    "        else:\n",
    "            #多worker：按worker_id stride切分\n",
    "            wid = info.id\n",
    "            nw = info.num_workers\n",
    "            for i in range(wid, len(self.data), nw):\n",
    "                yield self.data[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bbfe3c",
   "metadata": {},
   "source": [
    "## 9. 性能调优：你要能像工程师一样解释瓶颈在哪里\n",
    "\n",
    "### 9.1 常见瓶颈\n",
    "\n",
    "训练 loop 卡在取 batch：CPU 处理/IO 慢\n",
    "\n",
    "GPU 利用率低：数据供不上（dataloader 慢）\n",
    "\n",
    "多 worker 反而更慢：进程间拷贝/序列化开销大、磁盘随机读更糟\n",
    "\n",
    "### 9.2 你应该会的调参逻辑\n",
    "\n",
    "先 num_workers=0 跑通（排除多进程坑）\n",
    "\n",
    "再逐步加：2 → 4 → 8，观察吞吐\n",
    "\n",
    "GPU 训练常开：pin_memory=True，搬运用 non_blocking=True\n",
    "\n",
    "epoch 间卡顿：试 persistent_workers=True\n",
    "\n",
    "典型训练 loop："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c1347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader:\n",
    "    batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98141c28",
   "metadata": {},
   "source": [
    "任务 A（基础）：写一个文本分类 Dataset：\n",
    "\n",
    "输入：texts: List[str]，labels: List[int]\n",
    "\n",
    "__getitem__ 返回 {\"input_ids\": List[int], \"label\": int}（你可以用简化 tokenizer：按空格分词再映射 id）\n",
    "\n",
    "任务 B（核心）：写 collate_fn：动态 padding + attention_mask。\n",
    "\n",
    "输出 shape：input_ids [B, T]，attention_mask [B, T]，labels [B]\n",
    "\n",
    "任务 C（进阶）：\n",
    "\n",
    "做长度分桶 batch（实现一个简单 bucket batch sampler 或者排序后分 batch）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a61bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Iterator, Optional\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "class SimpleVocab: #一个极简词表：token(str)->id(int)\n",
    "    def __init__(self, texts: List[str], pad_token: str=\"<PAD>\", unk_token: str=\"<UNK>\"):\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.token_to_id: Dict[str, int] = {}   #token到id的映射表\n",
    "        self.id_to_token: List[str] = []    #id到token的反查表\n",
    "        self._add_token(self.pad_token) #保证PAD id=0\n",
    "        self._add_token(self.unk_token) #保证UNK id=0\n",
    "        for text in texts:  #遍历所有文本构建词表\n",
    "            for tok in self._basic_tokenize(text):  #逐token加入词表\n",
    "                self._add_token(tok)    #加入词表，若已存在则忽略\n",
    "    \n",
    "    def _basic_tokenize(self, text: str) -> List[str]: #简化tokenizer，按空格切分\n",
    "        text = text.strip() #去掉首位空格\n",
    "        if text == \"\":  #如果整句为空\n",
    "            return []   #返回空的token列表\n",
    "        return text.strip() #按任意空白切分\n",
    "    \n",
    "    def _add_token_(self, token: str) -> None:  #把token加入词表\n",
    "        if token in self.token_to_id:\n",
    "            return\n",
    "        new_id = len(self.id_to_token)  #新token的id等于当前词表的大小\n",
    "        self.token_to_id[token] = new_id\n",
    "        self.id_to_token.append(token)\n",
    "\n",
    "    @property\n",
    "    def pad_id(self) -> int:    #暴露pad id，collate_fn会用\n",
    "        return self.token_to_id[self.pad_token]\n",
    "    \n",
    "    @property\n",
    "    def unk_id(self) -> int:    #暴露unk id，遇到OOV时会用\n",
    "        return self.token_to_id[self.unk_token]\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:   #把文本编成 token id 序列\n",
    "        tokens = self._basic_tokenize(text) #分词\n",
    "        ids: List[int] = [] #初始化id列表\n",
    "        for tok in tokens:\n",
    "            ids.append(self.token_to_id.get(tok, self.unk_id))\n",
    "        return ids  #反汇变长的序列\n",
    "    \n",
    "class TextDataset(Dataset): #任务A：文本分类数据集\n",
    "    def __init__(self, texts: List[str], labels: List[int], vocab: Optional[SimpleVocab] = None) -> None:\n",
    "        assert len(texts) == len(labels)    #防止索引错位\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab if vocab is not None else SimpleVocab    #若没传vocab，则用全量texts构建词表\n",
    "        self.lengths: List[int] = []    #任务C会用到，每条样本的序列长度\n",
    "        for t in self.texts:\n",
    "            self.lengths.append(len(self.vocab.encode(t)))  #预计算编码后的长度（bucketing需要）\n",
    "\n",
    "    def __len__(self) -> int:   #Dataset必备，返回样本长度\n",
    "        return len(self.texts)  #返回数据集大小\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, any]:  #Dataset必备，返回第idx个样本\n",
    "        text = self.texts[idx]\n",
    "        label = int(self.labels[idx])\n",
    "        input_ids = self.vocab.encode(text) #文本编码成 List[int]\n",
    "        return {\"input_ids\": input_ids, \"label\": label}\n",
    "        \n",
    "def make_collate_fn(pad_id: int): #任务B，返回一个闭包collate_fn(把pad_id带进去)\n",
    "    def collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids_list = [torch.tensor(x[\"input_ids\"], dtype=torch.long) for x in batch]    #把每条序列转换为tensor\n",
    "        labels = torch.tensor([x[\"label\"] for x in batch], dtype=torch.long)    #labels直接堆成[B]\n",
    "        lengths = torch.tensor([t.numel() for t in input_ids_list], dtype=torch.long)   #每条序列的真实长度[B]\n",
    "        max_len = int(lengths.max.item()) if len(batch) > 0 else 0  #batch内的最大长度，动态padding关键\n",
    "        B = len(batch)  #batch size\n",
    "        padded = torch.full((B, max_len), pad_id, dtype=torch.long) #先用PAD填满[B, T]\n",
    "        attention_mask = torch.zeros((B, max_len), dtype=torch.long)    #mask初始全0， [B, T]\n",
    "        for i , ids in enumerate(input_ids_list):\n",
    "            L = ids.numel()\n",
    "            if L == 0:\n",
    "                continue    #处理空句子，保持PAD, mask全是0\n",
    "            padded[i, :L] = ids #把真实token拷贝到前L位\n",
    "            attention_mask[i, :L] = 1   #把真实token标1\n",
    "\n",
    "        return {\"input_ids\": padded, \"attention_mask\": attention_mask, \"labels\":labels} #输出模型可以吃的batch\n",
    "    return collate_fn\n",
    "\n",
    "class BucketBatchSampler(Sampler[List[int]]):   #任务C，长度分桶batch sampler（返回“索引列表”）\n",
    "    def __init__(\n",
    "            self,\n",
    "            lengths: List[int], #每个batch的长度\n",
    "            batch_size: int,    #每个batch的样本数\n",
    "            shuffle: bool = True,    #是否做随机\n",
    "            drop_last: bool = False,    #丢弃最后不满batch的部分\n",
    "            bucket_size_multiplier: int = 50,   #\"局部窗口\"大小 = batch size * multiplier(越大越随机，越小越整齐)\n",
    "            seed: int = 42,\n",
    "    ) -> None:\n",
    "        self.lengths = lengths\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.droplast = drop_last\n",
    "        self.bucket_size = max(batch_size * bucket_size_multiplier, batch_size) #每个同窗口的样本数\n",
    "        self.seed = seed\n",
    "        self.epoch = 0  #epoch计数\n",
    "\n",
    "    def set_epoch(self, epoch: int) -> None:\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def __iter__(self) -> Iterator[List[int]]:  #迭代器：每次 yield 一个 batch 的 indices\n",
    "        n = len(self.lengths)   #样本总数\n",
    "        indices = list(range(n))    #构造所有样本的索引\n",
    "        indices.sort(key=lambda i: self.lengths[i]) #先按全局长度排序（实现“相近长度靠近”）\n",
    "        rng = random.Random(self.seed + self.epoch) #用 seed + epoch 构造可复现 rng\n",
    "        if self.shuffle:\n",
    "            buckets: List[List[int]] = []   #保存按窗口切分后的桶\n",
    "            for start in range(0, n, self.bucket_size): #每次取一段窗口\n",
    "                end = min(start + self.bucket_size, n)  #计算窗口右边界\n",
    "                bucket = indices[start: end]    #切出一个窗口的长度\n",
    "                rng.shuffle(bucket) #窗口内打乱\n",
    "                buckets.append(bucket)  #收集窗口\n",
    "            indices = [i for bucket in buckets for i in bucket] #把所有窗口拼回一个索引序列\n",
    "            batch_starts = list(range(0, n, self.batch_size))   #每个batch的起始位置\n",
    "            rng.shuffle(batch_starts)   #打乱batch的顺序（避免短的都在前面）\n",
    "            for s in batch_starts:  #按照打乱后的batch起点顺序产出batch\n",
    "                e = min(s + self.batch_size, n) #计算batch的终点\n",
    "                batch = indices[s: e]   #切出一个batch\n",
    "                if self.drop_last and len(batch) < self.batch_size: #如果不满batch，且drop_last\n",
    "                    continue    #跳过最后不满batch的部分\n",
    "                yield batch #产出一个batch的indices\n",
    "        else:   #如果不需要随机性，完全按照长度顺序分batch\n",
    "            for start in range(0, n ,self.batch_size):  #从头到尾按batch_size切\n",
    "                end = min(start + self.batch_size, n)\n",
    "                batch = indices[start:end]\n",
    "                if self.droplast and len(batch) < self.batch_size:\n",
    "                    continue\n",
    "                yield batch\n",
    "\n",
    "    def __len__(self) -> int:   #返回一个epoch有多少个batch（让DataLoader/进度条知道长度）\n",
    "        n = len(self.lengths)\n",
    "        if self.drop_last:\n",
    "            return n // self.batch_size #向下取整\n",
    "        return math.ceil(n / self.batch_size)   #否则向上取整\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    texts = [  # 构造一些示例文本\n",
    "    \"i love nlp\",  # 短句\n",
    "    \"pytorch dataloader is important\",  # 长句\n",
    "    \"nlp interview\",  # 中等\n",
    "    \"\",  # 空句（测试边界条件）\n",
    "    \"bucketing reduces padding waste\",  # 较长\n",
    "    \"hello world\",  # 短\n",
    "    ]\n",
    "    labels = [1, 0, 1, 0, 0, 1]  # 构造示例标签（0/1 二分类"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
