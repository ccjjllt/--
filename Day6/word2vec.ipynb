{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04349f05",
   "metadata": {},
   "source": [
    "## 1.One-hot核心问题：它不会“表达语义”，只会表达“身份”\n",
    ">设词表的大小|V|=100000,one-hot向量是10万维，但只有一个位置是1\n",
    "### (1)相似性完全消失\n",
    "- 猫 和 狗 的 one-hot 内积 = 0\n",
    "\n",
    "- 猫 和 苹果 的 one-hot 内积也 = 0\n",
    "\n",
    "⇒ 在 one-hot 空间里，“猫≈狗”这类语义相近关系根本不存在（所有不同词都等距/等不相似）。\n",
    "\n",
    "### (2)稀疏 + 维度灾难 + 泛化弱\n",
    "- 维度随词表线性增长，向量极稀疏\n",
    "\n",
    "- 任何依赖距离/角度的模型都学不到“相近词更近”的结构\n",
    "\n",
    "- 你在训练集中没怎么见过的词（低频词），更难泛化，因为表示里没有“可迁移的共享结构”\n",
    "\n",
    "### (3)参数量容易爆\n",
    "如果用 one-hot 直接接一个线性层得到隐藏层 $h \\in \\mathbb{R}^{d}$:\n",
    "- 权重矩阵 $W \\in \\mathbb{R}^{|V| \\times d}$\n",
    "\n",
    "- 计算 one-hot @ W 本质上就是查表：取出第 i 行作为词向量\n",
    "\n",
    "⇒ 这说明：embedding 层就是把 one-hot 变成“可学习的稠密向量”，并且让相似词能学到相似向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de596c81",
   "metadata": {},
   "source": [
    "## 2.Distributional Hypothesis: 语义来自上下文\n",
    ">一个词的语义由它常出现的上下文分布来决定。如果两个词在大量语境中“可互换/上下文相似”，它们的语义就相似。\n",
    "\n",
    "把它落实到可计算的东西上就是：\n",
    "- 统计“词w”附近窗口内出现“上下文词 c”的频次\n",
    "\n",
    "- 得到每个词的“上下文分布向量”\n",
    "\n",
    "- 分布相近 ⇒ 语义相近\n",
    "\n",
    "## 3.共现矩阵：把”语义来自上下文“变成一个可计算的矩阵\n",
    "共现矩阵 $X \\in \\mathbb{R}^{|V| \\times|V|}$；\n",
    "- 行：中心词w\n",
    "\n",
    "- 列：上下文词c\n",
    "\n",
    "- Xw,c：在窗口内c出现在w周围的次数\n",
    "\n",
    "## 4.共现矩阵示例\n",
    "\n",
    "### (1)语料\n",
    "1.我 喜欢 自然 语言 处理\n",
    "\n",
    "2.我 喜欢 深度 学习\n",
    "\n",
    "3.深度 学习 改变 世界\n",
    "\n",
    "### (2)此表顺序\n",
    "['世界','喜欢','处理','学习','我','改变','深度','自然','语言']\n",
    "\n",
    "### (3)共现矩阵（行=中心词，列=上下文词）\n",
    "|center\\context|世界|喜欢|处理|学习|我|改变|深度|自然|语言|\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|世界|0|0|0|0|0|1|0|0|0|\n",
    "|喜欢|0|0|0|0|2|0|1|1|0|\n",
    "|处理|0|0|0|0|0|0|0|0|1|\n",
    "|学习|0|0|0|0|0|1|2|0|0|\n",
    "|我|0|2|0|0|0|0|0|0|0|\n",
    "|改变|1|0|0|1|0|0|0|0|0|\n",
    "|深度|0|1|0|2|0|0|0|0|0|\n",
    "|自然|0|1|0|0|0|0|0|0|1|\n",
    "|语言|0|0|1|0|0|0|0|1|0|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c12c4e",
   "metadata": {},
   "source": [
    "## 5.代码任务：构建共现矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22903d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab = ['世界', '喜欢', '处理', '学习', '我', '改变', '深度', '自然', '语言']\n",
      "[[0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 2 0 1 1 0]\n",
      " [0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 2 0 0]\n",
      " [0 2 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 2 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "corpus = [\n",
    "    \"我 喜欢 自然 语言 处理\".split(),\n",
    "    \"我 喜欢 深度 学习\".split(),\n",
    "    \"深度 学习 改变 世界\".split()\n",
    "]\n",
    "\n",
    "# 1) vocab\n",
    "vocab = sorted(set(w for sent in corpus for w in sent))\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "V = len(vocab)\n",
    "\n",
    "# 2) co-occurrence matrix\n",
    "window = 1\n",
    "X = np.zeros((V, V), dtype=int)\n",
    "\n",
    "for sent in corpus:\n",
    "    n = len(sent)\n",
    "    for i, center in enumerate(sent):\n",
    "        left = max(0, i - window)\n",
    "        right = min(n, i + window + 1)\n",
    "        for j in range(left, right):\n",
    "            if j == i:\n",
    "                continue\n",
    "            context = sent[j]\n",
    "            X[word2idx[center], word2idx[context]] += 1\n",
    "\n",
    "print(\"vocab =\", vocab)\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a0b8fe",
   "metadata": {},
   "source": [
    "## 6.从共现矩阵到Word2Vec\n",
    "\n",
    "- 共现矩阵虽然有语义，但是太大太稀疏（|V| x |V|级别，不好存不好学）\n",
    "- Word2Vec 的目标：学一个低维向量 $e_{w} \\in \\mathbb{R}^{d}(d \\ll|V|)$ ，让它能预测上下文（Skip-gram）或被上下文预测（CBOW）."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30c9321",
   "metadata": {},
   "source": [
    "## 7.Word2Vec\n",
    ">Word2Vec 是一种用于将单词表示为向量的算法，它通过对大量文本进行训练，学习词语之间的语义关系，并将每个词表示为一个向量。Word2Vec 模型的核心目标是将词语转换为密集的、低维度的向量，以便计算机能够更好地理解词语的语义和它们之间的关系。\n",
    "\n",
    "### 原理\n",
    "\n",
    "Word2Vec 的基本原理是通过大量的文本数据，学习每个词在上下文中的分布式表示。它基于假设：如果两个词在语境中经常一起出现，那么它们的语义是相关的。\n",
    "\n",
    "Word2Vec 有两个主要的模型：\n",
    "\n",
    ">1.CBOW (Continuous Bag of Words)：给定上下文词预测目标词。其目标是根据上下文词来预测目标词。\n",
    "\n",
    ">2.Skip-gram：给定一个目标词，预测上下文词。该模型的目标是通过目标词来预测其周围的上下文词。\n",
    "\n",
    "### 数学原理\n",
    "\n",
    ">CBOW 模型：\n",
    "\n",
    "给定上下文词w1,w2,...,wn, 预测目标词wt。目标是通过最大化条件概率 $P\\left(w_{t} \\mid w_{1}, w_{2}, \\ldots, w_{n}\\right)$ 来训练模型。\n",
    "\n",
    "目标函数为最大化每个单词的条件概率：\n",
    "$$ J(\\theta)=\\sum_{t=1}^{T} \\log P\\left(w_{t} \\mid w_{t-n}, \\ldots, w_{t-1}, w_{t+1}, \\ldots, w_{t+n}\\right)$$\n",
    "其中T为词汇表大小，n为窗口大小。\n",
    "\n",
    ">Skip-gram 模型：\n",
    "\n",
    "给定一个目标词wt，并预测上下文词w1,w2,...,wn。\n",
    "\n",
    "目标是最大化目标词与上下文词的联合概率：\n",
    "$$ P\\left(w_{\\text {context }} \\mid w_{t}\\right)=\\prod_{c \\in \\text { context }} P\\left(w_{c} \\mid w_{t}\\right)$$\n",
    "这里的目标是通过学习使得预测的上下文词的概率最大化。\n",
    "\n",
    "### 代码实现\n",
    "实现 Word2Vec 的代码可以使用 Python 中的 gensim 库，它提供了高效的实现。以下是一个简单的代码示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbc18b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================================\n",
    "# 1. 准备数据 (模拟语料)\n",
    "# ==========================================\n",
    "# 为了演示效果，我们手动构造一些有明显语义聚类的句子\n",
    "# 分为三类：AI技术、水果、家庭关系\n",
    "corpus = [\n",
    "    # AI / NLP 相关\n",
    "    ['machine', 'learning', 'is', 'artificial', 'intelligence'],\n",
    "    ['deep', 'learning', 'uses', 'neural', 'networks'],\n",
    "    ['natural', 'language', 'processing', 'is', 'nlp'],\n",
    "    ['word', 'embedding', 'vector', 'space'],\n",
    "    ['skip-gram', 'model', 'predicts', 'context'],\n",
    "    ['algorithm', 'data', 'science', 'computer'],\n",
    "    ['gpu', 'trains', 'models', 'faster'],\n",
    "    \n",
    "    # 水果/食物 相关\n",
    "    ['apple', 'banana', 'orange', 'fruit'],\n",
    "    ['i', 'eat', 'apple', 'and', 'orange'],\n",
    "    ['banana', 'is', 'yellow', 'fruit'],\n",
    "    ['cherry', 'strawberry', 'sweet', 'food'],\n",
    "    ['lunch', 'dinner', 'breakfast', 'eat'],\n",
    "    \n",
    "    # 家庭/人称 相关\n",
    "    ['king', 'queen', 'prince', 'princess'],\n",
    "    ['father', 'mother', 'son', 'daughter'],\n",
    "    ['man', 'woman', 'boy', 'girl'],\n",
    "    ['brother', 'sister', 'family', 'home']\n",
    "]\n",
    "\n",
    "# 增加数据量以强化共现关系 (简单复制)\n",
    "training_data = corpus * 100\n",
    "\n",
    "print(f\"语料加载完毕，共 {len(training_data)} 个句子。\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. 训练 Word2Vec (Skip-gram)\n",
    "# ==========================================\n",
    "# 参数解析：\n",
    "# vector_size: 词向量维度 (通常 100-300，演示用 10)\n",
    "# window: 上下文窗口大小\n",
    "# min_count: 忽略出现次数过少的词\n",
    "# sg: 1 表示 Skip-gram (默认0是CBOW) -> 今天的重点\n",
    "# negative: 负采样个数 (通常 5-20) -> 今天的重点\n",
    "# epochs: 迭代次数\n",
    "model = Word2Vec(sentences=training_data, \n",
    "                 vector_size=10, \n",
    "                 window=3, \n",
    "                 min_count=1, \n",
    "                 sg=1,  # <--- 重点：开启 Skip-gram\n",
    "                 negative=5, # <--- 重点：负采样\n",
    "                 epochs=50,\n",
    "                 seed=42)\n",
    "\n",
    "print(\"\\nWord2Vec 模型训练完成。\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. 验证训练结果\n",
    "# ==========================================\n",
    "def check_similarity(word):\n",
    "    try:\n",
    "        sims = model.wv.most_similar(word, topn=3)\n",
    "        print(f\"\\n与 '{word}' 最相似的词:\")\n",
    "        for w, s in sims:\n",
    "            print(f\"  - {w}: {s:.4f}\")\n",
    "    except KeyError:\n",
    "        print(f\"'{word}' 不在词表中。\")\n",
    "\n",
    "check_similarity('apple')\n",
    "check_similarity('learning')\n",
    "check_similarity('king')\n",
    "\n",
    "# ==========================================\n",
    "# 4. 可视化 (PCA 降维)\n",
    "# ==========================================\n",
    "def plot_embeddings(model):\n",
    "    # 获取所有词向量\n",
    "    words = list(model.wv.index_to_key)\n",
    "    vectors = model.wv[words]\n",
    "    \n",
    "    # 使用 PCA 将 10维 降到 2维 以便绘图\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(vectors)\n",
    "    \n",
    "    # 创建 DataFrame 方便绘图\n",
    "    df = pd.DataFrame(result, columns=['x', 'y'])\n",
    "    df['word'] = words\n",
    "    \n",
    "    # 绘图设置\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # 绘制散点\n",
    "    sns.scatterplot(data=df, x='x', y='y', s=100, color='blue', alpha=0.6)\n",
    "    \n",
    "    # 添加标签\n",
    "    for i, line in df.iterrows():\n",
    "        plt.text(line['x']+0.02, line['y'], line['word'], fontsize=12)\n",
    "        \n",
    "    plt.title('Word2Vec (Skip-gram) Embeddings Visualization', fontsize=16)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    \n",
    "    # 保存并显示\n",
    "    plt.savefig('w2v_visualization.png')\n",
    "    print(\"\\n可视化图片已保存为 'w2v_visualization.png'\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n开始生成可视化...\")\n",
    "plot_embeddings(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9823e87",
   "metadata": {},
   "source": [
    "## 8.使用pytorch实现skip-gram\n",
    "### (1)Skip-gram + Softmax（最原始定义）\n",
    "给定中心词w，预测上下文词c：\n",
    "$$P(c \\mid w)=\\frac{\\exp \\left(\\mathbf{v}_{c}^{\\top} \\mathbf{u}_{w}\\right)}{\\sum_{c^{\\prime} \\in V} \\exp \\left(\\mathbf{v}_{c^{\\prime}}^{\\top} \\mathbf{u}_{w}\\right)}$$\n",
    "\n",
    "- ${u}_{w}$：中心词向量（input embedding）\n",
    "- ${v}_{c}$：上下文词向量（output embedding）\n",
    "- 目标：最大化所有真是共现对(w,c)的对数似然\n",
    "\n",
    "这样做的问题是每一个中心词的分母要对全词表求和，太慢了。\n",
    "\n",
    "### (2)负采样 Negative Simpling\n",
    "对每个正样本(w,c)，采K个负样本n1,...,nk(从噪声分布采)\n",
    "目标函数（对一个样本）：\n",
    "$$\\mathcal{L}=-\\left[\\log \\sigma\\left(\\mathbf{v}_{c}^{\\top} \\mathbf{u}_{w}\\right)+\\sum_{i=1}^{K} \\log \\sigma\\left(-\\mathbf{v}_{n_{i}}^{\\top} \\mathbf{u}_{w}\\right)\\right]$$\n",
    "\n",
    "- $\\sigma(x)$: Sigmoid 函数 $\\frac{1}{1+e^{-x}}$，把内积压缩到 $(0, 1)$ 之间变成概率。\n",
    "\n",
    "- 第一部分 $\\log \\sigma(v_c u_w)$: 希望中心词 $u_w$ 和目标词 $v_c$ 的内积越大越好（趋近于 1）。\n",
    "\n",
    "- 第二部分 $\\sum_{i=1}^k$: 我们随机采样 $k$ 个噪音词 $w_i$（通常 $k=5 \\sim 20$）。\n",
    "\n",
    "- $\\log \\sigma(-v_{n_i}^T u_w)$: 这里用了个数学技巧，$\\sigma(-x) = 1 - \\sigma(x)$。意思是我们希望中心词 $u_w$ 和噪音词 $v_{n_i}$ 的内积越小越好（趋近于 0），也就是让 $\\sigma(-x)$ 趋近于 1。\n",
    "\n",
    "### (3)使用pytorch实现skip-gram\n",
    "- 1.分词/建词表（word → id）\n",
    "- 2.用滑动窗口构造训练样本对 (center,context)\n",
    "- 3.两套 Embedding：in_embed 和 out_embed（对应u,v）\n",
    "- 用负采样 loss 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9748dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss=10.9880\n",
      "epoch 100, loss=10.9198\n",
      "epoch 150, loss=11.4682\n",
      "epoch 200, loss=8.1949\n",
      "cos(machine, deep) = 0.9651544094085693\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# -------------------------\n",
    "# 1) toy corpus\n",
    "# -------------------------\n",
    "corpus = [\n",
    "    # AI / NLP 相关\n",
    "    ['machine', 'learning', 'is', 'artificial', 'intelligence'],\n",
    "    ['deep', 'learning', 'uses', 'neural', 'networks'],\n",
    "    ['natural', 'language', 'processing', 'is', 'nlp'],\n",
    "    ['word', 'embedding', 'vector', 'space'],\n",
    "    ['skip-gram', 'model', 'predicts', 'context'],\n",
    "    ['algorithm', 'data', 'science', 'computer'],\n",
    "    ['gpu', 'trains', 'models', 'faster'],\n",
    "    \n",
    "    # 水果/食物 相关\n",
    "    ['apple', 'banana', 'orange', 'fruit'],\n",
    "    ['i', 'eat', 'apple', 'and', 'orange'],\n",
    "    ['banana', 'is', 'yellow', 'fruit'],\n",
    "    ['cherry', 'strawberry', 'sweet', 'food'],\n",
    "    ['lunch', 'dinner', 'breakfast', 'eat'],\n",
    "    \n",
    "    # 家庭/人称 相关\n",
    "    ['king', 'queen', 'prince', 'princess'],\n",
    "    ['father', 'mother', 'son', 'daughter'],\n",
    "    ['man', 'woman', 'boy', 'girl'],\n",
    "    ['brother', 'sister', 'family', 'home']\n",
    "]\n",
    "\n",
    "# build vocab\n",
    "words = [w for sent in corpus for w in sent]\n",
    "cnt = Counter(words)\n",
    "vocab = list(cnt.keys())\n",
    "#将词转换成id\n",
    "word2id = {w:i for i,w in enumerate(vocab)}\n",
    "#将id转换成词\n",
    "id2word = {i:w for w,i in word2id.items()}\n",
    "V = len(vocab)\n",
    "\n",
    "# -------------------------\n",
    "# 2) build (center, context) pairs\n",
    "# -------------------------\n",
    "def build_pairs(corpus, window=2):\n",
    "    pairs = []\n",
    "    for sent in corpus:\n",
    "        #取出每一句话词的id\n",
    "        ids = [word2id[w] for w in sent]\n",
    "        n = len(ids)\n",
    "        #遍历句子中每一个位置i，把该位置的词当成中心词\n",
    "        for i, center in enumerate(ids):\n",
    "            for j in range(max(0, i-window), min(n, i+window+1)):\n",
    "                if j == i: \n",
    "                    continue\n",
    "                pairs.append((center, ids[j]))\n",
    "    return pairs\n",
    "\n",
    "pairs = build_pairs(corpus, window=1)\n",
    "\n",
    "# -------------------------\n",
    "# 3) negative sampling distribution (unigram^0.75)\n",
    "# -------------------------\n",
    "#把每个词出现的次数按id顺序变成tensor\n",
    "pow_freq = torch.tensor([cnt[id2word[i]] for i in range(V)], dtype=torch.float)\n",
    "#压平分布，对词频做0.75次幂，高频词概率会下降，低频词概率会上升\n",
    "neg_dist = pow_freq.pow(0.75)\n",
    "#归一化概率分布（和为1）\n",
    "neg_dist = neg_dist / neg_dist.sum()\n",
    "\n",
    "#按 neg_dist 抽样：\n",
    "#1.num_samples=batch_size*K：一次性抽出整个 batch 需要的负样本数\n",
    "#2.replacement=True：允许重复抽到同一个词（这是正常的，word2vec 就是）\n",
    "#.view(batch_size, K)：型状变成（B,K），每一个正样本配K个负样本num_samples=batch\n",
    "def sample_negatives(batch_size, K):\n",
    "    # returns (batch_size, K) negative word ids\n",
    "    return torch.multinomial(neg_dist, num_samples=batch_size*K, replacement=True).view(batch_size, K)\n",
    "\n",
    "# -------------------------\n",
    "# 4) model\n",
    "# -------------------------\n",
    "class Word2VecNeg(nn.Module):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        #当词作为中心词时用\n",
    "        self.in_embed = nn.Embedding(vocab_size, dim)   # u_w\n",
    "        #当词作为上下文词（正/负样本）时用\n",
    "        self.out_embed = nn.Embedding(vocab_size, dim)  # v_c\n",
    "        #初始化权重\n",
    "        #1.计算初始化范围\n",
    "        initrange = 0.5 / dim\n",
    "        #2.对中心词向量矩阵进行均匀分布初始化\n",
    "        self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "\n",
    "    def forward(self, center_ids, pos_ids, neg_ids):\n",
    "        # center_ids: (B,)\n",
    "        # pos_ids:    (B,)\n",
    "        # neg_ids:    (B, K)\n",
    "\n",
    "        u = self.in_embed(center_ids)         # (B, D)\n",
    "        v_pos = self.out_embed(pos_ids)       # (B, D)\n",
    "        v_neg = self.out_embed(neg_ids)       # (B, K, D)\n",
    "\n",
    "        # positive score: (B,)\n",
    "        pos_score = torch.sum(u * v_pos, dim=1)\n",
    "        pos_loss = F.logsigmoid(pos_score)    # log σ(u·v_pos)\n",
    "\n",
    "        # negative score: (B, K)\n",
    "        #torch.bmm做的时batch的矩阵乘\n",
    "        neg_score = torch.bmm(v_neg, u.unsqueeze(2)).squeeze(2)  # (B,K,D) x (B,D,1) -> (B,K)\n",
    "        neg_loss = F.logsigmoid(-neg_score).sum(dim=1)           # Σ log σ(-u·v_neg)\n",
    "\n",
    "        loss = -(pos_loss + neg_loss).mean()\n",
    "        return loss\n",
    "\n",
    "# -------------------------\n",
    "# 5) training loop\n",
    "# -------------------------\n",
    "torch.manual_seed(0)\n",
    "random.shuffle(pairs)\n",
    "\n",
    "dim = 16\n",
    "K = 5\n",
    "model = Word2VecNeg(V, dim)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "batch_size = 8\n",
    "for epoch in range(200):\n",
    "    total = 0.0\n",
    "    random.shuffle(pairs)\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        batch = pairs[i:i+batch_size]\n",
    "        center = torch.tensor([x[0] for x in batch], dtype=torch.long)\n",
    "        pos = torch.tensor([x[1] for x in batch], dtype=torch.long)\n",
    "        neg = sample_negatives(len(batch), K)\n",
    "\n",
    "        loss = model(center, pos, neg)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"epoch {epoch+1}, loss={total:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 6) get word vectors + cosine similarity demo\n",
    "# -------------------------\n",
    "def cosine(a, b):\n",
    "    return F.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0)).item()\n",
    "\n",
    "with torch.no_grad():\n",
    "    W_in = model.in_embed.weight  # (V, D)\n",
    "    w_ai = word2id[\"machine\"]\n",
    "    w_deep = word2id[\"deep\"]\n",
    "    w_nlp = word2id[\"word\"] if \"word\" in word2id else None\n",
    "\n",
    "    print(\"cos(machine, deep) =\", cosine(W_in[w_ai], W_in[w_deep]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f924bec",
   "metadata": {},
   "source": [
    "### (4)cbow与skip-gram的区别\n",
    "| 维度       | **Skip-gram (中心词 -> 上下文)**                        | **CBOW (上下文 -> 中心词)**             |\n",
    "| :------- | :------------------------------------------------ | :-------------------------------- |\n",
    "| **输入输出** | 输入 1 个词，预测 k 个词                                   | 输入 k 个词（求和/平均），预测 1 个词            |\n",
    "| **对生僻词** | **更友好**。因为每个“中心词-上下文”对都是独立的样本，生僻词即便出现次数少，也多次参与更新。 | **较差**。上下文被平均了，生僻词的信息被周围常见词“淹没”了。 |\n",
    "| **训练速度** | **慢**。一个窗口产生 2k 个训练样本。                            | **快**。一个窗口只产生 1 个训练样本。            |\n",
    "| **适用场景** | **精细化任务**（大规模语料，对词义要求高）                           | **快速训练**（对词义精度要求不那么极端）            |\n",
    "\n",
    "### 如何把skip-gram改成cbow\n",
    "只需修改 forward 的输入处理：\n",
    "- Skip-gram: 输入 center_ids (Shape: [batch])。\n",
    "\n",
    "- CBOW: 输入 context_ids_list (Shape: [batch, 2*window])。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e57684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW forward 伪代码\n",
    "contexts = self.in_embed(context_ids_list) # [batch, 2*window, dim]\n",
    "hidden = torch.mean(contexts, dim=1)          # 求平均 -> [batch, dim]\n",
    "# 后面拿这个 hidden 当作中心向量，去和 u_embedding 做点积预测目标词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 计算初始化范围\n",
    "initrange = 0.5 / dim \n",
    "\n",
    "# 2. 对中心词向量矩阵进行均匀分布初始化\n",
    "self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "# 3. 对上下文词向量矩阵进行均匀分布初始化\n",
    "self.out_embed.weight.data.uniform_(-initrange, initrange)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376fbcfe",
   "metadata": {},
   "source": [
    "### 为什么模型要手动初始化？\n",
    "\n",
    "数学原理：\n",
    "* Word2Vec 的核心计算是两个向量的点积：$Score = v_c \\cdot u_o = \\sum_{i=1}^{d} v_c[i] \\times u_o[i]$。\n",
    "* $d$ 是维度（embed_dim）。如果 $d$ 很大（比如 300），而每个元素 $v_c[i]$ 初始值比较大（比如 1.0），那么 300 个数相加，结果会非常大（比如 300.0）。\n",
    "* Sigmoid 的灾难：\n",
    "* 公式是 $\\sigma(x) = \\frac{1}{1+e^{-x}}$。\n",
    "* 如果输入 $x$ 是 10 或 300，Sigmoid 输出极其接近 1。\n",
    "* Sigmoid 的导数是 $\\sigma(x)(1-\\sigma(x))$。当 $\\sigma(x) \\approx 1$ 时，导数 $\\approx 0$。\n",
    "* 结果：梯度消失，模型一开始就“死”了，怎么训练都不动。\n",
    "\n",
    "为什么除以 embed_dim：\n",
    "\n",
    "为了让点积的结果保持在 0 附近（Sigmoid 的线性区，梯度最大），我们需要让每个元素足够小。\n",
    "\n",
    "维度 $d$ 越大，累加项越多，所以单个元素必须越小。这是一个经典的缩放技巧（类似 Xavier 初始化思想）。\n",
    "\n",
    "0.5 是一个经验值（Heuristic），你用 1.0 或 0.1 也可以，重点是 与维度成反比。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
