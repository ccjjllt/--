{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b98d43",
   "metadata": {},
   "source": [
    "分类架构：文本 -> Token -> Embedding -> LSTM -> 序列表示 -> 分类器 -> Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b1fac7",
   "metadata": {},
   "source": [
    "LSTM层的输出形状：\n",
    "\n",
    ">output, (h_n, c_n) = lstm(embeddings)\n",
    "\n",
    "output：[batch_size, seq_len, hidden_size]\n",
    "\n",
    "h_n：[num_layers * num_directions, batch_size, hidden_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c5f45d",
   "metadata": {},
   "source": [
    "分类模型中，我们用哪个向量做分类\n",
    "\n",
    "| 方法             | 描述                    |\n",
    "| -------------- | --------------------- |\n",
    "| 最后一个 time step | `output[:, -1, :]`    |\n",
    "| 最后一层 h_n       | `h_n[-1]`             |\n",
    "| Mean Pooling   | `mean(output, dim=1)` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24784e79",
   "metadata": {},
   "source": [
    "## 模型结构图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea69a944",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (909900121.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Input Text\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Input Text\n",
    "   ↓\n",
    "Token IDs\n",
    "   ↓\n",
    "Embedding Layer\n",
    "   ↓\n",
    "LSTM Encoder\n",
    "   ↓\n",
    "Last Hidden State (h_n)\n",
    "   ↓\n",
    "Fully Connected\n",
    "   ↓\n",
    "Softmax\n",
    "   ↓\n",
    "Prediction\n",
    "\n",
    "\n",
    "[batch, seq]\n",
    "   ↓\n",
    "[batch, seq, emb]\n",
    "   ↓\n",
    "[batch, seq, hidden]\n",
    "   ↓\n",
    "[batch, hidden]\n",
    "   ↓\n",
    "[batch, class]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1970646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Loss: 207.0214\n",
      "Epoch 2/5 | Loss: 207.0007\n",
      "Epoch 3/5 | Loss: 207.0101\n",
      "Epoch 4/5 | Loss: 207.0195\n",
      "Epoch 5/5 | Loss: 207.0379\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "#构造一个模拟文本分类数据集\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, num_samples=10000, vocab_size=5000, max_len=20):\n",
    "\n",
    "        #用一个list保存所有样本\n",
    "        #每一个元素是一个(input_ids, label)二元组\n",
    "        self.samples = []\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            #随机生成一个句子长度，下限为5\n",
    "            seq_len = random.randint(5, max_len)\n",
    "            #生成一个长度为seq_len的token序列，在[a, b)间采样\n",
    "            input_ids = torch.randint(1, vocab_size, (seq_len,))\n",
    "            #生成一个标签(0或1)，(1,)表示shape是[1], 再用.item()取出python标量\n",
    "            label = torch.randint(0, 1, (1,)).item()\n",
    "            #将(input_ids, label)作为一个样本加入samples\n",
    "            self.samples.append((input_ids, label))\n",
    "\n",
    "    #返回数据集大小\n",
    "    #DataLoader会在内部调用这个函数    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    #根据索引idx取出一个样本\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "    \n",
    "#padding + collate\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: List[(input_ids, label)]\n",
    "    \"\"\"\n",
    "    input_ids_list, labels = zip(*batch)\n",
    "\n",
    "    lengths = [len(x) for x in input_ids_list]\n",
    "    max_len = max(lengths)\n",
    "\n",
    "    padded_inputs = []\n",
    "    #对每个句子进行padding，确保每个句子长度一致\n",
    "    for x in input_ids_list:\n",
    "        #计算需要padding的长度\n",
    "        pad_len = max_len - len(x)\n",
    "        #使用torch.cat拼接原始句子和pad_len的0向量\n",
    "        padded = torch.cat([x, torch.zeros(pad_len, dtype=torch.long)])\n",
    "\n",
    "        padded_inputs.append(padded)\n",
    "\n",
    "    #将padded_inputs转换成一个tensor\n",
    "    #padded_inputs是一个列表，其中的每一个元素是一个tensor\n",
    "    #使用tensor.stack将它们堆叠成一个大的tensor，形状是[B,L]\n",
    "    padded_inputs = torch.stack(padded_inputs)  #[B,L]\n",
    "\n",
    "    #将label转换成tensor格式\n",
    "    labels = torch.tensor(labels)   #[B]\n",
    "\n",
    "    return padded_inputs, labels\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        embedings = self.embedding(input_ids)       #[B, L, E]\n",
    "        _, (h_n, _) = self.lstm(embedings)          #h_n: [1, B, H]\n",
    "        last_hidden = h_n[-1]\n",
    "        logits = self.fc(last_hidden)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "def train():\n",
    "    vocab_size = 5000\n",
    "    embed_dim = 128\n",
    "    hidden_size = 256\n",
    "    num_classes = 2\n",
    "    batch_size = 32\n",
    "    lr = 1e-3\n",
    "    epochs = 5\n",
    "\n",
    "    #数据\n",
    "    train_dataset = TextClassificationDataset()\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    #模型，优化器\n",
    "    model = LSTMClassifier(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        hidden_size=hidden_size,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    #训练\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_ids, labels in train_loader:\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad\n",
    "            loss.backward()\n",
    "            optimizer.step\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | Loss: {total_loss:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ad80b",
   "metadata": {},
   "source": [
    "## LSTM + Attention 文本分类结构\n",
    "结构流程：\n",
    "### 1.Embedding 层\n",
    "输入：[batch_size, seq_len] 的 token id\n",
    "\n",
    "输出：[batch_size, seq_len, embed_dim]\n",
    "\n",
    "### 2.BiLSTM 层\n",
    "输入：embedding\n",
    "\n",
    "输出：H，形状 [batch_size, seq_len, hidden_dim * 2]\n",
    "\n",
    "用双向 LSTM，这样 H 每个位置有前后文信息。\n",
    "\n",
    "### 3.Attention 层\n",
    "我们用一种最常见的 Additive Attention（Bahdanau-style 简化版）：\n",
    "\n",
    "- 先把 H 通过一个线性 + tanh 得到“注意力打分”：\n",
    "$$u_{t}=\\tanh \\left(W h_{t}+b\\right), \\quad u_{t} \\in \\mathbb{R}^{d_{a t t}}$$\n",
    "\n",
    "- 再和一个可学习向量 v 做内积得到标量分数：\n",
    "$$e_{t}=v^{\\top} u_{t}$$\n",
    "\n",
    "- 对 e_t 做 softmax 得到权重：\n",
    "$$\\alpha_{t}=\\frac{\\exp \\left(e_{t}\\right)}{\\sum_{k=1}^{T} \\exp \\left(e_{k}\\right)}$$\n",
    "\n",
    "- 最后句向量\n",
    "$$c=\\sum_{t=1}^{T} \\alpha_{t} h_{t}$$\n",
    "\n",
    "实现时：\n",
    "\n",
    "H : [B, T, H_dim]\n",
    "\n",
    "u : [B, T, att_dim]\n",
    "\n",
    "e : [B, T]\n",
    "\n",
    "α : [B, T]\n",
    "\n",
    "c : [B, H_dim]\n",
    "\n",
    "### 4.分类层\n",
    "输入：c [B, H_dim]\n",
    "\n",
    "过全连接 + softmax / sigmoid：\n",
    "\n",
    "$$\\hat{y}=\\operatorname{softmax}\\left(W_{c} c+b_{c}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75494b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda_envs\\envs\\pytorch2.3.1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/imdb/resolve/main/README.md (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x1f31bd6bf40>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: d0278d29-3170-42e9-a21a-8d8f7d3756bb)')' thrown while requesting HEAD https://huggingface.co/datasets/imdb/resolve/main/README.md\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n",
      "20000 5000 25000\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "raw_datasets = load_dataset(\"imdb\")     #二分类：0=neg， 1=pos\n",
    "\n",
    "print(raw_datasets)\n",
    "\n",
    "train_valid = raw_datasets[\"train\"]\n",
    "test_dataset = raw_datasets[\"test\"]\n",
    "\n",
    "#拆成20000train和5000valid\n",
    "train_valid = train_valid.train_test_split(test_size=5000, seed=42)\n",
    "train_dataset = train_valid[\"train\"]\n",
    "valid_dataset = train_valid[\"test\"]\n",
    "\n",
    "print(len(train_dataset), len(valid_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ad4aab",
   "metadata": {},
   "source": [
    "写一个几件英文分词，实际项目中可以换成更好的分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e55df34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size=  25954\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    #r表示原始字符串raw string, 避免/这种转义带来的麻烦\n",
    "    #+表示前面的模式至少出现一次，可以连续多次\n",
    "    #可在一起的意思是，匹配text里连续的小写英文字母字符串，也就是按英文单词切分\n",
    "    tokens = re.findall(r\"[a-z]+\", text)\n",
    "    return tokens\n",
    "\n",
    "min_freq = 5 #词频小于5的当<unk>\n",
    "counter = Counter()\n",
    "\n",
    "for example in train_dataset:\n",
    "    tokens = simple_tokenizer(example[\"text\"])\n",
    "    counter.update(tokens)\n",
    "\n",
    "#special tokens\n",
    "PAD = \"<pad>\"\n",
    "UNK = \"<unk>\"\n",
    "\n",
    "#<pad>用于补齐长度，编号0；<unk>表示未知词，编号1\n",
    "vocab = {PAD: 0, UNK: 1}\n",
    "\n",
    "#counter.items()返回(单词，词频)\n",
    "for word, freq in counter.items():\n",
    "    if freq >= min_freq:\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"vocab_size= \", vocab_size)\n",
    "\n",
    "#encode: 把一条原始文本转换成固定长度的token id序列\n",
    "def encode(text, vocab, max_len=256):\n",
    "    tokens = simple_tokenizer(text)\n",
    "    #把token转换成对应的词表id，如果token不在词表中，则用<UNK>的id\n",
    "    ids = [vocab.get(tok, vocab[UNK]) for tok in tokens]\n",
    "\n",
    "    if len(ids) < max_len:\n",
    "        pad_len = max_len - len(ids)\n",
    "        ids += [vocab[PAD]] * pad_len\n",
    "    \n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59bd1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, vocab, max_len=256):\n",
    "        self.dataset = hf_dataset\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx][\"text\"]\n",
    "        label = self.dataset[idx][\"label\"]\n",
    "\n",
    "        input_ids = encode(text, vocab=self.vocab, max_len=self.max_len)\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "        return input_ids, label\n",
    "    \n",
    "max_len = 256\n",
    "train_data = TextClassificationDataset(train_dataset, vocab, max_len=max_len)\n",
    "valid_data = TextClassificationDataset(valid_dataset, vocab, max_len=max_len)\n",
    "test_data = TextClassificationDataset(test_dataset, vocab, max_len=max_len)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f644fa",
   "metadata": {},
   "source": [
    "### 基线模型：BiLSTM 文本分类\n",
    "\n",
    "这个版本没有 Attention，就用 “最后一步 hidden state” 当句向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cf67dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, \n",
    "                 num_classes=2, num_layers=1, \n",
    "                 bidirectional=True, dropout=0.5, pad_idx=0):\n",
    "        super().__init__()\n",
    "\n",
    "        #输入：[B, T]\n",
    "        #输出：[B, T, embed_dim]\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "\n",
    "        #输出 H: [B, T, hidden_dim * 2]\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        lstm_output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "\n",
    "        self.fc = nn.Linear(lstm_output_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids: [B, T]\n",
    "        \"\"\"\n",
    "        emb = self.embedding(input_ids)     #[B, T, E]\n",
    "        H, (h_n, c_n) = self.lstm(emb)      #H: [B, T, 2H]\n",
    "\n",
    "        #取最后一个时间步的hidden state\n",
    "        #H[:, -1, :] : [B, 2H]\n",
    "        last_hidden = H[:, -1, :]\n",
    "\n",
    "        out = self.dropout(last_hidden)\n",
    "        logits = self.fc(out)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e3a22c",
   "metadata": {},
   "source": [
    "### BiLSTM + Attention 文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b98aaa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAttentionClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim,\n",
    "                 num_classes=2, num_layers=1, bidirectional=True,\n",
    "                 dropout=0.5, pad_idx=0, attn_dim=128):\n",
    "        super().__init__()\n",
    "        #输入：[B, T]\n",
    "        #输出：[B, T, embed_dim]\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "\n",
    "       #输出 H: [B, T, hidden_dim * 2]\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        lstm_out_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "\n",
    "        #attention\n",
    "        self.atten_w = nn.Linear(lstm_out_dim, attn_dim, bias=True)\n",
    "        self.atten_v = nn.Linear(attn_dim, 1, bias=False)\n",
    "\n",
    "        #分类器\n",
    "        self.fc = nn.Linear(lstm_out_dim, num_classes)\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def attention(self, H, mask=None):\n",
    "        \"\"\"\n",
    "        H: [B, T, H_dim]\n",
    "        mask: [B, T]，padding 位置为 0, 真实token为1\n",
    "        \"\"\"\n",
    "        u = torch.tanh(self.atten_w(H))     #[B, T, atten_dim]\n",
    "        #线性映射成标量打分\n",
    "        e = self.atten_v(u).squeeze(-1)     #[B, T]\n",
    "\n",
    "        #mask掉padding的位置\n",
    "        if mask is not None:\n",
    "            e = e.masked_fill(mask==0, float(\"-inf\"))\n",
    "\n",
    "        #softmax归一化得到attention的权重分布\n",
    "        alpha = F.softmax(e, dim=-1)        #[B, T]\n",
    "\n",
    "        alpha_expanded = alpha.unsqueeze(-1)\n",
    "\n",
    "        #加权隐藏状态\n",
    "        weighted_H = H * alpha_expanded     #[B, T, H_dim]\n",
    "\n",
    "        #沿时间求和得到上下文向量\n",
    "        context = weighted_H.sum(dim=1)     #[B, H_dim]\n",
    "\n",
    "        return context, alpha\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids: [B, T]\n",
    "        \"\"\"\n",
    "        emb = self.embedding(input_ids)     #[B, T, E]\n",
    "        H, _ = self.lstm(emb)               #[B, T, H_dim]\n",
    "\n",
    "        #标记真实token的位置\n",
    "        mask = (input_ids != self.pad_idx).long() #[B, T]\n",
    "\n",
    "        context, atten_weights = self.attention(H, mask)\n",
    "        out = self.dropout(context)\n",
    "        logits = self.fc(out)               #[B, num_classes]\n",
    "\n",
    "        return logits, atten_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767761d2",
   "metadata": {},
   "source": [
    "### 训练 + 评估 + 指标对比表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc889be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_examples = 0, 0, 0\n",
    "\n",
    "    for input_ids, labels in dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #区分两种模型，有无attention\n",
    "        outputs = model(input_ids)\n",
    "        if isinstance(outputs, tuple):\n",
    "            logits, _ = outputs\n",
    "        else:\n",
    "            logits = outputs\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_examples += input_ids.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_examples\n",
    "    avg_acc = total_correct / total_examples\n",
    "\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_examples = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for input_ids, labels in dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids)\n",
    "        if isinstance(outputs, tuple):\n",
    "            logits, _ = outputs\n",
    "        else:\n",
    "            logits = outputs\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_examples += input_ids.size(0)\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    avg_loss = total_loss / total_examples\n",
    "    avg_acc = total_correct / total_examples\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    return avg_loss, avg_acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7b8ea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BiLSTM] Epoch 1: train_loss: 0.6941, train_acc: 0.5039, valid_loss: 0.6955, valid_acc: 0.4994, valid_f1: 0.4294\n",
      "[BiLSTM] Epoch 2: train_loss: 0.6911, train_acc: 0.5311, valid_loss: 0.6960, valid_acc: 0.5044, valid_f1: 0.4401\n",
      "[BiLSTM] Epoch 3: train_loss: 0.6858, train_acc: 0.5363, valid_loss: 0.7049, valid_acc: 0.5058, valid_f1: 0.4077\n",
      "[BiLSTM] Epoch 4: train_loss: 0.6783, train_acc: 0.5498, valid_loss: 0.6982, valid_acc: 0.5198, valid_f1: 0.4760\n",
      "[BiLSTM] Epoch 5: train_loss: 0.6710, train_acc: 0.5508, valid_loss: 0.7107, valid_acc: 0.4970, valid_f1: 0.4195\n",
      "[BiLSTM] Test: loss= 0.7026, acc= 0.5084, f1= 0.4267\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "hidden_dim = 128\n",
    "num_classes = 2\n",
    "pad_idx = vocab[PAD]\n",
    "num_epochs = 5\n",
    "\n",
    "baseline_model = BiLSTMClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_classes=num_classes,\n",
    "    num_layers=1,\n",
    "    bidirectional=True,\n",
    "    dropout=0.5,\n",
    "    pad_idx=pad_idx\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(baseline_model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_one_epoch(baseline_model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc, val_f1 = evaluate(baseline_model, valid_loader, criterion)\n",
    "    print(f\"[BiLSTM] Epoch {epoch+1}: \"\n",
    "          f\"train_loss: {train_loss:.4f}, train_acc: {train_acc:.4f}, \"\n",
    "          f\"valid_loss: {val_loss:.4f}, valid_acc: {val_acc:.4f}, valid_f1: {val_f1:.4f}\")\n",
    "    \n",
    "#最后在test数据集上评估\n",
    "test_loss, test_acc, test_f1 = evaluate(baseline_model, test_loader, criterion)\n",
    "print(f\"[BiLSTM] Test: loss= {test_loss:.4f}, acc= {test_acc:.4f}, f1= {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "574c79d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BiLSTM+Attn] Epoch 1: train_loss=0.4868, train_acc=0.7499, val_loss=0.3692, val_acc=0.8360, val_f1=0.8356\n",
      "[BiLSTM+Attn] Epoch 2: train_loss=0.2997, train_acc=0.8742, val_loss=0.3170, val_acc=0.8646, val_f1=0.8643\n",
      "[BiLSTM+Attn] Epoch 3: train_loss=0.2082, train_acc=0.9182, val_loss=0.3002, val_acc=0.8768, val_f1=0.8767\n",
      "[BiLSTM+Attn] Epoch 4: train_loss=0.1256, train_acc=0.9564, val_loss=0.3484, val_acc=0.8732, val_f1=0.8731\n",
      "[BiLSTM+Attn] Epoch 5: train_loss=0.0653, train_acc=0.9783, val_loss=0.4549, val_acc=0.8732, val_f1=0.8732\n",
      "[BiLSTM+Attn] Test: loss=0.5245, acc=0.8522, f1=0.8522\n"
     ]
    }
   ],
   "source": [
    "attn_model = LSTMAttentionClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_classes=num_classes,\n",
    "    num_layers=1,\n",
    "    bidirectional=True,\n",
    "    dropout=0.5,\n",
    "    pad_idx=pad_idx,\n",
    "    attn_dim=128\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(attn_model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = train_one_epoch(attn_model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc, val_f1 = evaluate(attn_model, valid_loader, criterion)\n",
    "    print(f\"[BiLSTM+Attn] Epoch {epoch}: \"\n",
    "          f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, \"\n",
    "          f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, val_f1={val_f1:.4f}\")\n",
    "\n",
    "test_loss, test_acc, test_f1 = evaluate(attn_model, test_loader, criterion)\n",
    "print(f\"[BiLSTM+Attn] Test: loss={test_loss:.4f}, acc={test_acc:.4f}, f1={test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4806f891",
   "metadata": {},
   "source": [
    "### Attention的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd5dcdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there           |  (0.000)\n",
      "is              |  (0.000)\n",
      "no              | ##### (0.005)\n",
      "relation        |  (0.000)\n",
      "at              |  (0.000)\n",
      "all             |  (0.000)\n",
      "between         |  (0.001)\n",
      "<unk>           |  (0.000)\n",
      "and             |  (0.000)\n",
      "<unk>           |  (0.000)\n",
      "but             |  (0.000)\n",
      "the             |  (0.000)\n",
      "fact            |  (0.000)\n",
      "that            |  (0.000)\n",
      "both            | ### (0.004)\n",
      "are             | ####### (0.007)\n",
      "police          | ############################### (0.031)\n",
      "series          | ####################### (0.024)\n",
      "about           | #################### (0.020)\n",
      "violent         | ################# (0.017)\n",
      "crimes          |  (0.000)\n",
      "<unk>           |  (0.000)\n",
      "looks           | # (0.001)\n",
      "<unk>           |  (0.000)\n",
      "<unk>           |  (0.000)\n",
      "looks           | # (0.002)\n",
      "classic         |  (0.000)\n",
      "<unk>           |  (0.000)\n",
      "plots           |  (0.001)\n",
      "are             | ### (0.003)\n"
     ]
    }
   ],
   "source": [
    "def show_attention_one_sample(model, dataset, idx, top_k=30):\n",
    "    \"\"\"\n",
    "    model: 已训练好的 LSTM + Attention 模型\n",
    "    dataset: TextClassificationDataset（自定义的数据集）\n",
    "    idx: 要查看的样本在 dataset 中的索引\n",
    "    top_k: 最多显示前多少个 token（防止句子太长）\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    #从dataset中提取第idx的样本\n",
    "    #input_ids: [T], label: 标量\n",
    "    input_ids, label = dataset[idx]\n",
    "\n",
    "    #增加batch长度，把[T] -> [1, T]\n",
    "    input_ids = input_ids.unsqueeze(0).to(device)   #[1, T]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #logits: [1, num_classes]\n",
    "        #attn_weights: [1, T](每个时间步的attention权重)\n",
    "        logits, attn_weights = model(input_ids)\n",
    "\n",
    "    #去掉batch长度：[1, T] -> [T]\n",
    "    attn_weights = attn_weights.squeeze(0).cpu().numpy()    #[T]\n",
    "\n",
    "    #同时去掉batch维度：[1, T] -> [T]\n",
    "    ids = input_ids.squeeze(0).cpu().tolist()\n",
    "\n",
    "    #把padding的token去掉\n",
    "    valid_pairs = [(i, w) for i, w in zip(ids, attn_weights) if i != vocab[PAD]]\n",
    "\n",
    "    #避免句子太长，只显示前top_k个token\n",
    "    valid_pairs = valid_pairs[:top_k]\n",
    "\n",
    "    tokens, weights = [], []\n",
    "\n",
    "    for i, w in valid_pairs:\n",
    "        tokens.append(id2word.get(i, UNK))\n",
    "        weights.append(w)\n",
    "\n",
    "    #将token和attention权重一一对应打印出来\n",
    "    for tok, w in zip(tokens, weights):\n",
    "        #用“#”的长度来表示attention权重的大小\n",
    "        bar = \"#\" * int(w * 1000)\n",
    "\n",
    "        #左边是token(固定长度为15)\n",
    "        print(f\"{tok:15s} | {bar} ({w:.3f})\")\n",
    "\n",
    "id2word = {idx: word for word, idx in vocab.items()}\n",
    "show_attention_one_sample(attn_model, valid_data, idx=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
