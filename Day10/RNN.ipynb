{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0702b9b",
   "metadata": {},
   "source": [
    "# RNN计算机制\n",
    "\n",
    "循环神经网络（recurrent neural network，简称RNN）源自于1982年由Saratha Sathasivam 提出的霍普菲尔德网络。循环神经网络，是指在全连接神经网络的基础上增加了前后时序上的关系，可以更好地处理比如机器翻译等的与时序相关的问题。\n",
    "\n",
    "在传统的神经网络模型中，是从输入层到隐含层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。\n",
    "\n",
    "<center><img src=\"https://i-blog.csdnimg.cn/blog_migrate/9d914ca33c0ab5e3899f1c43c317442e.png\"></center>\n",
    "\n",
    "一个典型的 RNN 网络架构包含一个输入，一个输出和一个神经网络单元 。和普通的前馈神经网络的区别在于：RNN 的神经网络单元不但与输入和输出存在联系，而且自身也存在一个循环 / 回路 / 环路 / 回环 (loop)。这种回路允许信息从网络中的一步传递到下一步。\n",
    "\n",
    ">RNN的本质：上一个时刻的网络状态将会作用于（影响）到下一个时刻的网络状态\n",
    "\n",
    "## 1.最经典的RNN公式（vanilla rnn）\n",
    "\n",
    "隐状态ht是对“历史的压缩表示”。\n",
    "给定序列输入x1, x2,...,xt，隐状态ht的更新为：\n",
    "$$ h_{t}=\\phi\\left(W_{x h} x_{t}+W_{h h} h_{t-1}+b_{h}\\right)$$\n",
    "\n",
    "其中：\n",
    "- $x_{t} \\in \\mathbb{R}^{d_{x}}$，$x_{t}$代表当前时刻的输入\n",
    "- $h_{t-1} \\in \\mathbb{R}^{d_{h}}$, $h_{t-1}$代表上一时刻的记忆，它不是某个词，而是模型到目前为止对历史的总结\n",
    "- $W_{x h} \\in \\mathbb{R}^{d_{h} \\times d_{x}}$, $W_{x h}$是“输入 → 记忆”的转换规则，将当前的输入，转换成对记忆有用的形式\n",
    "- $W_{h h} \\in \\mathbb{R}^{d_{h} \\times d_{h}}$, $W_{h h}$是“记忆 → 记忆”的转换规则，它决定过去的信息要保留多少\n",
    "- $b_{h} \\in \\mathbb{R}^{d_{h}}$, $b_{h}$是偏置\n",
    "- $\\phi$常见是 tanh 或 ReLU（vanilla RNN 最常用 tanh）*\n",
    "\n",
    ">一个简单的表示：$新的记忆=tanh(当前信息+过去记忆)$\n",
    "\n",
    "**注意**：每个时间步用的都是同一组参数$W_{x h}$, $W_{h h}$, $b_{h}$。称为参数共享，也就是说，RNN在时间维度上是同一个函数的反复应用。\n",
    "\n",
    "这可以带来一些好处：\n",
    "- 参数量与序列长度T 无关\n",
    "\n",
    "- 能泛化到不同长度序列\n",
    "\n",
    "- 但也导致训练时梯度要穿越很多步\n",
    "\n",
    "## 2.时间展开\n",
    "\n",
    "RNN原图是有环的，有环图就像你想一次性画出整个序列的计算，但它永远在转圈。\n",
    "\n",
    "时间展开（time unrolling / unfolding）本质上是把“带环的递归计算”变成“无环的前向计算图”。\n",
    "\n",
    "<center><img src=\"https://i-blog.csdnimg.cn/blog_migrate/7e3c62157f034a10c33f35cab65c5ba1.png\"></center>\n",
    "\n",
    ">为什么需要时间展开？\n",
    "\n",
    "### (1)为了“按时间顺序”把前向算清楚（依赖链）\n",
    "RNN 的前向本质就是：h1 -> h2 ->...-> ht\n",
    "\n",
    "想算$h_t$就必须算$h_{t-1}$, 没法跳过中间步骤。\n",
    "\n",
    ">时间展开把这种依赖结构变成明确的“链式流程”。这点对代码时间很重要，在代码里就是一个for循环\n",
    "\n",
    "### (2)为了训练 —— BPTT（时间反向传播）\n",
    "深度学习训练依靠反向传播（backprop），反向传播的前提是你有一个无环的计算图（DAG），才能按拓扑顺序做梯度回传。\n",
    "\n",
    "原始 RNN 是有环的，没法直接在“环”上定义标准的反向传播流程。\n",
    "\n",
    "时间展开把环拆开后，就得到得到一个标准的前馈网络（只不过层数 = 时间步数 T），于是可以做反向传播。\n",
    "\n",
    "### ()\n",
    "\n",
    "## 3.手写一个RNN cell\n",
    "\n",
    "使用pytorch实现一个单步的RNN Cell，输入xt 和上一时刻$h_{t-1}$，输出ht。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "022dad8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_all shape:  torch.Size([2, 4, 5])\n",
      "h_t shape:  torch.Size([2, 5])\n",
      "Quick test passed.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VanilaaRnnCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla (Elman) RNN Cell:\n",
    "        h_t = tanh(x_t @ W_xh^T + h_{t-1} @ W_hh^T + b)\n",
    "\n",
    "    Shapes:\n",
    "        x_t:  (B, input_size)\n",
    "        h_tm1:(B, hidden_size)\n",
    "        h_t:  (B, hidden_size)\n",
    "\n",
    "    Parameters are stored in (hidden_size, input_size) and (hidden_size, hidden_size),\n",
    "    matching PyTorch nn.Linear weight convention for easy comparison.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        if input_size <= 0 or hidden_size <= 0:\n",
    "            raise ValueError(\"input_size and hidden_size must be positive integers.\")\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        # Weight matrices:\n",
    "        # W_xh: (hidden_size, input_size)\n",
    "        # W_hh: (hidden_size, hidden_size)\n",
    "        self.W_xh = nn.Parameter(torch.empty(hidden_size, input_size))\n",
    "        self.W_hh = nn.Parameter(torch.empty(hidden_size, hidden_size))\n",
    "\n",
    "        if bias:\n",
    "            self.b_h = nn.Parameter(torch.empty(hidden_size))\n",
    "        else:\n",
    "            self.register_parameter(\"b_h\", None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        A reasonable default init:\n",
    "        - Use uniform in [-k, k], k = 1/sqrt(hidden_size)\n",
    "        This is similar to common RNN initializations.\n",
    "        \"\"\"\n",
    "        k = 1.0 / math.sqrt(self.hidden_size)\n",
    "        nn.init.uniform_(self.W_xh, -k, k)\n",
    "        nn.init.uniform_(self.W_hh, -k, k)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.b_h, -k, k)\n",
    "\n",
    "    def forward(self, x_t: torch.Tensor, h_tm1: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Single-step forward.\n",
    "\n",
    "        Args:\n",
    "            x_t:   (B, input_size)\n",
    "            h_tm1: (B, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            h_t:   (B, hidden_size)\n",
    "        \"\"\"\n",
    "        if x_t.dim() != 2:\n",
    "            raise ValueError(f\"x_t must be 2D (B, input_size), got shape {tuple(x_t.shape)}\")\n",
    "        if h_tm1.dim() != 2:\n",
    "            raise ValueError(f\"h_tm1 must be 2D (B, hidden_size), got shape {tuple(h_tm1.shape)}\")\n",
    "        if x_t.size(1) != self.input_size:\n",
    "            raise ValueError(f\"x_t second dim must be input_size={self.input_size}, got {x_t.size(1)}\")\n",
    "        if h_tm1.size(1) != self.hidden_size:\n",
    "            raise ValueError(f\"h_tm1 second dim must be hidden_size={self.hidden_size}, got {h_tm1.size(1)}\")\n",
    "        if x_t.size(0) != h_tm1.size(0):\n",
    "            raise ValueError(f\"Batch sizes must match, got {x_t.size(0)} vs {h_tm1.size(0)}\")\n",
    "        \n",
    "        # (B, hidden) = (B, input) @ (hidden, input)^T\n",
    "        x_part = x_t @ self.W_xh.t()\n",
    "        # (B, hidden) = (B, hidden) @ (hidden, hidden)^T\n",
    "        h_part = h_tm1 @ self.W_hh.t()\n",
    "\n",
    "        preact = x_part + h_part\n",
    "        if self.b_h is not None:\n",
    "            preact = preact + self.b_h # broadcasts (hidden,) over batch\n",
    "        \n",
    "        h_t = torch.tanh(preact)\n",
    "        return h_t\n",
    "    \n",
    "    #时间展开\n",
    "    @torch.no_grad\n",
    "    def forward_sequence(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        h0: Optional[torch.Tensor] = None,\n",
    "        batch_first: bool = True\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Convenience function to run through a full sequence for testing.\n",
    "\n",
    "        Args:\n",
    "            x: (B, T, input_size) if batch_first else (T, B, input_size)\n",
    "            h0: (B, hidden_size) optional, defaults to zeros\n",
    "            batch_first: whether x is batch-first\n",
    "\n",
    "        Returns:\n",
    "            h_all: (B, T, hidden_size) if batch_first else (T, B, hidden_size)\n",
    "            h_T:   (B, hidden_size)\n",
    "        \"\"\"\n",
    "        if x.dim() != 3:\n",
    "            raise ValueError(f\"x must be 3D, got shape {tuple(x.shape())}\")        \n",
    "        \n",
    "        if batch_first:\n",
    "            B, T, D = x.shape\n",
    "            x_seq = x\n",
    "        else:\n",
    "            T, B, D = x.shape\n",
    "            x_seq = x.transpose(0, 1) #-> (B, T, D)\n",
    "        \n",
    "        if D != self.input_size:\n",
    "            raise ValueError(f\"input last dim must be input_size={self.input_size}, got {D}\")\n",
    "        \n",
    "        if h0 is None:\n",
    "            h_t = torch.zeros(B, self.hidden_size, device=x.device, dtype=x.dtype)\n",
    "        else:\n",
    "            if h0.shape != (B, self.hidden_size):\n",
    "                raise ValueError(f\"h0 must be shape (B, hidden_size) = ({B}, {self.hidden_size})\")\n",
    "            else:\n",
    "                h_t = h0\n",
    "\n",
    "        h_list = []\n",
    "        for t in range(T):\n",
    "            h_t = self.forward(x_seq[:, t, :], h_t)\n",
    "            h_list.append(h_t)\n",
    "        \n",
    "        h_all = torch.stack(h_list, dim=1) #(B, T, hidden)\n",
    "        if not batch_first:\n",
    "            h_all = h_all.transpose(0, 1) #back to (T, B, hidden)\n",
    "        \n",
    "        return h_all, h_t\n",
    "    \n",
    "#验证cell是否与手写的时间步循环计算结果是否一致\n",
    "def _quick_test():\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    B, T, Din, H = 2, 4, 3, 5\n",
    "    x = torch.randn(B, T, Din)\n",
    "\n",
    "    cell = VanilaaRnnCell(input_size=Din, hidden_size=H, bias=True)\n",
    "\n",
    "    h_all, h_t = cell.forward_sequence(x, batch_first=True)\n",
    "    print(\"h_all shape: \", h_all.shape) #(B, T, H)\n",
    "    print(\"h_t shape: \", h_t.shape) #(B, H)\n",
    "\n",
    "    #compare one step with manual call\n",
    "    h0 = torch.zeros(B, H)\n",
    "    h1 = cell(x[:, 0, :], h0)\n",
    "\n",
    "    # 断言（assert）：\n",
    "    # 检查“手动算出来的 h1”\n",
    "    # 是否和“循环版本里存下来的第 0 个时间步输出 h_all[:, 0, :]”几乎相等\n",
    "    # atol=1e-6 表示允许的数值误差（浮点误差）\n",
    "    # 如果不相等，就报错并显示 \"Step output mismatch!\"\n",
    "    assert torch.allclose(h1, h_all[:, 0, :], atol=1e-6), \"Step output mismatch\"\n",
    "    \n",
    "    print(\"Quick test passed.\")\n",
    "    # 如果没有触发 assert，说明结果一致\n",
    "    # 打印提示：快速测试通过\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _quick_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacf6d54",
   "metadata": {},
   "source": [
    "## 4.防止参数爆炸\n",
    "\n",
    "我们注意到代码中有一个函数reset_parameters(self), 把 W_xh、W_hh、b 初始化到 [-k, k], 这是为什么呢？\n",
    "\n",
    "先看公式：\n",
    "$$ h_{t}=\\phi\\left(W_{x h} x_{t}+W_{h h} h_{t-1}+b_{h}\\right)$$\n",
    "\n",
    "如果W初始化很大，h_t 一开始就会变得很大 → 过非线性（tanh/sigmoid）会饱和 → 梯度变很小 → 学不动；或者反复递推时数值越滚越大 → 爆炸\n",
    "\n",
    "如果W初始化很小，h_t 会非常小，反复乘 W_hh 会越乘越小 → 消失\n",
    "\n",
    ">所以我们想要W 的尺度刚刚好，让 h 的尺度在时间上传播时比较稳定。\n",
    "\n",
    "**下面我们来推导为什么k = 1/sqrt(hidden_size)**\n",
    "\n",
    "设 h_{t-1} 的每个分量都像“平均大小差不多”的随机数。\n",
    "W_hh h_{t-1} 的某个输出分量是：\n",
    "$$\\sum_{i=1}^{H} W_{j i} h_{i}$$\n",
    "\n",
    "这是一堆随机项相加。相加项越多（H 越大），总的波动就越大，除非你把每个 W_{ji} 的尺度调小。\n",
    "\n",
    "更具体一点（非常常见的近似）：\n",
    "\n",
    "假设 W_{ji} 独立、均值 0，方差是 Var(W)\n",
    "\n",
    "假设 h_i 独立、均值 0，方差是 Var(h)\n",
    "\n",
    "那么和的方差大约是：\n",
    "$$\\operatorname{Var}\\left(\\sum_{i=1}^{H} W_{j i} h_{i}\\right) \\approx \\sum_{i=1}^{H} \\operatorname{Var}\\left(W_{j i} h_{i}\\right)=H \\cdot \\operatorname{Var}(W) \\cdot \\operatorname{Var}(h)$$\n",
    "\n",
    "我们希望输出方差和输入方差差不多（别越来越大或越来越小），即：\n",
    "$$H \\cdot \\operatorname{Var}(W) \\approx 1 \\Rightarrow \\operatorname{Var}(W) \\approx \\frac{1}{H}$$\n",
    "\n",
    "这就是关键：\n",
    ">权重的方差应当随 hidden_size H 增大而按 1/H 缩小。\n",
    "\n",
    "可以使用均匀分布初始化，也可以使用正态分布初始化：\n",
    "\n",
    "均匀分布 U(-k, k) 的方差是：$\\operatorname{Var}=\\frac{k^{2}}{3}$\n",
    "\n",
    "正态分布 N(0, σ²) 的方差是: $\\operatorname{Var}=\\sigma^{2}$\n",
    "\n",
    "我们的代码里用的是均匀分布，如果我们想要 Var(W) ≈ 1/H，那：\n",
    "\n",
    "$$\\frac{k^{2}}{3} \\approx \\frac{1}{H} \\Rightarrow k \\approx \\sqrt{\\frac{3}{H}}$$\n",
    "\n",
    "常数因子差一点影响不大，训练会自己调整,选 1/sqrt(H) 更保守一点（范围稍小一点），在 RNN 里常常更稳。\n",
    "\n",
    "那么正态分布的$\\sigma$值该取多少就留给各位自己去算了。\n",
    "\n",
    "这里有个细节，我们把bias也放在了[-k,k],是为了：\n",
    "\n",
    "避免一开始就强偏置到某个方向（比如 tanh 一直在正区间）\n",
    "\n",
    "让 pre-activation（线性部分）整体尺度一致，更稳定\n",
    "\n",
    "所以bias 通常也用相似尺度的初始化，保持“整体输入到非线性之前”的数值范围合理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0070d",
   "metadata": {},
   "source": [
    "## 5.正交初始化\n",
    "\n",
    "很多现代 RNN 还会对 W_hh 用 正交初始化，进一步让谱半径接近 1，更利于长序列梯度传播。\n",
    "\n",
    "对于RNN的公式有：\n",
    "$$h_{t} \\approx W_{h h} h_{t-1} \\Rightarrow h_{T} \\approx W_{h h}^{T} h_{0}$$\n",
    "\n",
    "**重点：**同一个W_hh被乘了 T 次（这里的上标 T 是次数，不是转置）。\n",
    "\n",
    "如果W_hh在某个方向上“放大一点点”> 1;乘很多次后会 指数级放大 → 前向值爆炸，反向梯度也爆炸\n",
    "\n",
    "如果W_hh在某个方向上“缩小一点点”< 1;乘很多次后会 指数级缩小 → 变成 0，梯度消失\n",
    "\n",
    ">RNN 在时间上传播信息 = 多次连乘同一个矩阵；连乘最怕“略大于 1”或“略小于 1”。\n",
    "\n",
    "如果 Q 是正交矩阵：$Q^{\\top} Q=I$\n",
    "\n",
    "那么对任意向量 v：$\\|Q v\\|=\\|v\\|$\n",
    "\n",
    "**正交变换 = 纯旋转/镜像，不拉伸不压缩。**\n",
    "\n",
    "如果把W_hh初始化成正交矩阵Q：\n",
    "$$h_{T} \\approx Q^{T} h_{0}$$\n",
    "\n",
    "- 不改变向量长度\n",
    "\n",
    "- 谱半径 = 1\n",
    "\n",
    "- 时间连乘不容易爆炸 / 消失\n",
    "\n",
    "因为每次乘都不改变长度，所以“能量”不会在时间上莫名其妙爆炸或消失。\n",
    "\n",
    "正交矩阵的特征值都落在单位圆上（复平面上模长为 1 的圆），因此：\n",
    "$$\\rho(Q)=1$$\n",
    "\n",
    "也就是说，正交初始化天然就把谱半径钉在 1 附近（理想情况下就是 1）。\n",
    "\n",
    "谱半径的定义是：$\\rho(W)=\\max _{i}\\left|\\lambda_{i}\\right|$，其中$\\lambda_{i}$是矩阵W的特征值。\n",
    "\n",
    ">正交初始化比均匀分布、正态分布等随机初始化更适合RNN，"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
