{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TF-IDF from Scratch (BoW / TF / IDF 数学本质)\n",
        "\n",
        "**Day 3 目标：** 能手写 TF-IDF 公式；能解释 IDF 为什么有用；能手写实现（不用 sklearn）。\n",
        "\n",
        "> 建议你把这份 notebook 里的关键公式、实现步骤自己再手敲一遍（面试就是要你能当场写出来）。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. 你需要掌握的两句话\n",
        "1) **BoW 假设：** 文档只由词的出现次数/权重表示，忽略词序（语法/局部上下文）。\n",
        "2) **TF-IDF：** 用 **TF** 表示“这个词在当前文档的重要性”，用 **IDF** 抑制“全语料到处都是的词”。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. BoW 数学形式\n",
        "设词表大小为 $|V|$，文档 $d$ 的 BoW 向量 $\\mathbf{x}^{(d)} \\in \\mathbb{R}^{|V|}$：\n",
        "\n",
        "- 最朴素：$x^{(d)}_t = \\text{count}(t, d)$\n",
        "- 关键特性：**高维 + 稀疏**（大部分维度为 0）\n",
        "\n",
        "**面试常问：** 为什么稀疏？因为每个文档只覆盖词表里极少一部分词。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. TF 的几种常见定义\n",
        "设词 $t$ 在文档 $d$ 中出现次数为 $f(t,d)$，文档总词数为 $|d|$。\n",
        "\n",
        "**(A) Raw count：**\n",
        "$$\\mathrm{tf}(t,d)=f(t,d)$$\n",
        "表达的是最原始的假设：出现越多越重要\n",
        "\n",
        "存在的问题：\n",
        "\n",
        "1.长文档天生占便宜：同一个主题，长文档里几乎所有的词count都大。长文档会因为“数值更大”被偏爱，而不是因为”更相关“\n",
        "\n",
        "2.重复刷词会线性变强：比如一篇文档里”npl“出现了100次，Raw count会让它比重复10次强10倍，但现实中相关性通常没这么线性。\n",
        "\n",
        "**(B) Length-normalized：**\n",
        "$$\\mathrm{tf}(t,d)=\\frac{f(t,d)}{|d|}$$\n",
        "归一化把”绝对次数“变成了”相对频率“，也就是这个词在这篇文档中占了多大比例。\n",
        "\n",
        "例子：文档1：长度100，npl出现5次 -> tf=0.05 ； 文档2：长度10，npl出现2次 -> tf=0.2\n",
        "\n",
        "Raw count 会说文档1更重要（5 > 2），但频率告诉你文档2更“集中讨论”nlp。\n",
        "\n",
        "副作用：\n",
        "\n",
        "对特别短的文档，频率会跳得特别大\n",
        "\n",
        "只解决了”长短“问题，没有解决”刷词线性增长问题“\n",
        "\n",
        "**(C) Sublinear（减弱重复带来的线性增长）：**\n",
        "$$\\mathrm{tf}(t,d)=1+\\log f(t,d) \\quad (f>0),\\;0\\;\\text{otherwise}$$\n",
        "取log是在修复词偏差性，你希望出现次数从1 → 2有明显增益，但从50 → 51增益应该很小。log正好是边际收益递减。\n",
        "\n",
        "例子：\n",
        "\n",
        "count：1 -> 2 是翻倍\n",
        "\n",
        "log tf：(1+log1)=1 -> (1+log2)≈1.693，增益约为0.693\n",
        "\n",
        "count：10 -> 100 是10倍\n",
        "\n",
        "log tf: (1+log10)≈3.303 -> (1+log100)≈5.605，增益约为2.302\n",
        "\n",
        "为什么要加“1”？避免 f=1 时 log(1)=0 让 tf 变成 0（失去“出现过”的信号）。\n",
        "\n",
        "你在实现里可以任选一种，但要能解释“为什么要归一化/对数”。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7ddf090",
      "metadata": {},
      "source": [
        "## 3. IDF 推导直觉（重点：为什么有用）\n",
        "设语料有 $N$ 篇文档，词 $t$ 出现过的文档数为 $\\mathrm{df}(t)$。\n",
        "\n",
        "为什么用 df(t)（文档频次）而不是词频总和？\n",
        "\n",
        "因为我们关心的是：这个词能不能区分文档。\n",
        "区分能力取决于“覆盖了多少文档”，而不是“在少数文档里刷了多少次”。\n",
        "\n",
        "**想要的性质：**\n",
        "- 词在越多文档出现（df 大），它越“不区分”文档 → 权重应该越小。\n",
        "- 词越稀有（df 小），它越“区分”文档 → 权重应该越大。\n",
        "\n",
        "满足“反比”直觉的一种形式是 $\\frac{N}{\\mathrm{df}(t)}$，再取对数压缩动态范围：\n",
        "$$\\mathrm{idf}(t)=\\log\\frac{N}{\\mathrm{df}(t)}$$\n",
        "这个公式是怎么来的？我们想要：\n",
        "\n",
        "df 大 → 权重小\n",
        "\n",
        "df 小 → 权重大\n",
        "\n",
        "最简单满足单调性的就是反比：weight(t)∝1/df(t)\n",
        "\n",
        "再乘上N做尺度化（让它跟语料规模有关系）：N/df(t)\n",
        "\n",
        "取log是为了:\n",
        "\n",
        "(1) 动态范围太夸张，少量稀有词会“爆表”，例如：N=1000000\n",
        "\n",
        "df=1, N/df = 1000000; df=10000, N/df=100. 差了10000倍，模型被极少数词强行主宰。\n",
        "\n",
        "使用log：log(1000000)≈13.8; log(10000)≈4.6. 差值变得可控\n",
        "\n",
        "（2）“边际收益递减”更合理\n",
        "\n",
        "df 从 1 增加到 2（变得更常见一倍）对区分度影响很大；df 从 100001 增加到 100002 基本没区别。\n",
        "\n",
        "\n",
        "\n",
        "**平滑（避免 df=0，且让常见词也有稳定数值）** 常见写法：\n",
        "$$\\mathrm{idf}(t)=\\log\\frac{N+1}{\\mathrm{df}(t)+1}+1$$\n",
        "为什么分子分母都加“1”？\n",
        "\n",
        "1.避免 df=0 时除零：如果模型在推理的时候遇到了在训练时没见过的词，即df=0，这时不平滑会直接爆炸（idf=∞）\n",
        "\n",
        "2.让数值更稳定：即使 df 很小，也不会无限大到离谱。\n",
        "\n",
        "为什么最后还要加“1”？\n",
        "\n",
        "保证 IDF 非负，并且让 df=N 的词不至于变成 0。全出现的词 idf=1，而不是 0。idf=0，意味着我们要永远丢弃这些词，这在很多场景是可以的（停用词），但库一般不替你做“强硬裁剪”，而是留一点权重，再让下游（比如模型/分类器/相似度）去决定；同时也避免了某些数值/实现里出现全 0 特征导致的边界问题。\n",
        "\n",
        "这也是很多库（如 scikit-learn）常用的平滑变体之一。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e70231",
      "metadata": {},
      "source": [
        "## 4. TF-IDF 公式（你必须能手写）\n",
        "$$\\mathrm{tfidf}(t,d) = \\mathrm{tf}(t,d)\\cdot \\mathrm{idf}(t)$$\n",
        "（重要）TF-IDF 向量是什么？（从“单词权重”到“文档表示”）\n",
        "\n",
        "向量空间模型（Vector Space Model）的核心思想是：把一篇文档表示成一个向量，而向量的每一维，对应词表里的一个词\n",
        "\n",
        "假设整个语料库的词表是V={t1, t2, t3,...,tv}. 那么向量的第j维就是tj。\n",
        "\n",
        "对于文档d，我们构造一个向量d = {tfidf(t1,d), tfidf(t2,d), tfidf(t3,d),...,tfidf(tv,d)}. 在文档向量中，第j个分词的数值，等于词tj在在文档d中的tf-idf权重。\n",
        "\n",
        "例子： \n",
        "    t1 = \"猫\"；\n",
        "    t2 = \"狗\"；\n",
        "    t3 = \"鱼\"；\n",
        "\n",
        "在某篇文档中：\n",
        "    tfidf(\"猫\", d) = 0.3；\n",
        "    tfidf(\"狗\", d) = 0；\n",
        "    tfidf(\"鱼\", d) = 0.8；\n",
        "\n",
        "向量d = (0.3,0,0.8)\n",
        "\n",
        "可选的向量归一化（面试常问）：对每个文档向量做 L2 normalize：\n",
        "$$\\hat{\\mathbf{x}}^{(d)}=\\frac{\\mathbf{x}^{(d)}}{\\left\\|\\mathbf{x}^{(d)}\\right\\|_{2}}, \\quad\\|\\mathbf{x}\\|_{2}=\\sqrt{\\sum_{j} x_{j}^{2}}$$\n",
        "L2 normalize解决的问题：\n",
        "\n",
        "向量“尺度”（整体大小）会影响相似度。假如使用点积做相似度计算，Xa x Xb，向量越大（词更多/TF 总量更大），点积往往越大 → 长文档/词多文档更占便宜。\n",
        "\n",
        "经过L2 normalize之后每个文档的向量长度都变成了1，这样比较的时候就不会因为谁向量大就偏向谁了。\n",
        "\n",
        "L2 normalize 和 cosine 相似度的关系\n",
        "\n",
        "cosine相似度：$\\cos (\\theta)=\\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\|_{2}\\|\\mathbf{y}\\|_{2}}$\n",
        "\n",
        "如果你先把两边都 L2 normalize：$\\hat{\\mathbf{x}}=\\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_{2}}, \\hat{\\mathbf{y}}=\\frac{\\mathbf{y}}{\\|\\mathbf{y}\\|_{2}}$\n",
        "\n",
        "那么就会得到：$\\hat{\\mathbf{x}} \\cdot \\hat{\\mathbf{y}}=\\cos (\\theta)$\n",
        "\n",
        "所以结论就是：对 TF-IDF 向量做 L2 normalize + 点积” 等价于 “直接做 cosine 相似度\n",
        "\n",
        "normalize 把长度因素去掉，让你更关注“词权重分布结构”，这对检索/聚类通常更合理，因为长度不等于相关性。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db2474f4",
      "metadata": {},
      "source": [
        "## 5. 代码任务：手写 TF-IDF（不用 sklearn）\n",
        "实现目标：输入 `List[List[str]]`（已分好词的语料），输出：\n",
        "- `vocab: Dict[str,int]` 词到索引\n",
        "- `idf: List[float]` 或 `np.ndarray`\n",
        "- `X_tfidf: np.ndarray` (N, |V|) 稀疏可以先用 dense（面试写起来更快），但要能解释如何用稀疏矩阵优化。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e87f376a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Iterable, Optional\n",
        "import math\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a0fdfd0",
      "metadata": {},
      "source": [
        "### 5.1 一个极小语料（先跑通，再泛化）\n",
        "你也可以换成自己的语料，但建议先用这个做 sanity check。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aac126d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    [\"i\", \"love\", \"nlp\", \"nlp\"],\n",
        "    [\"i\", \"love\", \"deep\", \"learning\"],\n",
        "    [\"nlp\", \"is\", \"fun\"],\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bbd76fe",
      "metadata": {},
      "source": [
        "### 5.2 你要实现的最小 API\n",
        "- `build_vocab(corpus)`\n",
        "- `compute_df(corpus, vocab)`\n",
        "- `compute_idf(df, N)`\n",
        "- `compute_tfidf(corpus, vocab, idf, tf_variant='log')`\n",
        "- 可选：`l2_normalize_rows(X)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d9acba4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_vocab(corpus: List[List[str]]) -> Dict[str, int]:\n",
        "    \"\"\"Build vocabulary mapping token -> index.\"\"\"\n",
        "    vocab: Dict[str, int] = {}\n",
        "    for doc in corpus:\n",
        "        for tok in doc:\n",
        "            if tok not in vocab:\n",
        "                vocab[tok] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "def compute_df(corpus: List[List[str]], vocab: Dict[str, int]) -> np.ndarray:\n",
        "    \"\"\"Document frequency: df[t] = number of documents containing token t.\"\"\"\n",
        "    V = len(vocab)\n",
        "    df = np.zeros(V, dtype=np.int64)\n",
        "    for doc in corpus:\n",
        "        seen = set(doc)  # unique tokens in this doc\n",
        "        for tok in seen:\n",
        "            idx = vocab.get(tok)\n",
        "            if idx is not None:\n",
        "                df[idx] += 1\n",
        "    return df\n",
        "\n",
        "def compute_idf(df: np.ndarray, N: int, smooth: bool = True) -> np.ndarray:\n",
        "    \"\"\"Compute IDF vector.\n",
        "    smooth=True uses: idf = log((N+1)/(df+1)) + 1\n",
        "    \"\"\"\n",
        "    df = df.astype(np.float64)\n",
        "    if smooth:\n",
        "        return np.log((N + 1.0) / (df + 1.0)) + 1.0\n",
        "    else:\n",
        "        # NOTE: if df==0, this will be inf; in practice, vocab comes from corpus so df>0.\n",
        "        return np.log(N / df)\n",
        "\n",
        "def _tf_value(count: int, doc_len: int, variant: str) -> float:\n",
        "    \"\"\"Compute TF for a single token in a doc.\"\"\"\n",
        "    if count <= 0:\n",
        "        return 0.0\n",
        "    if variant == \"raw\":\n",
        "        return float(count)\n",
        "    if variant == \"freq\":\n",
        "        return float(count) / float(doc_len)\n",
        "    if variant == \"log\":\n",
        "        return 1.0 + math.log(float(count))\n",
        "    raise ValueError(f\"Unknown tf variant: {variant}\")\n",
        "\n",
        "def compute_tfidf(\n",
        "    corpus: List[List[str]],\n",
        "    vocab: Dict[str, int],\n",
        "    idf: np.ndarray,\n",
        "    tf_variant: str = \"log\",\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Return dense TF-IDF matrix X with shape (N, V).\"\"\"\n",
        "    N = len(corpus)\n",
        "    V = len(vocab)\n",
        "    X = np.zeros((N, V), dtype=np.float64)\n",
        "\n",
        "    for i, doc in enumerate(corpus):\n",
        "        doc_len = len(doc)\n",
        "        # term counts for this doc\n",
        "        counts: Dict[int, int] = {}\n",
        "        for tok in doc:\n",
        "            j = vocab.get(tok)\n",
        "            if j is None:\n",
        "                continue\n",
        "            counts[j] = counts.get(j, 0) + 1\n",
        "\n",
        "        for j, c in counts.items():\n",
        "            tf = _tf_value(c, doc_len, tf_variant)\n",
        "            X[i, j] = tf * float(idf[j])\n",
        "\n",
        "    return X\n",
        "\n",
        "def l2_normalize_rows(X: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
        "    \"\"\"Row-wise L2 normalization.\"\"\"\n",
        "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
        "    return X / (norms + eps)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3515609e",
      "metadata": {},
      "source": [
        "### 5.3 跑一遍：输出 vocab / df / idf / tfidf\n",
        "你要能手算一个小例子：比如词 `nlp` 在第 1 篇文档 tfidf 是多少？\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4c560331",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'i': 0, 'love': 1, 'nlp': 2, 'deep': 3, 'learning': 4, 'is': 5, 'fun': 6},\n",
              " array([2, 2, 2, 1, 1, 1, 1], dtype=int64),\n",
              " array([1.28768207, 1.28768207, 1.28768207, 1.69314718, 1.69314718,\n",
              "        1.69314718, 1.69314718]),\n",
              " array([[1.28768207, 1.28768207, 2.18023527, 0.        , 0.        ,\n",
              "         0.        , 0.        ],\n",
              "        [1.28768207, 1.28768207, 0.        , 1.69314718, 1.69314718,\n",
              "         0.        , 0.        ],\n",
              "        [0.        , 0.        , 1.28768207, 0.        , 0.        ,\n",
              "         1.69314718, 1.69314718]]),\n",
              " array([[0.45329466, 0.45329466, 0.76749457, 0.        , 0.        ,\n",
              "         0.        , 0.        ],\n",
              "        [0.42804604, 0.42804604, 0.        , 0.5628291 , 0.5628291 ,\n",
              "         0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.4736296 , 0.        , 0.        ,\n",
              "         0.62276601, 0.62276601]]))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = build_vocab(corpus)\n",
        "df = compute_df(corpus, vocab)\n",
        "idf = compute_idf(df, N=len(corpus), smooth=True)\n",
        "X = compute_tfidf(corpus, vocab, idf, tf_variant=\"log\")\n",
        "X_norm = l2_normalize_rows(X)\n",
        "\n",
        "vocab, df, idf, X, X_norm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64474cc7",
      "metadata": {},
      "source": [
        "### 5.4 让输出更可读：按词表顺序打印\n",
        "面试点：你要能解释某些词权重为什么变大/变小。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c006514f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab order: ['i', 'love', 'nlp', 'deep', 'learning', 'is', 'fun']\n",
            "df: [2, 2, 2, 1, 1, 1, 1]\n",
            "idf: [1.2877, 1.2877, 1.2877, 1.6931, 1.6931, 1.6931, 1.6931]\n",
            "\n",
            "TF-IDF (dense):\n",
            "[[1.2877 1.2877 2.1802 0.     0.     0.     0.    ]\n",
            " [1.2877 1.2877 0.     1.6931 1.6931 0.     0.    ]\n",
            " [0.     0.     1.2877 0.     0.     1.6931 1.6931]]\n",
            "\n",
            "TF-IDF (L2-normalized):\n",
            "[[0.4533 0.4533 0.7675 0.     0.     0.     0.    ]\n",
            " [0.428  0.428  0.     0.5628 0.5628 0.     0.    ]\n",
            " [0.     0.     0.4736 0.     0.     0.6228 0.6228]]\n"
          ]
        }
      ],
      "source": [
        "# Pretty print\n",
        "inv_vocab = {i: t for t, i in vocab.items()}\n",
        "terms = [inv_vocab[i] for i in range(len(inv_vocab))]\n",
        "\n",
        "print(\"Vocab order:\", terms)\n",
        "print(\"df:\", df.tolist())\n",
        "print(\"idf:\", np.round(idf, 4).tolist())\n",
        "print(\"\\nTF-IDF (dense):\")\n",
        "print(np.round(X, 4))\n",
        "print(\"\\nTF-IDF (L2-normalized):\")\n",
        "print(np.round(X_norm, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7cfa4a9",
      "metadata": {},
      "source": [
        "## 6. 你必须能回答的 3 个面试追问（写下你的答案）\n",
        "1) **IDF 为什么有用？**（提示：区分性、信息量、抑制停用词）\n",
        "\n",
        "2) **为什么要对数/归一化 TF？**（提示：重复词、文档长度偏置）\n",
        "\n",
        "3) **稀疏向量的工程影响？**（提示：内存、矩阵乘法、倒排索引/稀疏矩阵）\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f379100d",
      "metadata": {},
      "source": [
        "1.IDF 用来衡量一个词在全语料层面的区分能力。\n",
        "如果一个词在几乎所有文档中都出现（df 很大），它携带的信息量很低，无法区分文档，应当被压低权重；反之，只在少数文档中出现的词更具判别性。\n",
        "因此 IDF 通过对高 df 词施加惩罚，抑制停用词的干扰，放大“主题词”的影响，使 TF-IDF 更关注能区分文档的特征。\n",
        "\n",
        "2.Raw TF 会带来两个问题：长文档天然数值更大，以及词频重复会线性放大权重。\n",
        "Length-normalized TF 通过除以文档长度，消除了文档长短带来的偏置，使权重反映“相对重要性”而非“绝对次数”。\n",
        "Log TF 则引入边际收益递减，避免某个词通过大量重复主宰向量，但仍保留“出现过 vs 未出现”的关键信号。\n",
        "\n",
        "3.TF-IDF 向量维度等于词表大小，而单篇文档只包含极少数词，因此向量高度稀疏。\n",
        "工程上如果用 dense 表示，会造成巨大内存浪费和无效计算，实际系统通常采用稀疏矩阵或倒排索引。\n",
        "这种表示可以显著降低存储成本，并加速相似度计算和检索，是搜索引擎和文本检索系统的基础。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6506e02a",
      "metadata": {},
      "source": [
        "## 7. 加餐：你会怎么处理 OOV？\n",
        "- 做法 A：忽略 OOV（线上检索/匹配时可能还行）\n",
        "- 做法 B：用 `<UNK>` 统一映射\n",
        "- 做法 C：子词/字符级（到后面 wordpiece/BPE 再系统讲）\n",
        "\n",
        "写出你更倾向哪种，以及理由。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abbe2fe4",
      "metadata": {},
      "source": [
        "优先选择做法 C（子词 / 字符级建模），必要时结合 B 作为兜底。\n",
        "\n",
        "理由:\n",
        "\n",
        "从根本上解决 OOV 问题.\n",
        "子词（BPE / WordPiece）可以把任何新词拆解，理论上几乎不存在 OOV。\n",
        "\n",
        "信息损失最小，泛化能力更强.\n",
        "相比 <UNK> 把所有未知词混为一类，子词保留了词形和语义组合信息，模型能对未见词做合理推断。\n",
        "\n",
        "特别适合中文和专业领域.\n",
        "中文新词多、组合灵活，子词/字符级比词级更鲁棒。\n",
        "\n",
        "工业界与主流模型的事实标准.\n",
        "BERT、GPT 等主流模型都采用子词方案，工程上成熟可靠。\n",
        "\n",
        "A（忽略 OOV）：只适合检索或规则系统，不适合作为模型建模方案\n",
        "\n",
        "B：实现简单，但语义信息损失大，只适合作为兜底"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da32530f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'i': 0, 'love': 1, 'nlp': 2, 'deep': 3, 'learning': 4, 'is': 5, 'fun': 6} [2 2 2 1 1 1 1] [1.28768207 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718] [[1.28768207 1.28768207 2.18023527 0.         0.         0.\n",
            "  0.        ]\n",
            " [1.28768207 1.28768207 0.         1.69314718 1.69314718 0.\n",
            "  0.        ]\n",
            " [0.         0.         1.28768207 0.         0.         1.69314718\n",
            "  1.69314718]] [[0.45329466 0.45329466 0.76749457 0.         0.         0.\n",
            "  0.        ]\n",
            " [0.42804604 0.42804604 0.         0.5628291  0.5628291  0.\n",
            "  0.        ]\n",
            " [0.         0.         0.4736296  0.         0.         0.62276601\n",
            "  0.62276601]]\n",
            "\n",
            "\n",
            "\n",
            "Vocab order: ['i', 'love', 'nlp', 'deep', 'learning', 'is', 'fun']\n",
            "df: [2, 2, 2, 1, 1, 1, 1]\n",
            "idf: [1.2877, 1.2877, 1.2877, 1.6931, 1.6931, 1.6931, 1.6931]\n",
            "\n",
            "TF-IDF (dense):\n",
            "[[1.2877 1.2877 2.1802 0.     0.     0.     0.    ]\n",
            " [1.2877 1.2877 0.     1.6931 1.6931 0.     0.    ]\n",
            " [0.     0.     1.2877 0.     0.     1.6931 1.6931]]\n",
            "\n",
            "TF-IDF (L2-normalized):\n",
            "[[0.4533 0.4533 0.7675 0.     0.     0.     0.    ]\n",
            " [0.428  0.428  0.     0.5628 0.5628 0.     0.    ]\n",
            " [0.     0.     0.4736 0.     0.     0.6228 0.6228]]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "corpus = [\n",
        "    [\"i\", \"love\", \"nlp\", \"nlp\"],\n",
        "    [\"i\", \"love\", \"deep\", \"learning\"],\n",
        "    [\"nlp\", \"is\", \"fun\"],\n",
        "]\n",
        "\n",
        "#list[list[str]] -> dict[str,int]\n",
        "def build_vocab(corpus):\n",
        "    \"\"\"build vocabulary making token -> index\"\"\"\n",
        "    vocab = {}\n",
        "    for doc in corpus:\n",
        "        for tok in doc:\n",
        "            if tok not in vocab:\n",
        "                vocab[tok] = len(vocab)\n",
        "    \n",
        "    return vocab\n",
        "\n",
        "def compute_df(corpus, vocab):\n",
        "    \"\"\"document frequency: df[t] = number of documents containing token t\"\"\"\n",
        "    V = len(vocab)\n",
        "    df = np.zeros(V, dtype=np.int64)\n",
        "    for doc in corpus:\n",
        "        seen = set(doc)\n",
        "        for tok in seen:\n",
        "            idx = vocab.get(tok)\n",
        "            if idx is not None:\n",
        "                df[idx] += 1\n",
        "    \n",
        "    return df\n",
        "\n",
        "def compute_idf(df, N, smooth=True):\n",
        "    \"\"\"\n",
        "    compute idf\n",
        "    smooth = True\n",
        "    idf = log((N+1) / (df+1)) + 1\n",
        "    \"\"\"\n",
        "    df = df.astype(np.float64)\n",
        "    if smooth:\n",
        "        return np.log((N + 1.0) / (df + 1.0)) + 1\n",
        "    else:\n",
        "        #note: if df=0, this will be inf; in practice, vocab comes from corpus so df > 0.\n",
        "        return np.log(N / df)\n",
        "    \n",
        "def _tf_value(count, doc_len, variant):\n",
        "    \"\"\"compute TF in a single token in a doc.\"\"\"\n",
        "    if count < 0:\n",
        "        return 0.0\n",
        "    if variant == \"raw\":\n",
        "        return count\n",
        "    if variant == \"freq\":\n",
        "        return float(count) / float(doc_len)\n",
        "    if variant == \"log\":\n",
        "        return np.log(float(count)) + 1.0\n",
        "    raise ValueError(f\"unknown TF variant: {variant}\")\n",
        "\n",
        "def compute_tfidf(corpus, vocab, idf, tf_variant=\"log\"):\n",
        "    \"\"\"return dense tf-idf matrix X with shape (N, V)\"\"\"\n",
        "    N = len(corpus)\n",
        "    V = len(vocab)\n",
        "    X = np.zeros((N, V), dtype=np.float64)\n",
        "\n",
        "    for i, doc in enumerate(corpus):\n",
        "        doc_len = len(doc)\n",
        "        counts = {}\n",
        "        for tok in doc:\n",
        "            j = vocab.get(tok)\n",
        "            if j is None:\n",
        "                continue\n",
        "            counts[j] = counts.get(j, 0) + 1\n",
        "\n",
        "        for j, c in counts.items():\n",
        "            tf = _tf_value(c, doc_len, tf_variant)\n",
        "            X[i, j] = tf * float(idf[j])\n",
        "\n",
        "    return X\n",
        "\n",
        "def l2_normalize_rows(X, eps=1e-12):\n",
        "    \"\"\"Row-wise L2 normalization\"\"\"\n",
        "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
        "    return X / (norms + eps)\n",
        "\n",
        "\n",
        "vocab = build_vocab(corpus)\n",
        "df = compute_df(corpus, vocab)\n",
        "idf = compute_idf(df, N=len(corpus), smooth=True)\n",
        "X = compute_tfidf(corpus, vocab, idf, tf_variant=\"log\")\n",
        "X_norm = l2_normalize_rows(X)\n",
        "\n",
        "print(vocab, df, idf, X, X_norm)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "#Pretty print\n",
        "inv_vocab = {i: t for t, i in vocab.items()}\n",
        "terms = [inv_vocab[i] for i in range(len(inv_vocab))]\n",
        "\n",
        "print(\"Vocab order:\", terms)\n",
        "print(\"df:\", df.tolist())\n",
        "print(\"idf:\", np.round(idf, 4).tolist())\n",
        "print(\"\\nTF-IDF (dense):\")\n",
        "print(np.round(X, 4))\n",
        "print(\"\\nTF-IDF (L2-normalized):\")\n",
        "print(np.round(X_norm, 4))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch2.3.1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.25"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
